from dotenv import load_dotenv
load_dotenv()
import nest_asyncio # V_CURRENT_FIX: 允许多层异步事件循环嵌套执行 (Allow multi-level asynchronous event loop nested execution)
nest_asyncio.apply() # V_CURRENT_FIX: 应用补丁以支持多层异步 (Apply patch to support multi-level asynchronous)
import os
import openai
import fitz # 导入 PyMuPDF 库，其模块名为 fitz (Import PyMuPDF library, its module name is fitz)
import tiktoken # 导入 tiktoken 库用于估算 token 数量 (Import tiktoken library for estimating token count)
import time # 导入 time 模块用于测量耗时 (Import time module for measuring time consumption)
import sys # 导入 sys 模块 (Import sys module)
import json # 新增导入 json 模块 (Newly added: import json module)
import hashlib # 新增导入 hashlib 模块，用于计算外部资料的哈希值 (Newly added: import hashlib module, used to calculate the hash value of external data)
import logging # 新增导入 logging 模块 (Newly added: import logging module)
from datetime import datetime # 新增导入 datetime 模块 (Newly added: import datetime module)
import re # 导入正则表达式模块 (Import regular expression module)
import requests
from bs4 import BeautifulSoup
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import httplib2 # 用于 Google API 客户端的代理配置 (Used for proxy configuration of Google API client)
from urllib.parse import urlparse # 用于解析代理 URL (Used to parse proxy URL)
import socket # V11.1 新增，用于 perform_search 超时 (V11.1 Newly added, used for perform_search timeout)
import asyncio # 用于异步操作 (Used for asynchronous operations)
import aiohttp # 用于异步 HTTP 请求 (Used for asynchronous HTTP requests)
import ssl # 新增导入 (Newly added import)
import certifi # 新增导入 (Newly added import)
from thefuzz import fuzz # V_CURRENT_FIX: 用于 apply_patch 中的模糊字符串匹配 (V_CURRENT_FIX: Used for fuzzy string matching in apply_patch)
from google_auth_httplib2 import AuthorizedHttp # 用于带认证的 Google API 代理 (Used for Google API proxy with authentication)
from google.oauth2.service_account import Credentials # 如果使用服务账户密钥访问 Google API (If using service account key to access Google API)
import chromadb # 新增：导入 ChromaDB 向量数据库 (Newly added: import ChromaDB vector database)
import collections # V5 新增：用于实现任务队列 (V5 Newly added: used to implement task queue)
import tenacity # V11.1 新增，用于健壮的重试机制 (V11.1 Newly added, used for robust retry mechanism)

# 尝试从 .env 文件加载环境变量 (Try to load environment variables from .env file)
try:
    from dotenv import load_dotenv
    load_dotenv()
    # 尽早初始化日志记录，以捕获 dotenv 加载过程中的消息 (Initialize logging as early as possible to capture messages during the dotenv loading process)
    # 如果完整设置尚未运行，则为初始消息提供基本配置 (If the full setup has not run yet, provide basic configuration for initial messages)
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s')
    logging.info("正在尝试加载 .env 文件... (Attempting to load .env file...)")
    if load_dotenv():
        logging.info(".env 文件加载成功。(Successfully loaded .env file.)")
    else:
        logging.info(".env 文件未找到或为空。将依赖系统环境变量。(env file not found or empty. Will rely on system environment variables.)")
except ImportError:
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s')
    logging.info("dotenv 库不可用。将依赖系统环境变量。(dotenv library not available. Will rely on system environment variables.)")
except Exception as e:
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s')
    logging.warning(f"加载 .env 文件时出错 (Error loading .env file): {e}")

# --- 工具定义辅助函数 (大纲审查) --- (Tool definition helper function (outline review))
# 【最终方案】为大纲审查定义的专用工具 (Tool) (【Final Solution】Dedicated tool (Tool) defined for outline review)
# 这个结构将强制AI返回一个格式完美的、包含计划列表的JSON (This structure will force AI to return a perfectly formatted JSON containing a list of plans)
def get_outline_review_tool_definition():
    # 定义“章节”的递归结构，以便AI能理解并生成包含子章节的嵌套计划 (Define the recursive structure of "chapter" so that AI can understand and generate nested plans containing sub-chapters)
    chapter_schema = {
        "type": "object",
        "properties": {
            "title": {
                "type": "string",
                "description": "章节的标题，必须简洁明了。"
            },
            "description": {
                "type": "string",
                "description": "对本章节核心内容的2-3句话简要描述。"
            },
            "target_chars_ratio": {
                "type": "number",
                "description": "本章节预计占剩余总字数的比例（一个0到1之间的小数）。"
            },
            "sections": {
                "type": "array",
                "description": "子章节列表（可选），其结构与父章节相同。",
                "items": {"$ref": "#/definitions/chapter_definition"}
            }
        },
        "required": ["title", "description"] # 规定title和description是必填项 (Specify that title and description are required items)
    }

    # 这是我们提供给AI的完整工具定义 (This is the complete tool definition we provide to AI)
    tools_definition = [
        {
            "type": "function",
            "function": {
                "name": "update_document_outline",
                "description": "根据评审意见和已完成的工作，更新或修正文档的剩余大纲。如果原始计划依然完美，则必须提交与原始计划完全相同的计划。",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "revised_plan": {
                            "type": "array",
                            "description": "一个包含所有剩余章节对象的列表。",
                            "items": {"$ref": "#/definitions/chapter_definition"}
                        }
                    },
                    "required": ["revised_plan"],
                    "definitions": {
                        "chapter_definition": chapter_schema
                    }
                }
            }
        }
    ]
    return tools_definition

# --- 工具定义辅助函数 (初始大纲生成) --- (Tool definition helper function (initial outline generation))
def get_initial_outline_tool_definition():
    # 定义“章节”模式，允许递归的子章节 ("sections") (Define "chapter" mode, allowing recursive sub-chapters ("sections"))
    chapter_schema_for_initial_outline = {
        "type": "object",
        "properties": {
            "title": {
                "type": "string",
                "description": "本章节或小节的标题。" # 中文翻译 (Chinese translation)
            },
            "description": {
                "type": "string",
                "description": "对本章节内容和目的的简要描述（2-3句话）。" # 中文翻译 (Chinese translation)
            },
            "target_chars_ratio": {
                "type": "number",
                "description": "本章节预计占文档总长度的比例（例如，0.1代表10%）。顶层章节的比例总和应约为1.0。" # 中文翻译 (Chinese translation)
            },
            "sections": {
                "type": "array",
                "description": "可选的子章节或小节列表，每个都遵循此相同结构。", # 中文翻译 (Chinese translation)
                "items": {"$ref": "#/definitions/chapter_definition_initial"} # 递归定义 (Recursive definition)
            }
        },
        "required": ["title", "description"] # target_chars_ratio 可以被AI初步省略，稍后会进行规范化 (target_chars_ratio can be initially omitted by AI and will be normalized later)
    }

    tools_definition = [
        {
            "type": "function",
            "function": {
                "name": "create_initial_document_outline",
                "description": "根据用户的问题陈述生成结构化的文档大纲。大纲应包括一个主标题和章节列表，每个章节都有标题、描述以及可选的子小节。", # 中文翻译 (Chinese translation)
                "parameters": {
                    "type": "object",
                    "properties": {
                        "title": {
                            "type": "string",
                            "description": "整个文档/报告的主标题。" # 中文翻译 (Chinese translation)
                        },
                        "outline": {
                            "type": "array",
                            "description": "一个对象列表，其中每个对象代表文档的一个主要章节。", # 中文翻译 (Chinese translation)
                            "items": {"$ref": "#/definitions/chapter_definition_initial"}
                        }
                    },
                    "required": ["title", "outline"],
                    "definitions": {
                        "chapter_definition_initial": chapter_schema_for_initial_outline
                    }
                }
            }
        }
    ]
    return tools_definition

# --- 工具定义辅助函数 (Patcher JSON) --- (Tool definition helper function (Patcher JSON))
def get_patcher_tool_definition():
    patch_action_enum = ["REPLACE", "INSERT_AFTER", "DELETE"] #可选的补丁操作类型 (Optional patch operation types)
    
    patch_object_schema = {
        "type": "object",
        "properties": {
            "action": {
                "type": "string",
                "enum": patch_action_enum,
                "description": "要执行的补丁操作。" # 中文翻译 (Chinese translation)
            },
            "target_section": {
                "type": "string",
                "description": "目标小节的精确 Markdown 标题字符串（例如，'## 引言', '### 2.1 分析'）。这必须与文档中的某个标题完全匹配。" # 中文翻译 (Chinese translation)
            },
            "new_content": {
                "type": "string",
                "description": "该小节的新 Markdown 内容。对于 'REPLACE' 和 'INSERT_AFTER' 操作是必需的。对于 'DELETE' 操作应省略或为空字符串。" # 中文翻译 (Chinese translation)
            }
        },
        "required": ["action", "target_section"]
    }

    tools_definition = [
        {
            "type": "function",
            "function": {
                "name": "generate_json_patch_list",
                "description": "根据反馈生成一个 JSON 补丁操作列表，以修改 Markdown 文档。每个补丁通过其精确的标题来定位一个小节。", # 中文翻译 (Chinese translation)
                "parameters": {
                    "type": "object",
                    "properties": {
                        "patches": {
                            "type": "array",
                            "description": "一个补丁操作列表。如果不需要任何更改，则应为空列表。", # 中文翻译 (Chinese translation)
                            "items": patch_object_schema
                        }
                    },
                    "required": ["patches"]
                }
            }
        }
    ]
    return tools_definition

# --- 优化过程的自定义异常 --- (Custom exception for optimization process)
class OptimizationError(RuntimeError):
    """在优化过程中发生错误时抛出的自定义异常，可能携带部分数据。""" # 中文翻译 (Custom exception thrown when an error occurs during the optimization process, may carry partial data.)
    def __init__(self, message, partial_data=None):
        super().__init__(message)
        self.partial_data = partial_data if partial_data is not None else {}

class Config:
    def __init__(self):
        # 主要 LLM API 配置 (DeepSeek) (Main LLM API configuration (DeepSeek))
        self.deepseek_api_key = os.getenv("DEEPSEEK_API_KEY")
        self.deepseek_base_url = os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com/v1")

        # 嵌入模型 API 配置 (BGE-M3 via Gitee) (Embedding model API configuration (BGE-M3 via Gitee))
        self.embedding_api_base_url = os.getenv("EMBEDDING_API_BASE_URL")
        self.embedding_api_key = os.getenv("EMBEDDING_API_KEY")
        self.embedding_model_name = os.getenv("EMBEDDING_MODEL_NAME", "bge-m3")

        # Google 搜索 API 配置 (Google Search API configuration)
        self.google_api_key = os.getenv("GOOGLE_API_KEY")
        self.google_cse_id = os.getenv("GOOGLE_CSE_ID")
        self.google_service_account_path = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")

        # --- 向量数据库配置 --- (Vector database configuration)
        self.vector_db_path = os.getenv("VECTOR_DB_PATH", "./chroma_db")
        self.vector_db_collection_name = os.getenv("VECTOR_DB_COLLECTION_NAME", "experience_store") # 经验库集合名称 (Experience base collection name)
        self.embedding_batch_size = int(os.getenv("EMBEDDING_BATCH_SIZE", 32)) # 嵌入批处理大小 (Embedding batch size)
        self.num_retrieved_experiences = int(os.getenv("NUM_RETRIEVED_EXPERIENCES", 3)) # 检索的经验数量 (Number of retrieved experiences)

        # 模型名称 (根据新计划更新) (Model names (updated according to new plan))
        self.main_ai_model = os.getenv("MAIN_AI_MODEL", "deepseek-chat") # 默认内容编写器 (Default content writer)
        self.main_ai_model_heavy = os.getenv("MAIN_AI_MODEL_HEAVY", "deepseek-reasoner") # 用于复杂/推理密集型内容 (Used for complex/inference-intensive content)
        self.secondary_ai_model = os.getenv("SECONDARY_AI_MODEL", "deepseek-reasoner") # 评论 AI (Review AI)
        self.summary_model_name = os.getenv("SUMMARY_MODEL_NAME", "deepseek-coder") # 摘要器，擅长结构化处理 (Summarizer, good at structured processing)
        self.researcher_model_name = os.getenv("RESEARCHER_MODEL_NAME", "deepseek-reasoner") # 用于生成搜索查询 (Used to generate search queries)
        self.outline_model_name = os.getenv("OUTLINE_MODEL_NAME", "deepseek-coder") # 大纲生成 (Outline generation)
        self.planning_review_model_name = os.getenv("PLANNING_REVIEW_MODEL_NAME", "deepseek-coder") # 审查和修正计划 (Review and correct plan)
        self.editorial_model_name = os.getenv("EDITORIAL_MODEL_NAME", self.main_ai_model) # 默认为 main_ai_model (deepseek-chat) 进行最终润色 (Defaults to main_ai_model (deepseek-chat) for final polishing)
        self.json_fixer_model_name = os.getenv("JSON_FIXER_MODEL_NAME", "deepseek-coder") # 用于修复格式错误的 JSON (Used to fix malformed JSON)


        # 分词器 (Tokenizer)
        try:
            self.encoder = tiktoken.get_encoding("cl100k_base")
        except Exception as e:
            print(f"严重错误：初始化 tiktoken 编码器失败 (Fatal error: failed to initialize tiktoken encoder): {e}", file=sys.stderr) # 中文翻译 (Chinese translation)
            self.encoder = None

        # 路径和目录 (Paths and directories)
        self.checkpoint_file_name = "optimization_checkpoint_outline.json" # 优化检查点大纲文件名 (Optimization checkpoint outline filename)
        self.session_base_dir = "output" # 会话基础目录 (Session base directory)
        self.session_dir = "" # 会话目录 (Session directory)
        self.log_file_path = "" # 日志文件路径 (Log file path)

        # 数值参数 (Numerical parameters)
        self.api_request_timeout_seconds = int(os.getenv("API_TIMEOUT_SECONDS", 900)) # API 请求超时秒数 (API request timeout seconds)
        self.max_iterations = int(os.getenv("MAX_ITERATIONS", 7)) # 最大迭代次数 (Maximum number of iterations)
        self.initial_solution_target_chars = int(os.getenv("INITIAL_SOLUTION_TARGET_CHARS", 20000)) # 初始解决方案目标字符数 (Initial solution target character count)
        self.max_context_for_long_text_review_tokens = int(os.getenv("MAX_CONTEXT_TOKENS_REVIEW", 30000)) # 长文本审查的最大上下文 Token 数 (Maximum context token count for long text review)
        self.intermediate_edit_max_tokens = int(os.getenv("INTERMEDIATE_EDIT_MAX_TOKENS", 8192)) # 中间编辑最大 Token 数 (Intermediate edit maximum token count)
        
        # API 调用重试参数 (API call retry parameters)
        self.api_retry_max_attempts = int(os.getenv("API_RETRY_MAX_ATTEMPTS", 3)) # API 重试最大尝试次数 (API retry maximum attempts)
        self.api_retry_wait_multiplier = int(os.getenv("API_RETRY_WAIT_MULTIPLIER", 1)) # 指数等待的基数，例如 1 表示 2,4,8秒；2 表示 4,8,16秒 (Base for exponential wait, e.g., 1 means 2,4,8 seconds; 2 means 4,8,16 seconds)
        self.api_retry_max_wait = int(os.getenv("API_RETRY_MAX_WAIT", 60)) # 重试之间的最大等待秒数 (Maximum wait seconds between retries)

        self.max_chunk_tokens = int(os.getenv("MAX_CHUNK_TOKENS", 4096)) # 每块最大 Token 数 (Maximum token count per chunk)
        self.overlap_chars = int(os.getenv("OVERLAP_CHARS", 800)) # 重叠字符数 (Number of overlapping characters)
        self.max_chunks_per_section = int(os.getenv("MAX_CHUNKS_PER_SECTION", 20)) # 每节最大块数 (Maximum number of chunks per section)
        self.num_search_results = int(os.getenv("NUM_SEARCH_RESULTS", 3)) # 每个查询的搜索结果数 (Number of search results per query)
        self.max_queries_per_gap = int(os.getenv("MAX_QUERIES_PER_GAP", 5)) # V8: 单个知识空白的最大查询次数 (V8: Maximum number of queries for a single knowledge gap)
        self.min_chars_for_short_chunk_warning = int(os.getenv("MIN_CHARS_SHORT_CHUNK", 50)) # 短块警告的最小字符数 (Minimum character count for short chunk warning)
        self.min_allocated_chars_for_section = int(os.getenv("MIN_ALLOCATED_CHARS_SECTION", 100)) # 分配给小节的最小字符数 (Minimum character count allocated to a section)

        # 布尔标志 (Boolean flags)
        self.interactive_mode = os.getenv("INTERACTIVE_MODE", "False").lower() == "true" # 交互模式 (Interactive mode)
        self.use_async_research = os.getenv("USE_ASYNC_RESEARCH", "True").lower() == "true" # 使用异步研究 (Use asynchronous research)
        self.enable_dynamic_outline_correction = os.getenv("ENABLE_DYNAMIC_OUTLINE_CORRECTION", "True").lower() == "true" # 启用动态大纲修正 (Enable dynamic outline correction)

        # 用户特定输入 (User specific input)
        self.user_problem = "" # 用户问题 (User problem)
        self.external_data_files = [] # 外部数据文件 (External data files)

        self.prompts = {} # 提示词 (Prompts)
        self.client = None # 这将是 DeepSeek 客户端 (This will be the DeepSeek client)


    def setup_logging(self, logging_level=logging.INFO): # 设置日志记录 (Set up logging)
        now = datetime.now()
        session_timestamp = now.strftime("%Y%m%d_%H%M%S")
        self.session_dir = os.path.join(self.session_base_dir, f"session_{session_timestamp}")
        os.makedirs(self.session_dir, exist_ok=True)
        self.log_file_path = os.path.join(self.session_dir, "session.log")

        root_logger = logging.getLogger()
        if root_logger.hasHandlers(): # 如果已有处理器，则移除 (If handlers already exist, remove them)
            for handler in root_logger.handlers[:]:
                root_logger.removeHandler(handler)
                handler.close()

        logging.basicConfig(
            level=logging_level,
            format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s',
            handlers=[
                logging.FileHandler(self.log_file_path, encoding='utf-8'), # 文件处理器 (File handler)
                logging.StreamHandler(sys.stdout) # 流处理器 (控制台) (Stream handler (console))
            ]
        )
        logging.info(f"日志记录已初始化。会话目录 (Logging initialized. Session directory): {self.session_dir}") # 中文翻译 (Chinese translation)
        logging.info(f"日志文件 (Log file): {self.log_file_path}") # 中文翻译 (Chinese translation)

    def _initialize_deepseek_client(self): # 初始化 DeepSeek 客户端 (Initialize DeepSeek client)
        if not self.deepseek_api_key:
            logging.critical("DEEPSEEK_API_KEY 环境变量未设置。(DEEPSEEK_API_KEY environment variable not set.)") # 中文翻译 (Chinese translation)
            raise ValueError("DEEPSEEK_API_KEY 环境变量未设置。(DEEPSEEK_API_KEY environment variable not set.)") # 中文翻译 (Chinese translation)
        try:
            self.client = openai.OpenAI(
                api_key=self.deepseek_api_key,
                base_url=self.deepseek_base_url,
                timeout=float(self.api_request_timeout_seconds) # 在客户端初始化时设置超时 (Set timeout during client initialization)
            )
            logging.info(f"DeepSeek 客户端初始化成功，连接至 (DeepSeek client initialized successfully, connected to) {self.deepseek_base_url}") # 中文翻译 (Chinese translation)
        except Exception as e:
            logging.critical(f"DeepSeek 客户端初始化期间出错 (Error during DeepSeek client initialization): {e}。请检查 API 密钥/URL 和网络连接。(Please check API key/URL and network connection.)") # 中文翻译 (Chinese translation)
            raise RuntimeError(f"DeepSeek 客户端初始化失败 (DeepSeek client initialization failed): {e}") # 中文翻译 (Chinese translation)

    def count_tokens(self, text: str) -> int: # 计算 Token 数量 (Count token quantity)
        if not text: return 0
        if self.encoder:
            try:
                return len(self.encoder.encode(text))
            except Exception as e:
                logging.warning(f"Tiktoken 编码失败 (Tiktoken encoding failed): {e}。回退到近似计算。(Falling back to approximate calculation.)") # 中文翻译 (Chinese translation)
                return len(text) // 4 # 近似计算 (Approximate calculation)
        logging.warning("Tiktoken 编码器不可用，Token 计数使用近似值 (长度 // 4)。(Tiktoken encoder not available, token count uses approximate value (length // 4).)") # 中文翻译 (Chinese translation)
        return len(text) // 4

# V11 新增类：ContextManager 用于防止上下文过载 (V11 New class: ContextManager to prevent context overload)
class ContextManager:
    def __init__(self, config: Config, style_guide: str, outline: dict, external_data: str):
        self.config = config
        self.style_guide = style_guide # 风格指南 (Style guide)
        self.outline = outline # 大纲 (Outline)
        self.external_data_summary = "" # 外部数据摘要 (External data summary)
        self.chapter_summaries = {} # 存储已完成主章节的摘要 (Store summaries of completed main chapters)
        self.introduction_summary = "" # 引言摘要 (Introduction summary)
        self.generated_subsection_content = collections.defaultdict(dict) # 存储已生成小节的完整内容 (Store full content of generated subsections)
                                                                        # 结构: self.generated_subsection_content[章节标题][小节标题] = 内容 (Structure: self.generated_subsection_content[chapter title][subsection title] = content)
        self.generated_chapter_content = {} # 存储主章节在小节合并（并可能进行章节级润色）后的完整内容 (Store full content of main chapters after subsection merging (and possible chapter-level polishing))
        self._summarize_external_data(external_data)

    def _summarize_external_data(self, external_data: str): # 摘要外部数据 (Summarize external data)
        if not external_data or external_data.isspace():
            logging.info("  上下文管理器：无外部数据需要摘要。(Context Manager: No external data to summarize.)") # 中文翻译 (Chinese translation)
            return
        logging.info("  上下文管理器：正在为上下文包摘要外部数据... (Context Manager: Summarizing external data for context package...)") # 中文翻译 (Chinese translation)
        truncated_data = truncate_text_for_context(self.config, external_data, 8000)
        prompt = f"请对以下参考资料进行密集、真实的摘要。重点关注可能与技术报告相关的关键定义、方程式和结论。保持在500字以内。\n\n---参考资料---\n{truncated_data}" # 中文提示词 (Chinese prompt)
        self.external_data_summary = call_ai(self.config, self.config.summary_model_name, [{"role": "user", "content": prompt}], max_tokens_output=1024)
        logging.info(f"  上下文管理器：外部数据摘要已创建 ({len(self.external_data_summary)} 字符)。(Context Manager: External data summary created ({len(self.external_data_summary)} characters).)") # 中文翻译 (Chinese translation)

    def record_completed_subsection(self, chapter_title: str, subsection_title: str, subsection_content: str): # 记录已完成的小节 (Record completed subsection)
        """记录已完成小节的完整内容。""" # 中文翻译 (Record the full content of the completed subsection.)
        logging.info(f"  上下文管理器：正在记录章节“{chapter_title}”中小节“{subsection_title}”的内容。(Context Manager: Recording content of subsection “{subsection_title}” in chapter “{chapter_title}”.)") # 中文翻译 (Chinese translation)
        self.generated_subsection_content[chapter_title][subsection_title] = subsection_content

    def get_previous_subsection_content(self, current_chapter_title: str, current_subsection_index: int) -> str: # 获取前一个小节的内容 (Get content of the previous subsection)
        """检索同一章节中当前小节紧邻的前一个小节的完整内容。""" # 中文翻译 (Retrieve the full content of the subsection immediately preceding the current subsection in the same chapter.)
        if current_subsection_index == 0:
            return "这是本章的第一个小节，没有直接的前文小节内容。"
        
        current_chapter_obj = None
        for ch in self.outline.get('outline', []):
            if ch.get('title') == current_chapter_title:
                current_chapter_obj = ch
                break
        
        if not current_chapter_obj or not current_chapter_obj.get("sections"):
            logging.warning(f"  上下文管理器：在 get_previous_subsection_content 中未能找到章节“{current_chapter_title}”或其小节。(Context Manager: Failed to find chapter “{current_chapter_title}” or its subsections in get_previous_subsection_content.)") # 中文翻译 (Chinese translation)
            return "错误：无法在提纲中定位当前章节或其小节。"

        subsections = current_chapter_obj["sections"]
        if current_subsection_index > 0 and current_subsection_index < len(subsections):
            previous_subsection_title = subsections[current_subsection_index - 1].get("title")
            if previous_subsection_title:
                content = self.generated_subsection_content.get(current_chapter_title, {}).get(previous_subsection_title)
                if content:
                    logging.info(f"  上下文管理器：已检索到前一个小节“{previous_subsection_title}”的内容。(Context Manager: Retrieved content of previous subsection “{previous_subsection_title}”.)") # 中文翻译 (Chinese translation)
                    return truncate_text_for_context(self.config, content, 3000, truncation_style="tail")
                else:
                    logging.warning(f"  上下文管理器：未找到前一个小节“{previous_subsection_title}”的内容。(Context Manager: Content of previous subsection “{previous_subsection_title}” not found.)") # 中文翻译 (Chinese translation)
                    return f"错误：未能找到前一个小节 '{previous_subsection_title}' 的内容。"
            else:
                logging.warning(f"  上下文管理器：未找到索引 {current_subsection_index - 1} 处前一个小节的标题。(Context Manager: Title of previous subsection at index {current_subsection_index - 1} not found.)") # 中文翻译 (Chinese translation)
                return "错误：未能获取前一个小节的标题。"
        else:
            logging.warning(f"  上下文管理器：章节“{current_chapter_title}”的当前小节索引 {current_subsection_index} 无效。(Context Manager: Current subsection index {current_subsection_index} for chapter “{current_chapter_title}” is invalid.)") # 中文翻译 (Chinese translation)
            return "错误：当前小节索引无效。"

    def update_completed_chapter_content(self, chapter_title: str, full_chapter_content: str): # 更新已完成章节的内容 (Update content of completed chapter)
        """存储主章节的完整合并（并可能经过章节级润色）内容，并生成其摘要。""" # 中文翻译 (Store the full merged (and possibly chapter-level polished) content of the main chapter and generate its summary.)
        logging.info(f"  上下文管理器：正在存储章节“{chapter_title}”的完整内容并生成摘要。(Context Manager: Storing full content of chapter “{chapter_title}” and generating summary.)") # 中文翻译 (Chinese translation)
        self.generated_chapter_content[chapter_title] = full_chapter_content
        
        content_for_summary = truncate_text_for_context(self.config, full_chapter_content, 6000)
        summary_prompt_text = f"请为以下标题为“{chapter_title}”的章节提供一个简洁的摘要（约200-300字）。重点关注本章内的关键论点、发现和结论。此摘要将用作撰写后续章节的上下文，因此需要信息丰富且简短。\n\n--- 章节内容开始 ---\n{content_for_summary}\n--- 章节内容结束 ---" # 中文提示词 (Chinese prompt)
        
        summary = call_ai(self.config, self.config.summary_model_name,
                          [{"role": "user", "content": summary_prompt_text}],
                          max_tokens_output=512)
        
        if "AI模型调用失败" in summary or not summary.strip():
            logging.warning(f"  上下文管理器：为章节“{chapter_title}”生成摘要失败。存储空摘要。AI 响应 (Context Manager: Failed to generate summary for chapter “{chapter_title}”. Storing empty summary. AI Response): {summary}") # 中文翻译 (Chinese translation)
            self.chapter_summaries[chapter_title] = "本章节摘要生成失败。" # 中文翻译 (Chinese translation)
        else:
            self.chapter_summaries[chapter_title] = summary.strip()
            logging.info(f"  上下文管理器：章节“{chapter_title}”的摘要已创建并存储 ({len(summary)} 字符)。(Context Manager: Summary for chapter “{chapter_title}” created and stored ({len(summary)} characters).)") # 中文翻译 (Chinese translation)

        if "introduction" in chapter_title.lower() or "引言" in chapter_title.lower(): # 检查是否为引言章节 (Check if it is an introduction chapter)
            self.introduction_summary = self.chapter_summaries[chapter_title]
            logging.info(f"  上下文管理器：已从章节“{chapter_title}”捕获引言摘要。(Context Manager: Captured introduction summary from chapter “{chapter_title}”.)") # 中文翻译 (Chinese translation)

    def get_context_for_subsection(self, current_chapter_title: str, current_subsection_index: int) -> str: # 获取小节的上下文 (Get context for subsection)
        """
        为生成特定小节组装上下文包，
        现在更符合“精确上下文”以实现章节级感知。
        """ # 中文翻译 (Assemble context package for generating specific subsection, now more in line with "precise context" to achieve chapter-level awareness.)
        logging.info(f"  上下文管理器：正在为章节“{current_chapter_title}”，小节索引 {current_subsection_index} 组装上下文。(Context Manager: Assembling context for chapter “{current_chapter_title}”, subsection index {current_subsection_index}.)") # 中文翻译 (Chinese translation)

        full_doc_outline_str = json.dumps(self.outline, ensure_ascii=False, indent=2)
        style_guide_str = self.style_guide if self.style_guide else "无特定风格指南。"
        
        all_chapter_titles = [ch.get('title', '未命名章节') for ch in self.outline.get('outline', [])] # 中文翻译 (Chinese translation)
        other_chapter_titles_list = [t for t in all_chapter_titles if t != current_chapter_title]
        other_chapter_titles_str = "\n - ".join(other_chapter_titles_list)
        if not other_chapter_titles_str: other_chapter_titles_str = "无其他章节。"

        current_chapter_obj = None
        main_chapter_index = -1
        outline_chapters = self.outline.get('outline', [])
        for idx, ch in enumerate(outline_chapters):
            if ch.get('title') == current_chapter_title:
                current_chapter_obj = ch
                main_chapter_index = idx
                break
        
        if not current_chapter_obj:
            logging.error(f"  上下文管理器：在提纲中未找到章节“{current_chapter_title}”。(Context Manager: Chapter “{current_chapter_title}” not found in outline.)") # 中文翻译 (Chinese translation)
            return "[错误：无法定位当前主章节信息]"
        
        current_chapter_desc_str = current_chapter_obj.get('description', '无详细描述。')
        
        prev_main_chapter_full_text_str = "这是报告的第一个主章节，没有前序主章节。"
        if main_chapter_index > 0:
            prev_main_chapter_title = outline_chapters[main_chapter_index - 1].get("title")
            if prev_main_chapter_title:
                prev_main_chapter_full_text_str = self.generated_chapter_content.get(prev_main_chapter_title, f"前一主章节“{prev_main_chapter_title}”的完整内容尚未生成或记录。") # 中文翻译 (Chinese translation)
                prev_main_chapter_full_text_str = truncate_text_for_context(self.config, prev_main_chapter_full_text_str, 4000, "tail")

        subsections = current_chapter_obj.get("sections", [])
        if not subsections or current_subsection_index >= len(subsections):
            logging.error(f"  上下文管理器：章节“{current_chapter_title}”的小节索引 {current_subsection_index} 超出范围或无小节。(Context Manager: Subsection index {current_subsection_index} for chapter “{current_chapter_title}” is out of range or no subsections.)") # 中文翻译 (Chinese translation)
            if not subsections: return f"[错误：章节“{current_chapter_title}”在大纲中没有定义小节，但请求了小节上下文]" # 中文翻译 (Chinese translation)
            return f"[错误：小节索引 {current_subsection_index} 超出范围]"
        
        content_of_chapter_N_so_far = []
        if current_subsection_index > 0:
            for sub_idx_prev in range(current_subsection_index):
                prev_sub_title_iter = subsections[sub_idx_prev].get("title")
                if prev_sub_title_iter:
                    sub_content = self.generated_subsection_content.get(current_chapter_title, {}).get(prev_sub_title_iter)
                    if sub_content:
                        content_of_chapter_N_so_far.append(f"--- 内容来自：{prev_sub_title_iter} ---\n{sub_content}")
        
        chapter_N_content_str = "\n\n".join(content_of_chapter_N_so_far)
        if not chapter_N_content_str:
            chapter_N_content_str = "这是本章的第一个小节，之前没有内容。"
        else:
            chapter_N_content_str = f"--- 本章《{current_chapter_title}》已生成内容开始 ---\n{chapter_N_content_str}\n--- 本章《{current_chapter_title}》已生成内容结束 ---"
        chapter_N_content_str = truncate_text_for_context(self.config, chapter_N_content_str, 4000, "tail")

        next_context_element_str = "这是本章的最后一个小节，没有后续小节需要铺垫。"
        if current_subsection_index < len(subsections) - 1:
            next_subsection_obj = subsections[current_subsection_index + 1]
            next_subsection_title = next_subsection_obj.get("title", "未命名小节")
            next_subsection_desc = next_subsection_obj.get("description", "无详细描述。")
            next_context_element_str = f"下一个小节《{next_subsection_title}》计划阐述：{next_subsection_desc}"
        elif main_chapter_index < len(outline_chapters) - 1:
            next_main_chapter_title = outline_chapters[main_chapter_index + 1].get("title")
            if next_main_chapter_title:
                summary_N_plus_1 = self.chapter_summaries.get(next_main_chapter_title, f"下一主章节“{next_main_chapter_title}”的摘要尚未生成。") # 中文翻译 (Chinese translation)
                next_context_element_str = f"完成本小节后，将结束章节《{current_chapter_title}》。\n下一主章节《{next_main_chapter_title}》的核心摘要：\n{summary_N_plus_1}"
        else:
             next_context_element_str = "这是报告的最后一个小节，之后是全文总结。"

        context_packet = f"""
[报告的完整大纲]
{full_doc_outline_str}

[风格与声音指南]
{style_guide_str}

[其他章节标题列表 (供结构参考)]
 - {other_chapter_titles_str}

[上一主章节《{outline_chapters[main_chapter_index - 1].get("title") if main_chapter_index > 0 else "N/A"}》的全文/核心内容回顾]
{prev_main_chapter_full_text_str}

[当前主章节《{current_chapter_title}》的核心目标]
{current_chapter_desc_str}

[当前主章节《{current_chapter_title}》已生成的小节内容（你正在续写）]
{chapter_N_content_str}

[为后续内容的铺垫信息]
{next_context_element_str}
"""
        logging.info(f"  上下文管理器：小节上下文包已创建。大约长度 {len(context_packet)} 字符。(Context Manager: Subsection context package created. Approximate length {len(context_packet)} characters.)") # 中文翻译 (Chinese translation)
        return context_packet

    def get_context_for_standalone_chapter(self, chapter_title: str) -> str: # 获取独立章节的上下文 (Get context for standalone chapter)
        """
        为生成一个没有小节、打算一次性写完的主章节组装上下文包。遵循“精确上下文”。
        上下文：完整的 N-1 章节，（N 章节正在编写中），N+1 章节的摘要，其他章节的标题列表。
        """ # 中文翻译 (Assemble context package for generating a main chapter that has no subsections and is intended to be written in one go. Follow "precise context". Context: full N-1 chapter, (N chapter is being written), summary of N+1 chapter, list of titles of other chapters.)
        logging.info(f"  上下文管理器：正在为独立章节“{chapter_title}”组装上下文。(Context Manager: Assembling context for standalone chapter “{chapter_title}”.)") # 中文翻译 (Chinese translation)

        full_doc_outline_str = json.dumps(self.outline, ensure_ascii=False, indent=2)
        style_guide_str = self.style_guide if self.style_guide else "无特定风格指南。"
        all_chapter_titles = [ch.get('title', '未命名章节') for ch in self.outline.get('outline', [])] # 中文翻译 (Chinese translation)
        other_chapter_titles_list = [t for t in all_chapter_titles if t != chapter_title]
        other_chapter_titles_str = "\n - ".join(other_chapter_titles_list)
        if not other_chapter_titles_str: other_chapter_titles_str = "无其他章节。"
        
        current_chapter_obj = None
        main_chapter_index = -1
        outline_chapters = self.outline.get('outline', [])
        for idx, ch in enumerate(outline_chapters):
            if ch.get('title') == chapter_title:
                current_chapter_obj = ch
                main_chapter_index = idx
                break
        
        if not current_chapter_obj:
            logging.error(f"  上下文管理器：在提纲中未找到独立章节“{chapter_title}”。(Context Manager: Standalone chapter “{chapter_title}” not found in outline.)") # 中文翻译 (Chinese translation)
            return "[错误：无法定位当前独立章节信息]"
        
        current_chapter_desc_str = current_chapter_obj.get('description', '无详细描述。')
        
        prev_main_chapter_full_text_str = "这是报告的第一个主章节，没有前序主章节。"
        if main_chapter_index > 0:
            prev_main_chapter_title = outline_chapters[main_chapter_index - 1].get("title")
            if prev_main_chapter_title:
                prev_main_chapter_full_text_str = self.generated_chapter_content.get(prev_main_chapter_title, f"前一主章节“{prev_main_chapter_title}”的内容尚未生成或记录。") # 中文翻译 (Chinese translation)
                prev_main_chapter_full_text_str = truncate_text_for_context(self.config, prev_main_chapter_full_text_str, 6000, "tail")

        summary_N_plus_1_str = "这是报告的最后一个主章节，没有后续主章节需要铺垫。"
        if main_chapter_index < len(outline_chapters) - 1:
            next_main_chapter_title = outline_chapters[main_chapter_index + 1].get("title")
            if next_main_chapter_title:
                summary_N_plus_1_str = self.chapter_summaries.get(next_main_chapter_title, f"下一主章节“{next_main_chapter_title}”的摘要尚未生成。") # 中文翻译 (Chinese translation)
                summary_N_plus_1_str = truncate_text_for_context(self.config, summary_N_plus_1_str, 1000)
        
        # 确保中文句号正确地作为字符串字面量的一部分。 (Ensure that Chinese periods are correctly part of the string literal.)
        context_packet = f"""
[报告的完整大纲]
{full_doc_outline_str}

[风格与声音指南]
{style_guide_str}

[其他章节标题列表 (供结构参考)]
 - {other_chapter_titles_str}

[【章节 N-1】上一主章节《{outline_chapters[main_chapter_index - 1].get("title") if main_chapter_index > 0 else "N/A"}》的完整内容回顾]
--- 前一章节内容开始 ---
{prev_main_chapter_full_text_str}
--- 前一章节内容结束 ---

[【章节 N】当前主章节《{chapter_title}》的核心目标与描述]
{current_chapter_desc_str}
重要提示: 你将一次性完成本章节的全部内容。

[【章节 N+1】下一主章节《{outline_chapters[main_chapter_index + 1].get("title") if main_chapter_index < len(outline_chapters) - 1 else "N/A"}》的核心摘要]
{summary_N_plus_1_str}
"""
        logging.info(f"  上下文管理器：独立章节上下文包已创建。大约长度 {len(context_packet)} 字符。(Context Manager: Standalone chapter context package created. Approximate length {len(context_packet)} characters.)") # 中文翻译 (Chinese translation)
        return context_packet

    def get_context_for_chapter_critique(self, chapter_title_being_critiqued: str, full_document_text: str) -> str: # 获取章节评论的上下文 (Get context for chapter critique)
        """
        为评论/修补特定章节 (N) 组装上下文包。
        上下文：完整的 N-1 章节，完整的 N 章节，N+1 章节的摘要，其他章节的标题列表。
        """ # 中文翻译 (Assemble context package for critiquing/patching a specific chapter (N). Context: full N-1 chapter, full N chapter, summary of N+1 chapter, list of titles of other chapters.)
        logging.info(f"  上下文管理器：正在为评论章节“{chapter_title_being_critiqued}”组装精确上下文。(Context Manager: Assembling precise context for critiquing chapter “{chapter_title_being_critiqued}”.)") # 中文翻译 (Chinese translation)

        full_doc_outline_str = json.dumps(self.outline, ensure_ascii=False, indent=2)
        style_guide_str = self.style_guide if self.style_guide else "无特定风格指南。"
        all_chapter_titles = [ch.get('title', '未命名章节') for ch in self.outline.get('outline', [])] # 中文翻译 (Chinese translation)
        other_chapter_titles_list = [t for t in all_chapter_titles if t != chapter_title_being_critiqued]
        other_chapter_titles_str = "\n - ".join(other_chapter_titles_list)
        if not other_chapter_titles_str: other_chapter_titles_str = "无其他章节。"

        chapter_N_obj, chapter_N_minus_1_obj, chapter_N_plus_1_obj = None, None, None
        chapter_N_index = -1
        outline_chapters = self.outline.get('outline', [])

        for idx, ch in enumerate(outline_chapters):
            if ch.get('title') == chapter_title_being_critiqued:
                chapter_N_obj = ch
                chapter_N_index = idx
                if idx > 0:
                    chapter_N_minus_1_obj = outline_chapters[idx-1]
                if idx < len(outline_chapters) - 1:
                    chapter_N_plus_1_obj = outline_chapters[idx+1]
                break
        
        if not chapter_N_obj:
            logging.error(f"  上下文管理器：在提纲中未找到用于评论上下文的章节“{chapter_title_being_critiqued}”。(Context Manager: Chapter “{chapter_title_being_critiqued}” for critique context not found in outline.)") # 中文翻译 (Chinese translation)
            return "[错误：无法定位被评审章节信息]"

        text_N_minus_1 = "这是报告的第一个主章节，没有前序主章节。"
        title_N_minus_1 = "N/A"
        if chapter_N_minus_1_obj:
            title_N_minus_1 = chapter_N_minus_1_obj.get("title", "未知前文章节")
            text_N_minus_1 = self.generated_chapter_content.get(title_N_minus_1, f"章节《{title_N_minus_1}》的内容尚未生成或记录。") # 中文翻译 (Chinese translation)
            text_N_minus_1 = truncate_text_for_context(self.config, text_N_minus_1, 6000, "middle")

        text_N = f"未能从文档中提取章节《{chapter_title_being_critiqued}》的完整内容。"
        escaped_title_N = re.escape(chapter_title_being_critiqued)
        # 修正后的正则表达式，减少贪婪匹配并处理标题中潜在的换行符（尽管标题中不太可能有） (Corrected regular expression to reduce greedy matching and handle potential newlines in titles (although unlikely in titles))
        pattern_N = re.compile(rf"^(##\s*{escaped_title_N}.*?)(?=^## |\Z)", re.MULTILINE | re.DOTALL)
        match_N = pattern_N.search(full_document_text)
        if match_N:
            text_N = match_N.group(1).strip()
            text_N = truncate_text_for_context(self.config, text_N, 8000, "middle")
        else:
            logging.warning(f"无法使用正则表达式从完整文档中提取章节 N：“{chapter_title_being_critiqued}”的内容。(Unable to extract content of chapter N: “{chapter_title_being_critiqued}” from full document using regular expression.)") # 中文翻译 (Chinese translation)

        summary_N_plus_1 = "这是报告的最后一个主章节，没有后续主章节需要铺垫。"
        title_N_plus_1 = "N/A"
        if chapter_N_plus_1_obj:
            title_N_plus_1 = chapter_N_plus_1_obj.get("title", "未知后续章节")
            summary_N_plus_1 = self.chapter_summaries.get(title_N_plus_1, f"章节《{title_N_plus_1}》的摘要尚未生成。") # 中文翻译 (Chinese translation)
            summary_N_plus_1 = truncate_text_for_context(self.config, summary_N_plus_1, 1500)

        context_packet = f"""
[报告的完整大纲]
{full_doc_outline_str}

[风格与声音指南]
{style_guide_str}

[其他章节标题列表 (供结构参考)]
 - {other_chapter_titles_str}

[【章节 N-1】《{title_N_minus_1}》的全文回顾]
--- 内容开始 ---
{text_N_minus_1}
--- 内容结束 ---

[【章节 N】《{chapter_title_being_critiqued}》的当前全文 (此为重点评审/修改对象)]
--- 内容开始 ---
{text_N}
--- 内容结束 ---

[【章节 N+1】《{title_N_plus_1}》的核心摘要]
--- 内容开始 ---
{summary_N_plus_1}
--- 内容结束 ---
"""
        logging.info(f"  上下文管理器：章节评论的精确上下文已创建。大约长度 {len(context_packet)} 字符。(Context Manager: Precise context for chapter critique created. Approximate length {len(context_packet)} characters.)") # 中文翻译 (Chinese translation)
        return context_packet
# --- 嵌入模型管理器 (用于通过 Gitee 访问 BGE-M3) --- (Embedding model manager (used to access BGE-M3 via Gitee))
class EmbeddingModel:
    def __init__(self, config: Config):
        self.config = config
        self.client = None
        self.model_name = config.embedding_model_name # 嵌入模型名称 (Embedding model name)

        if not config.embedding_api_base_url or not config.embedding_api_key:
            logging.warning("EMBEDDING_API_BASE_URL 或 EMBEDDING_API_KEY 未设置。嵌入功能将被禁用。(EMBEDDING_API_BASE_URL or EMBEDDING_API_KEY not set. Embedding functionality will be disabled.)") # 中文翻译 (Chinese translation)
            return

        try:
            self.client = openai.OpenAI(
                base_url=config.embedding_api_base_url,
                api_key=config.embedding_api_key,
                timeout=300.0 # 为嵌入操作也设置一个较宽松的超时时间 (Also set a looser timeout for embedding operations)
            )
            logging.info(f"嵌入客户端初始化成功，连接至 {config.embedding_api_base_url}，模型为 '{self.model_name}' (Embedding client initialized successfully, connected to {config.embedding_api_base_url}, model is '{self.model_name}')") # 中文翻译 (Chinese translation)
        except Exception as e:
            logging.error(f"初始化 Gitee 嵌入客户端时出错 (Error initializing Gitee embedding client): {e}") # 中文翻译 (Chinese translation)
            self.client = None

    def get_embeddings(self, texts: list[str]) -> list[list[float]]: # 获取嵌入向量列表 (Get list of embedding vectors)
        if not self.client:
            logging.error("嵌入客户端不可用。无法获取嵌入向量。(Embedding client not available. Cannot get embeddings.)") # 中文翻译 (Chinese translation)
            return [[] for _ in texts]

        all_embeddings = []
        for i in range(0, len(texts), self.config.embedding_batch_size):
            batch_texts = texts[i:i+self.config.embedding_batch_size]
            # try: # 原始的 try 语句块，现已被下面的 tenacity 逻辑取代 (Original try statement block, now replaced by the tenacity logic below)
            #     logging.info(f"  正在为 {len(batch_texts)} 个文本片段获取嵌入向量 (模型: {self.model_name})")
            #     response = self.client.embeddings.create(
            #         model=self.model_name,
            #         input=batch_texts
            #     )
            #     embeddings = [item.embedding for item in response.data]
            #     all_embeddings.extend(embeddings)
            #     logging.info(f"  成功从 Gitee 获取 {len(embeddings)} 个嵌入向量。")
            # # except openai.InternalServerError as e: # 旧的错误处理方式 (Old error handling method)
            #     logging.error(f"  Gitee 嵌入 API 返回 500 内部服务器错误: {e}。该服务可能暂时不可用。跳过此批次。", exc_info=True)
            #     all_embeddings.extend([[] for _ in batch_texts]) # 保留此行作为后备 (Keep this line as a fallback)
            # except Exception as e: # 旧的错误处理方式 (Old error handling method)
            #     logging.error(f"  从 Gitee API 获取嵌入向量失败: {e}", exc_info=True)
            #     all_embeddings.extend([[] for _ in batch_texts]) # 保留此行作为后备 (Keep this line as a fallback)
            
            # V11.1: 使用 Tenacity 增强批处理嵌入的健壮性 (V11.1: Use Tenacity to enhance the robustness of batch embedding)
            try:
                retryer = tenacity.Retrying(
                    wait=tenacity.wait_exponential(
                        multiplier=self.config.api_retry_wait_multiplier,
                        min=2, # 最小等待时间 (Minimum wait time)
                        max=self.config.api_retry_max_wait # 最大等待时间 (Maximum wait time)
                    ),
                    stop=tenacity.stop_after_attempt(self.config.api_retry_max_attempts), # 最大尝试次数 (Maximum attempts)
                    retry=( # 重试条件 (Retry condition)
                        tenacity.retry_if_exception_type(openai.APITimeoutError) |
                        tenacity.retry_if_exception_type(openai.APIConnectionError) |
                        tenacity.retry_if_exception_type(openai.InternalServerError) |
                        tenacity.retry_if_exception_type(openai.RateLimitError)
                    ),
                    before_sleep=tenacity.before_sleep_log(logging.getLogger(__name__), logging.WARNING), # 重试前记录日志 (Log before retry)
                    reraise=True # 如果所有重试都失败，则重新引发异常 (If all retries fail, re-raise the exception)
                )
                response = retryer(
                    self.client.embeddings.create, # 调用绑定的方法 (Call the bound method)
                    model=self.model_name,
                    input=batch_texts
                )
                embeddings = [item.embedding for item in response.data]
                all_embeddings.extend(embeddings)
                logging.info(f"  成功为批次 {i // self.config.embedding_batch_size + 1} 从 Gitee 获取 {len(embeddings)} 个嵌入向量。(Successfully retrieved {len(embeddings)} embeddings from Gitee for batch {i // self.config.embedding_batch_size + 1}.)") # 中文翻译 (Chinese translation)
            except Exception as e_retry:
                logging.error(f"  在重试后未能为批次 {i // self.config.embedding_batch_size + 1} 获取嵌入向量 (Failed to get embeddings for batch {i // self.config.embedding_batch_size + 1} after retries): {e_retry}", exc_info=True) # 中文翻译 (Chinese translation)
                all_embeddings.extend([[] for _ in batch_texts]) # 为失败的批次添加空列表 (Add empty list for failed batch)
        return all_embeddings

    def get_embedding(self, text: str) -> list[float]: # 获取单个文本的嵌入向量 (Get embedding vector for a single text)
        embeddings = self.get_embeddings([text])
        return embeddings[0] if embeddings and embeddings[0] else []


# --- 向量数据库管理器 --- (Vector database manager)
class VectorDBManager:
    def __init__(self, config: Config, embedding_model: EmbeddingModel): # 初始化 (Initialize)
        self.config = config
        self.embedding_model = embedding_model
        self.client = None
        self.collection = None # 集合对象 (Collection object)
        self._initialize_db() # 初始化数据库 (Initialize database)

    def _initialize_db(self): # 初始化数据库 (Initialize database)
        try:
            logging.info(f"正在初始化 ChromaDB，路径 (Initializing ChromaDB, path): {self.config.vector_db_path}") # 中文翻译 (Chinese translation)
            self.client = chromadb.PersistentClient(path=self.config.vector_db_path) # 持久化客户端 (Persistent client)
            self.collection = self.client.get_or_create_collection(
                name=self.config.vector_db_collection_name # 获取或创建集合 (Get or create collection)
            )
            logging.info(f"ChromaDB 集合 '{self.config.vector_db_collection_name}' 已加载/创建。(ChromaDB collection '{self.config.vector_db_collection_name}' loaded/created.)") # 中文翻译 (Chinese translation)
            self.get_db_stats() # 获取数据库统计信息 (Get database statistics)
        except Exception as e:
            logging.error(f"初始化 ChromaDB 失败 (Failed to initialize ChromaDB): {e}", exc_info=True) # 中文翻译 (Chinese translation)
            self.client = None
            self.collection = None

    def add_experience(self, texts: list[str], metadatas: list[dict] | None = None, ids: list[str] | None = None) -> bool: # 添加经验 (Add experience)
        if not self.collection or not self.embedding_model:
            logging.error("向量数据库或嵌入模型未初始化。无法添加经验。(Vector database or embedding model not initialized. Cannot add experience.)") # 中文翻译 (Chinese translation)
            return False
        if not texts:
            logging.warning("试图向经验库添加一个空文本列表。(Attempting to add an empty list of texts to the experience base.)") # 中文翻译 (Chinese translation)
            return False

        if metadatas and len(texts) != len(metadatas):
            logging.error("文本列表和元数据列表的长度必须相同。(Length of text list and metadata list must be the same.)") # 中文翻译 (Chinese translation)
            return False
        if ids and len(texts) != len(ids):
            logging.error("文本列表和ID列表的长度必须相同。(Length of text list and ID list must be the same.)") # 中文翻译 (Chinese translation)
            return False

        try:
            embeddings = self.embedding_model.get_embeddings(texts) # 获取嵌入向量 (Get embedding vectors)

            valid_texts, valid_embeddings, valid_metadatas, valid_ids = [], [], [], [] # 有效的文本、嵌入、元数据和ID (Valid texts, embeddings, metadata, and IDs)

            for i, emb in enumerate(embeddings):
                if emb: # 如果嵌入向量有效 (If embedding vector is valid)
                    valid_texts.append(texts[i])
                    valid_embeddings.append(emb)
                    if metadatas: valid_metadatas.append(metadatas[i])
                    if ids: valid_ids.append(ids[i])
                    else: valid_ids.append(f"exp_{hashlib.md5(texts[i].encode()).hexdigest()}_{int(time.time())}") # 生成默认ID (Generate default ID)
                else:
                    logging.warning(f"未能为文本生成嵌入向量，跳过添加 (Failed to generate embedding for text, skipping addition): {texts[i][:100]}...") # 中文翻译 (Chinese translation)

            if not valid_texts:
                logging.warning("未生成有效的嵌入向量。不会添加新的经验。(No valid embeddings generated. No new experiences will be added.)") # 中文翻译 (Chinese translation)
                return False

            logging.info(f"正在向向量数据库添加 {len(valid_texts)} 条经验... (Adding {len(valid_texts)} experiences to vector database...)") # 中文翻译 (Chinese translation)
            self.collection.add(
                embeddings=valid_embeddings,
                documents=valid_texts,
                metadatas=valid_metadatas if valid_metadatas else None,
                ids=valid_ids
            )
            logging.info(f"成功向数据库添加 {len(valid_texts)} 条经验。(Successfully added {len(valid_texts)} experiences to database.)") # 中文翻译 (Chinese translation)
            self.get_db_stats()
            return True
        except Exception as e:
            logging.error(f"向向量数据库添加经验失败 (Failed to add experience to vector database): {e}", exc_info=True) # 中文翻译 (Chinese translation)
            return False

    def retrieve_experience(self, query_text: str, n_results: int = -1, where_filter: dict | None = None) -> list[dict]: # 检索经验 (Retrieve experience)
        if not self.collection or not self.embedding_model:
            logging.error("向量数据库或嵌入模型未初始化。无法检索经验。(Vector database or embedding model not initialized. Cannot retrieve experience.)") # 中文翻译 (Chinese translation)
            return []
        if n_results == -1: n_results = self.config.num_retrieved_experiences # 如果未指定结果数量，则使用配置中的默认值 (If the number of results is not specified, use the default value in the configuration)

        try:
            logging.info(f"正在从向量数据库检索与查询相关的经验: '{query_text[:100]}...' (前 {n_results} 条) (Retrieving experiences related to query from vector database: '{query_text[:100]}...' (top {n_results} items))") # 中文翻译 (Chinese translation)
            query_embedding = self.embedding_model.get_embedding(query_text) # 获取查询文本的嵌入向量 (Get embedding vector for query text)
            if not query_embedding:
                logging.error("未能为查询文本生成嵌入向量。无法检索。(Failed to generate embedding for query text. Cannot retrieve.)") # 中文翻译 (Chinese translation)
                return []

            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results,
                where=where_filter, # 可选的元数据过滤条件 (Optional metadata filter condition)
                include=['documents', 'metadatas', 'distances'] # 返回结果中包含的内容 (Content included in the returned results)
            )

            retrieved_experiences = [] # 检索到的经验列表 (List of retrieved experiences)
            if results and results.get('ids') and results.get('ids')[0]:
                for i in range(len(results['ids'][0])):
                    exp = {
                        "id": results['ids'][0][i],
                        "document": results['documents'][0][i] if results.get('documents') and results['documents'][0] else None,
                        "metadata": results['metadatas'][0][i] if results.get('metadatas') and results['metadatas'][0] else None,
                        "distance": results['distances'][0][i] if results.get('distances') and results['distances'][0] else None,
                    }
                    retrieved_experiences.append(exp)
                logging.info(f"成功检索到 {len(retrieved_experiences)} 条经验。(Successfully retrieved {len(retrieved_experiences)} experiences.)") # 中文翻译 (Chinese translation)
            else:
                logging.info("未找到相关的经验。(No relevant experiences found.)") # 中文翻译 (Chinese translation)
            return retrieved_experiences
        except Exception as e:
            logging.error(f"从向量数据库检索经验失败 (Failed to retrieve experience from vector database): {e}", exc_info=True) # 中文翻译 (Chinese translation)
            return []

    def get_db_stats(self): # 获取数据库统计信息 (Get database statistics)
        if not self.collection:
            logging.info("向量数据库集合未初始化。无法获取统计信息。(Vector database collection not initialized. Cannot get statistics.)") # 中文翻译 (Chinese translation)
            return {}
        try:
            count = self.collection.count() # 获取集合中的条目数 (Get the number of entries in the collection)
            logging.info(f"向量数据库 '{self.config.vector_db_collection_name}' 当前包含 {count} 条经验。(Vector database '{self.config.vector_db_collection_name}' currently contains {count} experiences.)") # 中文翻译 (Chinese translation)
            return {"count": count}
        except Exception as e:
            logging.error(f"获取向量数据库统计信息失败 (Failed to get vector database statistics): {e}", exc_info=True) # 中文翻译 (Chinese translation)
            return {}

# --- 使用工具调用生成文档大纲 (初始) --- (Generate document outline using tool call (initial))
def generate_document_outline_with_tools(config: Config, problem_statement: str) -> dict | None:
    logging.info(f"\n--- 正在为以下问题生成文档大纲: {problem_statement[:100]}... (使用工具调用) --- (Generating document outline for the following problem: {problem_statement[:100]}... (using tool call)) ---") # 中文翻译 (Chinese translation)
    
    tools = get_initial_outline_tool_definition() # 通常在 __main__ 块之后定义 (Usually defined after the __main__ block)
    
    outline_prompt = f"""
    分析以下用户请求并创建一个详细的文档大纲。
    最终报告的长度应约为 {config.initial_solution_target_chars} 个字符。
    您必须使用 `create_initial_document_outline` 工具来构建您的响应。

    用户请求: "{problem_statement}"
    """ # 中文提示词 (部分保留英文工具名和变量) (Chinese prompt (partially retaining English tool names and variables))
    messages = [{"role": "system", "content": "你是一位结构分析师，使用 `create_initial_document_outline` 工具创建文档大纲。"}, # 中文翻译 (Chinese translation)
                {"role": "user", "content": outline_prompt}]
    
    try:
        response = config.client.chat.completions.create(
            model=config.outline_model_name,
            messages=messages,
            tools=tools,
            tool_choice={"type": "function", "function": {"name": "create_initial_document_outline"}}, # 强制使用此工具 (Force use of this tool)
            temperature=0.05 # 为大纲生成设置略微更确定的温度 (Set a slightly more deterministic temperature for outline generation)
        )

        response_message = response.choices[0].message
        tool_calls = response_message.tool_calls

        if not tool_calls:
            logging.error("generate_document_outline_with_tools: AI 未按预期返回工具调用。(generate_document_outline_with_tools: AI did not return tool call as expected.)") # 中文翻译 (Chinese translation)
            return None
        
        function_args_str = tool_calls[0].function.arguments
        parsed_outline_data = json.loads(function_args_str) # 应该是有效的 JSON (Should be valid JSON)

        if not parsed_outline_data.get("outline") or not parsed_outline_data.get("title"):
             logging.error(f"从工具调用生成的 JSON 大纲缺少 'title' 或 'outline' 键。获取到 (JSON outline generated from tool call is missing 'title' or 'outline' key. Got): {parsed_outline_data}") # 中文翻译 (Chinese translation)
             return None

        logging.info(f"--- 通过工具调用生成的文档大纲 ({len(parsed_outline_data.get('outline',[]))} 个主要章节) --- (Document outline generated via tool call ({len(parsed_outline_data.get('outline',[]))} main chapters)) ---") # 中文翻译 (Chinese translation)
        if config.session_dir: # 保存原始工具输出以供调试 (Save original tool output for debugging)
            with open(os.path.join(config.session_dir, "generated_document_outline_tool_call.json"), "w", encoding="utf-8") as f:
                json.dump(parsed_outline_data, f, ensure_ascii=False, indent=4)
        return parsed_outline_data
        
    except Exception as e:
        logging.error(f"  使用工具生成文档大纲时出错 (Error generating document outline using tools): {e}", exc_info=True) # 中文翻译 (Chinese translation)
        return None

# V10 修改：为响应为空的情况添加了简单的重试逻辑 (V10 Modification: Added simple retry logic for empty response)
# V11.1 修改：使用 Tenacity 增强了重试机制的健壮性 (V11.1 Modification: Used Tenacity to enhance the robustness of the retry mechanism)
def call_ai_core(config: Config, model_name: str, messages: list, temperature: float,
                 effective_max_output_tokens: int, top_p: float,
                 frequency_penalty: float, presence_penalty: float) -> str:
    """核心 AI 调用逻辑，旨在由 tenacity 包装。""" # 中文翻译 (Core AI call logic, intended to be wrapped by tenacity.)
    client = config.client
    start_time = time.perf_counter() # 开始计时 (Start timing)
    
    response = client.chat.completions.create(
        model=model_name, messages=messages, temperature=temperature,
        max_tokens=effective_max_output_tokens, top_p=top_p,
        frequency_penalty=frequency_penalty, presence_penalty=presence_penalty
    )
    duration = time.perf_counter() - start_time # 计算耗时 (Calculate time consumption)
    content = response.choices[0].message.content
    
    # V12: <think> tag stripping for deepseek-reasoner
    final_content = content
    is_reasoner_model_for_stripping = (
        model_name == "deepseek-reasoner" or
        model_name == config.main_ai_model_heavy or
        model_name == config.secondary_ai_model
    )

    if is_reasoner_model_for_stripping:
        if content: # Only process if content is not None
            think_match = re.search(r"<think>([\s\S]*?)</think>", content, re.DOTALL)
            if think_match:
                thought_process = think_match.group(1).strip()
                logging.info(f"    - [DeepSeek Reasoner] Thought Process Extracted ({len(thought_process)} chars): {thought_process[:500]}...")
                final_content = content.replace(think_match.group(0), "").strip()
                logging.info(f"    - [DeepSeek Reasoner] Content after stripping <think> block ({len(final_content)} chars). Preview: {final_content[:300]}...")
            else:
                logging.info(f"    - [DeepSeek Reasoner] No <think> block found in response. Content length {len(content) if content else 'N/A'}.")
        else:
            logging.info(f"    - [DeepSeek Reasoner] Initial content from API was None or empty before think tag stripping.")

    logging.info(f"    - API Call successful ({duration:.2f}s), Model: {model_name}, Raw response length: {len(content) if content else 0} chars, Final content length: {len(final_content) if final_content else 0} chars, Total Tokens: {response.usage.total_tokens if response.usage else 'N/A'}.")

    if not final_content or final_content.isspace():
        logging.warning(f"    - AI call returned empty or whitespace-only content after potential <think> stripping. Model: {model_name}")
        if is_reasoner_model_for_stripping:
            # V12: Raise custom error for tenacity to catch and retry for empty reasoner responses
            raise EmptyResponseFromReasonerError(f"Empty or whitespace-only content from {model_name} after stripping <think> tags.")
    return final_content if final_content else ""


# Custom exception for retry logic
class EmptyResponseFromReasonerError(Exception):
    pass

def call_ai(config: Config, model_name: str, messages: list, temperature: float = 0.05,
            max_tokens_output: int = -1, top_p: float = 0.95, frequency_penalty: float = 0.2,
            presence_penalty: float = 0.05) -> str:

    # V12: Adjust max_tokens_output for deepseek-reasoner
    is_reasoner_model = (
        model_name == "deepseek-reasoner" or
        model_name == config.main_ai_model_heavy or # if main_ai_model_heavy is 'deepseek-reasoner'
        model_name == config.secondary_ai_model # if secondary_ai_model is 'deepseek-reasoner'
    )

    reasoner_min_tokens = 2048 # Minimum tokens to request for reasoner to allow for CoT
    
    if is_reasoner_model:
        if max_tokens_output == -1 or max_tokens_output < reasoner_min_tokens:
            logging.info(f"    - Adjusting max_tokens_output for {model_name} from {max_tokens_output} to {reasoner_min_tokens} (to accommodate CoT).")
            max_tokens_output = reasoner_min_tokens
    elif max_tokens_output == -1: # For non-reasoner models, use default if -1
        max_tokens_output = config.max_chunk_tokens

    # 模型上下文限制和特定最大输出 Token 逻辑 (针对新模型角色更新) (Model context limits and specific maximum output token logic (updated for new model roles))
    model_context_limits = {
        "deepseek-reasoner": 64000,  # 官方: 64k (旧配置中为 128k，可能是笔误) (Official: 64k (was 128k in old configuration, possibly a typo))
        "deepseek-chat":   64000,  # 官方: 64k (Official: 64k)
        "deepseek-coder": 128000,  # 官方: 128k (Official: 128k)
    }
    model_context_limits[config.main_ai_model] = model_context_limits.get(config.main_ai_model, 64000)
    model_context_limits[config.main_ai_model_heavy] = model_context_limits.get(config.main_ai_model_heavy, 64000)
    model_context_limits[config.secondary_ai_model] = model_context_limits.get(config.secondary_ai_model, 64000)
    model_context_limits[config.summary_model_name] = model_context_limits.get(config.summary_model_name, 128000 if config.summary_model_name == "deepseek-coder" else 32000)
    model_context_limits[config.researcher_model_name] = model_context_limits.get(config.researcher_model_name, 64000)
    model_context_limits[config.outline_model_name] = model_context_limits.get(config.outline_model_name, 128000)
    model_context_limits[config.planning_review_model_name] = model_context_limits.get(config.planning_review_model_name, 128000)
    model_context_limits[config.json_fixer_model_name] = model_context_limits.get(config.json_fixer_model_name, 128000)
    model_context_limits[config.editorial_model_name] = model_context_limits.get(config.editorial_model_name, 64000)

    if model_name not in model_context_limits:
        logging.warning(f"模型 {model_name} 未在显式 context_limits 中定义，默认为 32000。") # 中文翻译 (Chinese translation)
        model_context_limits[model_name] = 32000 

    # V12: Updated model_specific_max_output for reasoner
    model_specific_max_output = { 
        "deepseek-reasoner": 4096, # V12: Increased default for reasoner to ensure space for CoT + answer
        "deepseek-chat":    8192, 
        "deepseek-coder":   8192, 
    }
    # Populate with configured models, ensuring reasoner models get the higher default
    reasoner_models = {config.main_ai_model_heavy, config.secondary_ai_model, config.researcher_model_name}
    
    model_specific_max_output[config.main_ai_model] = model_specific_max_output.get(config.main_ai_model, 8192) # Default for chat-like
    if config.main_ai_model_heavy in reasoner_models:
        model_specific_max_output[config.main_ai_model_heavy] = model_specific_max_output.get(config.main_ai_model_heavy, 4096)
    else: # If not a known reasoner, use a general default
        model_specific_max_output[config.main_ai_model_heavy] = model_specific_max_output.get(config.main_ai_model_heavy, 8192)

    if config.secondary_ai_model in reasoner_models:
        model_specific_max_output[config.secondary_ai_model] = model_specific_max_output.get(config.secondary_ai_model, 4096) # Critic might be reasoner
    else:
        model_specific_max_output[config.secondary_ai_model] = model_specific_max_output.get(config.secondary_ai_model, 8192)


    model_specific_max_output[config.summary_model_name] = model_specific_max_output.get(config.summary_model_name, 4096) 
    
    if config.researcher_model_name in reasoner_models:
         model_specific_max_output[config.researcher_model_name] = model_specific_max_output.get(config.researcher_model_name, 4096) # Researcher might be reasoner
    else:
        model_specific_max_output[config.researcher_model_name] = model_specific_max_output.get(config.researcher_model_name, 2048)

    model_specific_max_output[config.outline_model_name] = model_specific_max_output.get(config.outline_model_name, 8192) 
    model_specific_max_output[config.planning_review_model_name] = model_specific_max_output.get(config.planning_review_model_name, 8192)
    model_specific_max_output[config.json_fixer_model_name] = model_specific_max_output.get(config.json_fixer_model_name, 8192)
    model_specific_max_output[config.editorial_model_name] = model_specific_max_output.get(config.editorial_model_name, 8192) 

    if model_name not in model_specific_max_output:
        # This case should be less likely now with more comprehensive population
        logging.warning(f"模型 {model_name} 未在更新后的 specific_max_output 中定义，默认为 4096。")
        model_specific_max_output[model_name] = 4096 
    
    # Determine the effective max output tokens
    # If user specified a value, use it, but cap it by model_specific_max_output.
    # If user did not specify (-1), use the model_specific_max_output.
    if max_tokens_output > 0 : # User specified a value
        effective_max_output_tokens = min(max_tokens_output, model_specific_max_output.get(model_name, 4096))
    else: # User did not specify (-1), so use the default for that model
        effective_max_output_tokens = model_specific_max_output.get(model_name, 4096)

    # If it's a reasoner model, ensure the effective_max_output_tokens is at least `reasoner_min_tokens`
    if is_reasoner_model and effective_max_output_tokens < reasoner_min_tokens:
        logging.info(f"    - Forcing effective_max_output_tokens for {model_name} to {reasoner_min_tokens} (was {effective_max_output_tokens}).")
        effective_max_output_tokens = reasoner_min_tokens
    
    total_input_tokens = sum(config.count_tokens(m['content']) for m in messages) # 计算总输入 Token 数 (Calculate total input token count)
    logging.info(f"    - AI 调用: 模型={model_name}, 输入 Tokens (估计): {total_input_tokens}, 请求输出 Tokens: {max_tokens_output} -> 有效最大值: {effective_max_output_tokens}") # 中文翻译 (Chinese translation)

    model_context_limit = model_context_limits.get(model_name, 32000) # 获取模型上下文限制 (Get model context limit)
    if total_input_tokens + effective_max_output_tokens > model_context_limit: # 如果输入+输出可能超出限制 (If input + output may exceed limit)
        logging.warning(f"    - 警告: 输入+输出 Tokens ({total_input_tokens + effective_max_output_tokens}) 可能超过模型 {model_name} 的上下文限制 ({model_context_limit})。") # 中文翻译 (Chinese translation)
        available_for_output = model_context_limit - total_input_tokens # 可用于输出的 Token 数 (Number of tokens available for output)
        if available_for_output < effective_max_output_tokens:
            new_max_output = max(100, available_for_output - 50) # 确保至少有 100 个 Token 用于输出，并留有余地 (Ensure at least 100 tokens for output, and leave some margin)
            logging.info(f"    - 调整 max_tokens_output 从 {effective_max_output_tokens} 到 {new_max_output} 以适应上下文。") # 中文翻译 (Chinese translation)
            effective_max_output_tokens = new_max_output
        if effective_max_output_tokens <= 0: # 防御性检查，尽管 max(100, ...) 应该能捕获 (Defensive check, although max(100, ...) should catch it)
            logging.error(f"    - 严重错误: 模型 {model_name} 没有可用的输出 Tokens。输入 Tokens: {total_input_tokens}, 上下文限制: {model_context_limit}") # 中文翻译 (Chinese translation)
            return "AI模型调用失败 (错误): 输入内容已占满上下文窗口，无法生成回复。" # 中文翻译 (Chinese translation)

    # 使用配置中的参数定义重试器 (Define retrier using parameters from configuration)
    retryer = tenacity.Retrying(
        wait=tenacity.wait_exponential(
            multiplier=config.api_retry_wait_multiplier, # 等待时间的乘数因子 (Multiplier factor for wait time)
            min=2,  # 最小等待2秒 (Minimum wait 2 seconds)
            max=config.api_retry_max_wait # 最大等待时间 (Maximum wait time)
        ),
        stop=tenacity.stop_after_attempt(config.api_retry_max_attempts), # 最大尝试次数后停止 (Stop after maximum attempts)
        retry=( # 定义重试的条件 (捕获特定类型的 OpenAI 异常) (Define retry conditions (catch specific types of OpenAI exceptions))
            tenacity.retry_if_exception_type(openai.APITimeoutError) |
            tenacity.retry_if_exception_type(openai.APIConnectionError) |
            tenacity.retry_if_exception_type(openai.InternalServerError) | # HTTP 500
            tenacity.retry_if_exception_type(openai.RateLimitError) | # HTTP 429
            tenacity.retry_if_exception_type(EmptyResponseFromReasonerError) # V12: Retry on custom empty response error
        ),
        before_sleep=tenacity.before_sleep_log(logging.getLogger(__name__), logging.WARNING), 
        reraise=True 
    )

    try:
        return retryer(call_ai_core, config, model_name, messages, temperature,
                       effective_max_output_tokens, top_p, frequency_penalty, presence_penalty)
    except openai.APIStatusError as e: # 捕获 tenacity 默认重试类型未覆盖的特定状态错误 (Catch specific status errors not covered by tenacity's default retry types)
        logging.error(f"    - 模型 {model_name} 的 API 调用状态错误 (未重试或最终尝试失败): {e.status_code} - {e.response.text if e.response else '无响应文本'}") # 中文翻译 (Chinese translation)
        if e.status_code == 400:
            logging.error(f"提示：请求可能无效 (例如，输入Token {total_input_tokens} + 输出 {effective_max_output_tokens} 超出模型限制)。这是一个不可重试的客户端错误。") # 中文翻译 (Chinese translation)
        error_message_detail = "未知错误" # 中文翻译 (Chinese translation)
        if e.response is not None:
            try:
                error_message_detail = e.response.json().get('error',{}).get('message','未知错误') # 中文翻译 (Chinese translation)
            except json.JSONDecodeError: # 如果响应不是 JSON (If response is not JSON)
                error_message_detail = e.response.text if e.response.text else "无响应文本" # 中文翻译 (Chinese translation)
        return f"AI模型调用失败 (API 错误 {e.status_code}): {error_message_detail}" # 中文翻译 (Chinese translation)
    except Exception as e:
        # 此处捕获 tenacity 重新引发的错误或其他意外错误 (Catch errors re-raised by tenacity or other unexpected errors here)
        logging.error(f"    - 模型 {model_name} 的 AI 调用因未处理的异常或所有重试后失败: {e}", exc_info=True) # 中文翻译 (Chinese translation)
        return "AI模型调用失败，请检查网络连接或API密钥/设置，或查看详细日志。" # 中文翻译 (Chinese translation)


def load_external_data(config: Config, file_paths: list[str]) -> str: # 加载外部数据 (Load external data)
    if not file_paths: return ""
    all_content = []
    for fp in file_paths:
        if not fp or not os.path.exists(fp):
            logging.warning(f"外部数据文件未找到: {fp}") # 中文翻译 (Chinese translation)
            continue
        ext = os.path.splitext(fp)[1].lower() # 文件扩展名 (File extension)
        content = ""
        try:
            if ext == '.txt':
                with open(fp, 'r', encoding='utf-8') as f: content = f.read()
            elif ext == '.pdf':
                with fitz.open(fp) as doc: content = "".join(page.get_text() for page in doc)
            else:
                logging.warning(f"不支持的文件类型: {ext} (文件: {fp})。跳过。") # 中文翻译 (Chinese translation)
                continue
            logging.info(f"成功读取 {ext.upper()} 文件: {fp} ({len(content)} 字符)") # 中文翻译 (Chinese translation)
            all_content.append(f"\n--- 文件开始: {os.path.basename(fp)} ---\n{content}\n--- 文件结束: {os.path.basename(fp)} ---\n")
        except Exception as e:
            logging.error(f"读取外部数据文件 {fp} 时出错: {e}") # 中文翻译 (Chinese translation)
    return "\n\n===== 新文件分隔符 =====\n\n".join(all_content)


# --- RAG 辅助函数：按大纲对文档进行分块 --- (RAG helper function: Chunk document by outline)
def chunk_document_for_rag(config: Config, document_text: str, document_outline_data: dict, doc_id: str) -> tuple[list[str], list[dict]]:
    """
    根据文档大纲结构为 RAG 对文档进行分块。
    每个块都关联元数据，将其链接到其章节和小节。
    """ # 中文翻译 (Chunk document for RAG based on document outline structure. Each chunk is associated with metadata, linking it to its chapter and subsection.)
    logging.info(f"  正在基于大纲为 RAG 对文档 (doc_id: {doc_id}) 进行分块...") # 中文翻译 (Chinese translation)
    chunks = [] # 文本块列表 (List of text chunks)
    metadatas = [] # 元数据列表 (List of metadata)
    
    if not document_text or not document_outline_data or not document_outline_data.get("outline"):
        logging.warning("  chunk_document_for_rag: 文档文本或大纲为空/无效。返回空块。") # 中文翻译 (Chinese translation)
        return [], []

    # 简单的文本分割辅助函数 (可以更复杂，例如使用 tiktoken 按 Token 数量分割) (Simple text splitting helper function (can be more complex, e.g., split by token count using tiktoken))
    def split_text_into_chunks(text_to_split: str, max_chars_per_chunk: int, overlap_chars: int) -> list[str]:
        if not text_to_split: return []
        if len(text_to_split) <= max_chars_per_chunk: return [text_to_split]
        
        split_chunks = []
        start = 0
        while start < len(text_to_split):
            end = min(start + max_chars_per_chunk, len(text_to_split))
            chunk = text_to_split[start:end]
            split_chunks.append(chunk)
            if end == len(text_to_split): break
            start += (max_chars_per_chunk - overlap_chars) # 移动起始点，考虑重叠 (Move starting point, considering overlap)
            if start >= len(text_to_split): break # 如果重叠过大或块大小过小，避免无限循环 (If overlap is too large or chunk size is too small, avoid infinite loop)
        return split_chunks

    processed_chapters_count = 0 # 已处理章节计数 (Processed chapter count)
    processed_sections_count = 0 # 已处理小节计数 (Processed section count)
    total_chunks_generated = 0 # 生成的总块数 (Total number of chunks generated)

    # 遍历主要章节 (Iterate through main chapters)
    for chapter_idx, chapter_item in enumerate(document_outline_data.get("outline", [])):
        chapter_title = chapter_item.get("title", f"未命名章节 {chapter_idx+1}") # 中文翻译 (Chinese translation)
        chapter_header = f"## {chapter_title}" # 章节标题标记 (Chapter title marker)
        escaped_chapter_title = re.escape(chapter_title) # 转义标题中的特殊字符以便用于正则表达式 (Escape special characters in title for use in regular expression)
        
        # 用于查找当前章节内容的正则表达式。它在下一个章节或文档末尾停止。 (Regular expression to find current chapter content. It stops at the next chapter or end of document.)
        # 假设章节严格按顺序排列，并由 "## " 清晰分隔。 (Assume chapters are strictly ordered and clearly separated by "## ".)
        chapter_content_match = re.search(
            rf"^(##\s*{escaped_chapter_title}.*?)(?=^##\s+[^#]|\Z)",
            document_text,
            re.MULTILINE | re.DOTALL # 多行匹配和点号匹配所有字符（包括换行符） (Multiline matching and dot matches all characters (including newline))
        )
        
        if not chapter_content_match:
            logging.warning(f"    在文档文本中未能找到章节“{chapter_title}”的内容以进行 RAG 分块。") # 中文翻译 (Chinese translation)
            continue
        
        current_chapter_full_text = chapter_content_match.group(1).strip() # 当前章节的完整文本 (Full text of current chapter)
        processed_chapters_count +=1

        # 如果章节有小节，则处理它们 (If chapter has subsections, process them)
        if chapter_item.get("sections"):
            for section_idx, section_item in enumerate(chapter_item.get("sections", [])):
                section_title = section_item.get("title", f"未命名小节 {section_idx+1}") # 中文翻译 (Chinese translation)
                section_header = f"### {section_title}" # 小节通常是 H3 标题 (Subsections are usually H3 titles)
                escaped_section_title = re.escape(section_title)

                # 用于在当前章节文本中查找当前小节内容的正则表达式。 (Regular expression to find current subsection content in current chapter text.)
                # 它在下一个小节 (###) 或章节内容末尾停止。 (It stops at the next subsection (###) or end of chapter content.)
                section_content_match = re.search(
                    rf"^(###\s*{escaped_section_title}.*?)(?=^###\s+[^#]|\Z)",
                    current_chapter_full_text, # 在提取的章节文本中搜索 (Search in extracted chapter text)
                    re.MULTILINE | re.DOTALL
                )

                if not section_content_match:
                    logging.warning(f"      在章节“{chapter_title}”内未能找到小节“{section_title}”的内容。") # 中文翻译 (Chinese translation)
                    # 后备方案：如果未找到小节，但章节没有其他小节， (Fallback: If subsection is not found, but chapter has no other subsections,)
                    # 则将整个章节内容（减去其自身标题）视为此单个小节的内容。 (then treat the entire chapter content (minus its own title) as the content of this single subsection.)
                    # 这是一种启发式方法，可能不总是正确。 (This is a heuristic method and may not always be correct.)
                    if len(chapter_item.get("sections", [])) == 1:
                        logging.info(f"      将完整章节内容（减去标题）归属于单个小节“{section_title}”。") # 中文翻译 (Chinese translation)
                        # 在这种情况下，从 current_chapter_full_text 中移除 "## Chapter Title" 行 (In this case, remove the "## Chapter Title" line from current_chapter_full_text)
                        section_text_for_chunking = re.sub(rf"^{re.escape(chapter_header)}\s*\n?", "", current_chapter_full_text, count=1).strip()
                    else:
                        continue # 如果有多个小节且未找到此小节，则跳过 (If there are multiple subsections and this subsection is not found, skip)
                else:
                    section_text_for_chunking = section_content_match.group(1).strip() # 用于分块的小节文本 (Subsection text for chunking)
                
                processed_sections_count += 1
                # 将小节文本分割成更小的块 (Split subsection text into smaller chunks)
                # 使用大约每 Token 2 个字符作为 max_chars_per_chunk 的粗略指导 (从 max_chunk_tokens 转换) (Use approximately 2 characters per token as a rough guide for max_chars_per_chunk (converted from max_chunk_tokens))
                text_chunks_for_section = split_text_into_chunks(
                    section_text_for_chunking,
                    int(config.max_chunk_tokens * 2.0), # 将 Token 转换为字符估计值 (Convert token to character estimate)
                    config.overlap_chars # 重叠字符数 (Number of overlapping characters)
                )

                for chunk_idx, text_chunk in enumerate(text_chunks_for_section):
                    chunks.append(text_chunk)
                    metadatas.append({
                        "doc_id": doc_id,
                        "chapter_title": chapter_title,
                        "section_title": section_title,
                        "original_header": section_header, # 存储此小节的特定标题 (Store the specific title of this subsection)
                        "chunk_index_in_section": chunk_idx # 块在小节内的索引 (Index of chunk within subsection)
                    })
                    total_chunks_generated +=1
        else: # 章节没有小节，将章节本身视为一个“小节” (Chapter has no subsections, treat the chapter itself as a "subsection")
            processed_sections_count += 1 # 将章节本身计为一个已处理的小节 (Count the chapter itself as a processed subsection)
            section_text_for_chunking = current_chapter_full_text # 完整的章节文本 (Full chapter text)
            
            text_chunks_for_chapter = split_text_into_chunks(
                section_text_for_chunking,
                int(config.max_chunk_tokens * 2.0), # Token 到字符的估计转换 (Token to character estimate conversion)
                config.overlap_chars
            )
            for chunk_idx, text_chunk in enumerate(text_chunks_for_chapter):
                chunks.append(text_chunk)
                metadatas.append({
                    "doc_id": doc_id,
                    "chapter_title": chapter_title,
                    "section_title": chapter_title, # 对于没有小节的章节，section_title 等于 chapter_title (For chapters without subsections, section_title equals chapter_title)
                    "original_header": chapter_header, # 存储章节标题 (Store chapter title)
                    "chunk_index_in_section": chunk_idx
                })
                total_chunks_generated += 1
                
    logging.info(f"  文档 RAG 分块完成。已处理 {processed_chapters_count} 个章节，{processed_sections_count} 个小节。生成了 {total_chunks_generated} 个块。") # 中文翻译 (Chinese translation)
    if not chunks:
        logging.warning(f"  doc_id '{doc_id}' 的 RAG 分块结果为零。请检查文档结构和提纲。") # 中文翻译 (Chinese translation)
    return chunks, metadatas


def generate_section_content( config: Config, section_title: str, section_specific_user_prompt: str,
                              system_prompt: str, target_length_chars: int, model_name: str,
                              overall_context: str = "", is_final_pass_for_section: bool = False) -> str: # 生成小节内容 (Generate section content)
    logging.info(f"\n--- 正在为小节生成内容: '{section_title}' (目标: {target_length_chars} 字符) ---") # 中文翻译 (Chinese translation)
    full_section_content = "" # 小节的完整内容 (Full content of section)

    base_chunk_prompt = """
    重要说明：
    1. 你正在撰写当前章节 '{section_title}' 的详细内容。请确保内容深入、逻辑严谨。
    2. {length_instruction}
    3. {concluding_instruction_section}
    4. 保持内容的连贯性和专业性，注意段落之间的衔接。请确保你撰写的内容能自然地承接上一章的结论，并巧妙地为下一章的主题做一些铺垫。
    5. 请严格按照学术报告的风格撰写。
    """ # 这个提示词大部分已经是中文，保留其结构，翻译其中的英文占位符描述 (This prompt is mostly in Chinese, retain its structure, translate the English placeholder descriptions within it)
    concluding_instr = "如果本章节需要一个小结或过渡，请自然地完成。" if is_final_pass_for_section else \
                       ("这是结论章节，请确保恰当地总结。" if "conclusion" in section_title.lower() or "总结" in section_title.lower() else \
                        "避免使用 \"综上所述\", \"总而言之\" 等总结全文的短语，你仅在撰写文档的一个章节。")
    # concluding_instr: 收尾指令 (Concluding instruction)
    # is_final_pass_for_section: 是否为小节的最终处理遍数 (Whether it is the final processing pass for the section)

    for i in range(config.max_chunks_per_section): # 遍历每个块 (Iterate through each chunk)
        remaining_chars = target_length_chars - len(full_section_content) # 剩余字符数 (Remaining character count)
        if remaining_chars < config.min_allocated_chars_for_section: # 如果剩余字符太少 (If remaining characters are too few)
            logging.info(f"  - 小节 '{section_title}' 即将完成。停止生成。") # 中文翻译 (Chinese translation)
            break

        logging.info(f"  - 小节 '{section_title}', 块 {i+1}/{config.max_chunks_per_section} (当前: {len(full_section_content)}/{target_length_chars} 字符)") # 中文翻译 (Chinese translation)

        messages = [{"role": "system", "content": system_prompt}] # 系统消息 (System message)
        
        chars_to_generate_this_chunk = min(remaining_chars, int(config.max_chunk_tokens * 2.5)) # 本块要生成的字符数 (Number of characters to generate for this chunk)
        max_tokens_for_chunk = min(config.max_chunk_tokens, int(chars_to_generate_this_chunk / 2.0)) # 本块的最大 Token 数 (Maximum token count for this chunk)
        max_tokens_for_chunk = max(200, max_tokens_for_chunk) # 确保至少有 200 Tokens (Ensure at least 200 tokens)

        length_instr = f"请撰写大约 {chars_to_generate_this_chunk} 字的内容来继续阐述。当论点阐述清晰、论证充分后，即可自然收尾，无需强行扩充。"
        # length_instr: 长度指令 (Length instruction)

        chunk_rules = base_chunk_prompt.format(
            section_title=section_title,
            length_instruction=length_instr, # 长度指令 (Length instruction)
            concluding_instruction_section=concluding_instr # 收尾指令 (Concluding instruction)
        )

        if i == 0: # 如果是第一个块 (If it is the first chunk)
            user_prompt = (
                f"你正在撰写报告中的一个章节，标题是：'{section_title}'。\n"
                f"本章节的具体要求是：{section_specific_user_prompt}\n\n"
                f"报告的整体上下文信息（供参考）：\n{overall_context}\n\n"
                f"请开始撰写 '{section_title}' 章节的内容。\n"
                f"{chunk_rules}"
            )
        else: # 如果不是第一个块 (If it is not the first chunk)
            context_from_section = full_section_content[max(0, len(full_section_content) - config.overlap_chars):] # 从小节中提取上下文 (Extract context from section)
            user_prompt = (
                f"你正在续写关于 '{section_title}' 的章节。以下是本章节已生成的部分内容：\n"
                f"--- 本章节上下文开始 ---\n{context_from_section}\n--- 本章节上下文结束 ---\n\n"
                f"报告的整体上下文信息（供参考）：\n{overall_context}\n\n"
                f"请继续详细阐述当前章节 '{section_title}'。\n"
                f"{chunk_rules}"
            )
        messages.append({"role": "user", "content": user_prompt}) # 用户消息 (User message)

        chunk = call_ai(config, model_name, messages, max_tokens_output=max_tokens_for_chunk) # 调用 AI 生成块内容 (Call AI to generate chunk content)

        if "AI模型调用失败" in chunk: # 如果 AI 调用失败 (If AI call fails)
            logging.error(f"  - 小节 '{section_title}' 的块生成失败: {chunk}") # 中文翻译 (Chinese translation)
            break

        stripped_chunk = chunk.strip() # 去除块内容两端的空白 (Remove whitespace from both ends of chunk content)
        if len(stripped_chunk) < config.min_chars_for_short_chunk_warning and not is_final_pass_for_section: # 如果块太短 (If chunk is too short)
            logging.warning(f"  - 小节 '{section_title}' 的块非常短 ({len(stripped_chunk)} 字符)。提前停止小节生成。") # 中文翻译 (Chinese translation)
            if len(stripped_chunk) > 5: full_section_content += "\n\n" + stripped_chunk
            break

        is_conclusion_type_section = "conclusion" in section_title.lower() or "总结" in section_title.lower() # 是否为结论型小节 (Whether it is a conclusion-type section)
        if not is_final_pass_for_section and not is_conclusion_type_section: # 如果不是最终处理遍数且不是结论型小节 (If it is not the final processing pass and not a conclusion-type section)
            if any(ep in stripped_chunk for ep in ["综上所述", "总而言之", "因此得出结论", "最终结论"]): # 如果包含过早结束文档的短语 (If it contains phrases that end the document prematurely)
                logging.warning(f"  - 小节 '{section_title}' 的块包含过早结束文档的短语。添加块并停止小节生成。") # 中文翻译 (Chinese translation)
                full_section_content += "\n\n" + stripped_chunk
                break

        full_section_content += "\n\n" + stripped_chunk # 累加块内容 (Accumulate chunk content)
        if i < config.max_chunks_per_section - 1: time.sleep(0.2) # 如果不是最后一个块，则稍作停顿 (If it is not the last chunk, pause for a while)

    logging.info(f"--- 小节 '{section_title}' 内容生成完毕 (已生成: {len(full_section_content)} 字符, 目标: {target_length_chars} 字符) ---") # 中文翻译 (Chinese translation)
    if not full_section_content.strip() and target_length_chars > 0: # 如果内容为空但目标字符数大于0 (If content is empty but target character count is greater than 0)
        logging.warning(f"小节 '{section_title}' 虽然目标字符数为 {target_length_chars}，但未生成任何内容。") # 中文翻译 (Chinese translation)
        return f"\n\n## {section_title}\n\n[本章节内容生成失败或为空]\n\n"
    return f"\n\n## {section_title}\n\n{full_section_content.strip()}\n\n"


def truncate_text_for_context(config: Config, text: str, max_tokens: int, truncation_style: str = "middle") -> str: # 为上下文截断文本 (Truncate text for context)
    if not text: return ""
    tokens = config.encoder.encode(text) if config.encoder else text.split() # 获取 token (Get tokens)
    if len(tokens) <= max_tokens: return text # 如果 token 数量未超限，则直接返回 (If token count does not exceed limit, return directly)
    logging.info(f"    - 正在截断文本: {len(tokens)} tokens -> {max_tokens} tokens (方式: {truncation_style})") # 中文翻译 (Chinese translation)
    decode_fn = config.encoder.decode if config.encoder else lambda t: " ".join(t) # 解码函数 (Decode function)
    if truncation_style == "head": # 头部截断 (Head truncation)
        return decode_fn(tokens[:max_tokens]) + "\n... [内容已截断，只显示开头部分] ..."
    elif truncation_style == "tail": # 尾部截断 (Tail truncation)
        return "... [内容已截断，只显示结尾部分] ...\n" + decode_fn(tokens[-max_tokens:])
    else: # 中间截断 (默认) (Middle truncation (default))
        h_len, t_len = max_tokens // 2, max_tokens - (max_tokens // 2) # 头部和尾部长度 (Head and tail length)
        if h_len > len(tokens) // 2: h_len = len(tokens) // 2; t_len = max_tokens - h_len
        if t_len > len(tokens) // 2: t_len = len(tokens) // 2; h_len = max_tokens - t_len
        if h_len + t_len > max_tokens: t_len = max_tokens - h_len # 调整长度以确保总长度不超过 max_tokens (Adjust length to ensure total length does not exceed max_tokens)
        h_len = max(0, h_len)
        t_len = max(0, t_len)
        if h_len == 0 and t_len == 0 and max_tokens > 0 : # 如果计算后头部和尾部都为0，但允许token数大于0 (If head and tail are both 0 after calculation, but allowed token count is greater than 0)
            h_len = min(max_tokens, len(tokens)) # 则取允许的最大token数或实际token数中较小者作为头部长度 (Then take the smaller of the allowed maximum token count or the actual token count as the head length)

        head_part = decode_fn(tokens[:h_len]) if h_len > 0 else "" # 头部内容 (Head content)
        tail_part = decode_fn(tokens[-t_len:]) if t_len > 0 else "" # 尾部内容 (Tail content)

        if h_len > 0 and t_len > 0: # 如果头部和尾部都有内容 (If both head and tail have content)
            return head_part + "\n... [中间内容已截断] ...\n" + tail_part
        elif h_len > 0: # 如果只有头部内容 (If only head has content)
            return head_part + "\n... [内容已截断，只显示开头部分] ..."
        elif t_len > 0: # 如果只有尾部内容 (If only tail has content)
            return "... [内容已截断，只显示结尾部分] ...\n" + tail_part
        else: # 如果都没有内容 (理论上不太可能发生，除非 max_tokens 为0) (If neither has content (theoretically unlikely to happen unless max_tokens is 0))
            return "... [内容已截断] ..." # 中文翻译 (Chinese translation)


def calculate_checksum(data: str) -> str: # 计算校验和 (Calculate checksum)
    return hashlib.sha256(data.encode('utf-8')).hexdigest()

def preprocess_json_string(json_string: str) -> str: # 预处理 JSON 字符串 (Preprocess JSON string)
    """
    应用一系列正则表达式修复 LLM 生成的常见 JSON 错误。
    """ # 中文翻译 (Apply a series of regular expressions to fix common JSON errors generated by LLM.)
    if not json_string or json_string.isspace(): # 如果字符串为空或仅包含空白 (If string is empty or contains only whitespace)
        return ""

    processed_string = json_string.strip() # 去除两端空白 (Remove whitespace from both ends)

    # 1. 移除 C 风格注释 (// 和 /* */) (Remove C-style comments (// and /* */))
    # 移除 // 注释 (Remove // comments)
    processed_string = re.sub(r"//.*", "", processed_string)
    # 移除 /* ... */ 注释 (Remove /* ... */ comments)
    processed_string = re.sub(r"/\*[\s\S]*?\*/", "", processed_string)

    # 0. 将中文引号替换为标准的英文双引号 (Replace Chinese quotes with standard English double quotes)
    processed_string = processed_string.replace("“", "\"").replace("”", "\"")
    processed_string = processed_string.replace("‘", "'").replace("’", "'") # 同时处理中文单引号，以防万一，后续的单引号转双引号逻辑会处理 (Also handle Chinese single quotes, just in case, the subsequent single quote to double quote logic will handle it)

    # 1. 移除有问题的控制字符 (ASCII 0-31，不包括在有效转义序列中的制表符、换行符、回车符、换页符、退格符) (Remove problematic control characters (ASCII 0-31, excluding tab, newline, carriage return, form feed, backspace in valid escape sequences))
    # 这个正则表达式移除了 \t, \n, \r, \f, \b 之外的控制字符。 (This regular expression removes control characters other than \t, \n, \r, \f, \b.)
    # 这很棘手，因为如果它们已经被转义 (例如 \\n)，我们不想动它们。 (This is tricky because if they are already escaped (e.g., \\n), we don't want to touch them.)
    # 目前一个更简单的方法是：移除 \t, \n, \r 之外的所有 C0 控制字符。 (Currently a simpler method is: remove all C0 control characters other than \t, \n, \r.)
    # JSON 规范只允许某些控制字符被转义。其他的则被禁止。 (The JSON specification only allows certain control characters to be escaped. Others are prohibited.)
    # 我们将移除 U+0000–U+001F 范围内不是 \t, \n, \r, \f, \b (如果已转义则允许) 的任何字符。 (We will remove any character in the U+0000–U+001F range that is not \t, \n, \r, \f, \b (allowed if escaped).)
    # 对于直接的未转义控制字符，大多数都有问题。 (For direct unescaped control characters, most are problematic.)
    # 让我们构建一个正则表达式来移除 \x09 (TAB), \x0A (LF), \x0D (CR) 之外的 C0 控制字符。 (Let's build a regular expression to remove C0 control characters other than \x09 (TAB), \x0A (LF), \x0D (CR).)
    # U+0000–U+001F 范围内的所有其他字符如果未作为有效转义的一部分，则应被移除。 (All other characters in the U+0000–U+001F range should be removed if not part of a valid escape.)
    # 如果没有状态机，用单个正则表达式很难完美地做到这一点。 (Without a state machine, it's difficult to do this perfectly with a single regular expression.)
    # 一个更简单、更激进的方法： (A simpler, more aggressive method:)
    # 移除 \x00-\x08, \x0B, \x0C, \x0E-\x1F 范围内的字符 (Remove characters in the range \x00-\x08, \x0B, \x0C, \x0E-\x1F)
    control_chars_to_remove_pattern = r"[\x00-\x08\x0B\x0C\x0E-\x1F]"
    processed_string = re.sub(control_chars_to_remove_pattern, "", processed_string)
    
    # 2. 将 Python 的 True, False, None 替换为 JSON 的 true, false, null (Replace Python's True, False, None with JSON's true, false, null)
    # 使用单词边界以避免替换像 "NoneSuch" 这样的词的一部分 (Use word boundaries to avoid replacing parts of words like "NoneSuch")
    processed_string = re.sub(r"\bTrue\b", "true", processed_string)
    processed_string = re.sub(r"\bFalse\b", "false", processed_string)
    processed_string = re.sub(r"\bNone\b", "null", processed_string)

    # 3. 尝试将键和字符串值中的单引号修复为双引号 (Try to fix single quotes in keys and string values to double quotes)
    # 这有点棘手，需要小心不要弄乱已经正确的 JSON 或转义的引号。 (This is a bit tricky and requires care not to mess up already correct JSON or escaped quotes.)
    # LLM 的一个常见错误是所有字符串都使用单引号。 (A common error of LLM is to use single quotes for all strings.)
    # 首先，尝试修复键：'key': -> "key": (First, try to fix keys: 'key': -> "key":)
    processed_string = re.sub(r"(['])([A-Za-z_]\w*)\1\s*:", r'"\2":', processed_string)
    # 然后，尝试修复简单的、在值末尾或数组中用单引号括起来的字符串值 (Then, try to fix simple string values enclosed in single quotes at the end of values or in arrays)
    # 示例：:"value", 或 :"value"] 或 :"value"} (Example: :"value", or :"value"] or :"value"})
    # 这仍然有风险。更健壮的方法是使用真正的解析器，但这是一个预处理器。 (This is still risky. A more robust method is to use a real parser, but this is a preprocessor.)
    # 这里我们保守一点。一个常见的错误是 'value' 而不是 "value"。 (Let's be conservative here. A common error is 'value' instead of "value".)
    # 如果字符串看起来像键值对的值部分或数组成员： (If the string looks like the value part of a key-value pair or an array member:)
    # ': '...' | " : '...' | , '...' | [ '...'
    # 这个正则表达式查找可能是值的单引号字符串。 (This regular expression looks for single-quoted strings that might be values.)
    # 它并不完美，可能会在复杂的转义字符串上出错。 (It's not perfect and may err on complex escaped strings.)
    # 一个更简单的方法：如果整个字符串似乎都使用单引号，则尝试替换所有未转义的单引号。 (A simpler method: if the entire string seems to use single quotes, try to replace all unescaped single quotes.)
    # 这是某些 LLM 在混淆时常见的模式。 (This is a common pattern when some LLMs are confused.)
    # 计算双引号与单引号的数量（不属于转义序列的） (Calculate the number of double quotes versus single quotes (not part of escape sequences))
    num_double_quotes = len(re.findall(r'(?<!\\)"', processed_string)) # 双引号数量 (Number of double quotes)
    num_single_quotes = len(re.findall(r"(?<!\\)'", processed_string)) # 单引号数量 (Number of single quotes)

    # 如果单引号占主导地位，且双引号很少（或没有）， (If single quotes are dominant and double quotes are few (or none),)
    # 那么 LLM 很可能对所有字符串都使用了单引号。 (then LLM most likely used single quotes for all strings.)
    if num_single_quotes > 0 and (num_double_quotes == 0 or num_single_quotes > num_double_quotes * 2):
        logging.info("  preprocess_json_string: 检测到主导性的单引号，尝试转换为双引号。") # 中文翻译 (Chinese translation)
        # 仅当单引号未被转义时，才将其替换为双引号。 (Replace single quotes with double quotes only if the single quotes are not escaped.)
        # 这仍然有风险。对于这种情况，更好的方法可能是 (This is still risky. A better method for this case might be)
        # 找到所有不属于更大字符串的 '...' 出现， (to find all occurrences of '...' that are not part of a larger string,)
        # 并将 ' 替换为 "。 (and replace ' with ".)
        # 目前，采用更简单、更有针对性的替换方式，针对 'string': 或 ,'string' (Currently, adopt a simpler, more targeted replacement method for 'string': or ,'string')
        
        # 将 'key': 替换为 "key": (Replace 'key': with "key":)
        processed_string = re.sub(r"(\s*\{\s*|\s*,\s*)([A-Za-z_]\w*)\s*:", r'\1"\2":', processed_string) # 'key':
        # 将 :'value' 替换为 :"value" (Replace :'value' with :"value")
        processed_string = re.sub(r":\s*'(.*?)'", r': "\1"', processed_string) # :'value'
        # 将 ['value'] 替换为 ["value"] (Replace ['value'] with ["value"])
        processed_string = re.sub(r"\[\s*'(.*?)'\s*\]", r'["\1"]', processed_string) # ['value']
        # 将 ,'value' 替换为 ,"value" (Replace ,'value' with ,"value")
        processed_string = re.sub(r",\s*'(.*?)'", r'," \1"', processed_string) # ,'value'


    # 4. 移除对象和数组中末尾的逗号 (Remove trailing commas in objects and arrays)
    # 从对象中移除末尾逗号 {..., "key": "value",} -> {..., "key": "value"} (Remove trailing comma from object {..., "key": "value",} -> {..., "key": "value"})
    processed_string = re.sub(r",\s*([}\]])", r"\1", processed_string)
    
    # 5. 尝试移除 JSON 对象/数组前后多余的文本 (Try to remove redundant text before and after JSON object/array)
    # 如果字符串包含有效的 JSON 结构，但在其前后有垃圾字符。 (If the string contains a valid JSON structure, but has garbage characters before and after it.)
    match_json_obj = re.search(r"(\{[\s\S]*\})", processed_string) # 匹配 JSON 对象 (Match JSON object)
    match_json_arr = re.search(r"(\[[\s\S]*\])", processed_string) # 匹配 JSON 数组 (Match JSON array)

    if match_json_obj and match_json_arr: # 如果同时找到对象和数组 (If both object and array are found)
        # 选择更外层或更早开始的那个 (Choose the one that is more outer or starts earlier)
        if match_json_obj.start() <= match_json_arr.start() and match_json_obj.end() >= match_json_arr.end():
            processed_string = match_json_obj.group(1)
        elif match_json_arr.start() <= match_json_obj.start() and match_json_arr.end() >= match_json_obj.end():
            processed_string = match_json_arr.group(1)
        # 如果它们是嵌套的，这个简单的检查可能不够，但这是一种启发式方法。 (If they are nested, this simple check may not be enough, but it's a heuristic method.)
        # 目前，如果对象看起来是外层的或先开始，则优先选择对象。 (Currently, if the object looks outer or starts first, prioritize the object.)
        elif match_json_obj.start() < match_json_arr.start():
             processed_string = match_json_obj.group(1)
        else: # 数组先开始或在相同位置开始 (Array starts first or at the same position)
             processed_string = match_json_arr.group(1)

    elif match_json_obj: # 只找到对象 (Only object is found)
        processed_string = match_json_obj.group(1)
    elif match_json_arr: # 只找到数组 (Only array is found)
        processed_string = match_json_arr.group(1)
    # 如果两者都未找到，则可能是一个裸值 (例如 "string", number, true, false, null) - 保持原样。 (If neither is found, it might be a bare value (e.g., "string", number, true, false, null) - keep it as is.)

    logging.debug(f"  预处理后的 JSON 字符串: {processed_string[:500]}...") # 中文翻译 (Chinese translation)
    return processed_string

def apply_patch(original_text: str, patches_json_str: str) -> str: # 应用补丁 (Apply patch)
    try:
        patches = json.loads(patches_json_str) # 加载补丁 JSON 字符串 (Load patch JSON string)
        if isinstance(patches, dict) and "patches" in patches and isinstance(patches["patches"], list): # 如果补丁是包含 "patches" 键的字典 (If patch is a dictionary containing "patches" key)
            patches = patches["patches"] # 则取 "patches" 键的值 (Then take the value of "patches" key)
        elif not isinstance(patches, list): # 如果补丁不是列表 (If patch is not a list)
            patches = [patches] # 则将其转换为单元素列表 (Then convert it to a single-element list)
    except json.JSONDecodeError as e:
        logging.error(f"补丁字符串中的 JSON 无效: {e}\n补丁: {patches_json_str}") # 中文翻译 (Chinese translation)
        return original_text # 返回原始文本 (Return original text)
    except Exception as e:
        logging.error(f"加载补丁 JSON 时发生意外错误: {e}\n补丁: {patches_json_str}") # 中文翻译 (Chinese translation)
        return original_text

    lines = original_text.splitlines(True) # 将原始文本按行分割 (保留换行符) (Split original text by line (keep newline characters))
    processed_text = list(lines) # 将行列表转换为可修改的列表 (Convert list of lines to a modifiable list)

    for i, op in enumerate(patches): # 遍历每个补丁操作 (Iterate through each patch operation)
        # 标准化键的访问，允许一些别名 (Standardize key access, allowing some aliases)
        action = op.get("action", op.get("operation", op.get("op"))) # 操作类型 (添加 "op" 作为 action 的别名) (Operation type (add "op" as an alias for action))
        target_header = op.get("target_section", op.get("anchor", op.get("section_anchor"))) # 目标章节标题 (Target chapter title)
        new_content = op.get("new_content", op.get("content", "")) # 新内容 (允许 "content" 作为 "new_content" 的别名) (New content (allow "content" as an alias for "new_content"))
        
        if not action or not target_header: # 如果缺少操作或目标 (If action or target is missing)
            logging.warning(f"跳过补丁 {i+1} (缺少 action/target_section 或其别名): {op}") # 中文翻译 (Chinese translation)
            continue
        logging.info(f"正在应用补丁 {i+1}: {action} 到 '{target_header}'") # 中文翻译 (Chinese translation)

        target_idx = -1 # 目标索引 (Target index)
        best_match_score = 0 #最佳匹配得分 (Best match score)
        
        # 对 target_section 进行模糊匹配 (Fuzzy match for target_section)
        # 仅考虑以 "## " 或 "### " 开头的行作为潜在的标题行 (Only consider lines starting with "## " or "### " as potential title lines)
        potential_header_indices = [idx for idx, line in enumerate(processed_text)
                                    if line.strip().startswith("## ") or line.strip().startswith("### ")]

        if not potential_header_indices: # 如果没有找到 Markdown 标题 (If no Markdown title is found)
            logging.warning(f"文档中未找到可用于补丁 {i+1} ('{target_header}') 匹配的 Markdown 标题。跳过。") # 中文翻译 (Chinese translation)
            continue

        for idx in potential_header_indices: # 遍历潜在的标题行索引 (Iterate through potential title line indices)
            line_content = processed_text[idx].strip() # 获取行内容并去除空白 (Get line content and remove whitespace)
            score = fuzz.ratio(target_header.strip(), line_content) # 计算相似度得分 (Calculate similarity score)
            if score > best_match_score: # 如果得分更高 (If score is higher)
                best_match_score = score # 更新最佳匹配得分 (Update best match score)
                target_idx = idx # 更新目标索引 (Update target index)
        
        # 应用匹配阈值 (例如 85%) (Apply match threshold (e.g., 85%))
        # 问题描述建议使用 85，此处采用该值。 (Problem description suggests using 85, use this value here.)
        SIMILARITY_THRESHOLD = 85 # 相似度阈值 (Similarity threshold)
        if target_idx == -1 or best_match_score < SIMILARITY_THRESHOLD: # 如果未找到或相似度不足 (If not found or similarity is insufficient)
            logging.warning(f"目标小节 '{target_header}' 未找到或相似度不足以应用补丁 {i+1} (最佳得分: {best_match_score}%)。跳过。") # 中文翻译 (Chinese translation)
            continue
        else:
            logging.info(f"  已将 '{target_header}' 与 '{processed_text[target_idx].strip()}' 匹配 (得分: {best_match_score}%)，位于索引 {target_idx}。") # 中文翻译 (Chinese translation)

        section_end_idx = len(processed_text) # 默认的小节结束索引为文本末尾 (Default section end index is end of text)
        for idx in range(target_idx + 1, len(processed_text)): # 从目标标题下一行开始查找下一个二级标题 (Start searching for the next secondary title from the line after the target title)
            if processed_text[idx].strip().startswith("## "): # 如果找到下一个二级标题 (If the next secondary title is found)
                section_end_idx = idx # 更新小节结束索引 (Update section end index)
                break

        try:
            if action == "REPLACE": # 替换操作 (Replace operation)
                del processed_text[target_idx + 1 : section_end_idx] # 删除旧内容 (标题行之后到小节结束) (Delete old content (from after title line to end of section))
                new_lines = (new_content.strip() + "\n").splitlines(True) if new_content.strip() else [] # 准备新内容行 (Prepare new content lines)
                for j, line_to_insert in enumerate(new_lines): processed_text.insert(target_idx + 1 + j, line_to_insert) # 插入新内容行 (Insert new content lines)
            elif action == "INSERT_AFTER": # 在之后插入操作 (Insert after operation)
                new_lines = ("\n" + new_content.strip() + "\n").splitlines(True) if new_content.strip() else [] # 准备新内容行 (确保前后有换行) (Prepare new content lines (ensure newlines before and after))
                for j, line_to_insert in enumerate(new_lines): processed_text.insert(section_end_idx + j, line_to_insert) # 在小节末尾插入新内容行 (Insert new content lines at the end of the section)
            elif action == "DELETE": # 删除操作 (Delete operation)
                del processed_text[target_idx : section_end_idx] # 删除整个小节 (包括标题行) (Delete the entire section (including title line))
            else: logging.warning(f"操作 {i+1} 中未知的补丁操作 '{action}'。跳过。") # 中文翻译 (Chinese translation)
        except Exception as e: logging.error(f"应用补丁操作 {op} 时出错: {e}") # 中文翻译 (Chinese translation)
    return "".join(processed_text) # 返回处理后的文本 (Return processed text)


def get_google_auth_http(config: Config): # 获取 Google 认证的 HTTP 对象 (Get Google authenticated HTTP object)
    proxy_url = os.environ.get("HTTP_PROXY") # 获取 HTTP 代理 URL (Get HTTP proxy URL)
    proxy_info = None # 代理信息 (Proxy information)
    if proxy_url: # 如果设置了代理 (If proxy is set)
        parsed_proxy = urlparse(proxy_url) # 解析代理 URL (Parse proxy URL)
        proxy_info = httplib2.ProxyInfo(
            proxy_type=httplib2.socks.PROXY_TYPE_HTTP if parsed_proxy.scheme == 'http' else httplib2.socks.PROXY_TYPE_HTTPS, # 代理类型 (Proxy type)
            proxy_host=parsed_proxy.hostname, proxy_port=parsed_proxy.port, # 代理主机和端口 (Proxy host and port)
            proxy_user=parsed_proxy.username, proxy_pass=parsed_proxy.password # 代理用户名和密码 (Proxy username and password)
        )
        logging.info(f"Google API 将使用代理: {parsed_proxy.hostname}:{parsed_proxy.port}") # 中文翻译 (Chinese translation)

    if config.google_service_account_path and os.path.exists(config.google_service_account_path): # 如果配置了服务账户路径且文件存在 (If service account path is configured and file exists)
        try:
            creds = Credentials.from_service_account_file(config.google_service_account_path, scopes=['https://www.googleapis.com/auth/cse']) # 从服务账户文件加载凭据 (Load credentials from service account file)
            return AuthorizedHttp(creds, http=httplib2.Http(proxy_info=proxy_info, timeout=config.api_request_timeout_seconds)) # 返回授权的 HTTP 对象 (Return authorized HTTP object)
        except Exception as e: logging.error(f"未能使用服务账户进行 Google API 认证，回退到默认方式。错误: {e}") # 中文翻译 (Chinese translation)
    return httplib2.Http(proxy_info=proxy_info, timeout=config.api_request_timeout_seconds) # 返回普通的 HTTP 对象 (可能带代理) (Return normal HTTP object (possibly with proxy))


def perform_search(config: Config, query: str) -> list[dict]: # 执行搜索 (Perform search)
    if not config.google_api_key or not config.google_cse_id: # 如果未配置 API 密钥或 CSE ID (If API key or CSE ID is not configured)
        logging.error("Google API 密钥或 CSE ID 未配置。") # 中文翻译 (Chinese translation)
        return []
    try:
        logging.info(f"正在执行 Google 搜索: '{query}'") # 中文翻译 (Chinese translation)
        http_auth = get_google_auth_http(config) # 获取认证的 HTTP 对象 (Get authenticated HTTP object)
        service = build("customsearch", "v1", developerKey=config.google_api_key, http=http_auth) # 构建搜索服务 (Build search service)
        res = service.cse().list(q=query, cx=config.google_cse_id, num=config.num_search_results).execute() # 执行搜索请求 (Execute search request)
        items = res.get('items', []) # 获取搜索结果项 (Get search result items)
        logging.info(f"  Google 搜索为 '{query}' 返回了 {len(items)} 个结果。") # 中文翻译 (Chinese translation)
        return items
    except HttpError as e: # 捕获 HTTP 错误 (Catch HTTP error)
        logging.error(f"  Google 搜索 API HTTP 错误: {e.resp.status} - {e.content.decode('utf-8', 'ignore') if isinstance(e.content, bytes) else e.content}") # 中文翻译 (Chinese translation)
        if hasattr(e, 'resp') and e.resp.status == 403: # 如果是 403 错误 (禁止访问) (If it is a 403 error (forbidden))
            try:
                error_details = json.loads(e.content).get('error', {}).get('errors', [{}])[0] # 解析错误详情 (Parse error details)
                message = error_details.get('message', '未知的 403 错误') # 中文翻译 (Chinese translation)
                logging.error(f"  详细错误信息 (403): {message}") # 中文翻译 (Chinese translation)
                if 'accessNotConfigured' in message or 'API has not been used' in message or 'serviceusage.googleapis.com' in message: # 如果是 API 未配置或未启用 (If API is not configured or not enabled)
                    logging.error("  提示: 请确保在 Google Cloud 项目中启用了 Custom Search API: https://console.cloud.google.com/apis/library/customsearch.googleapis.com") # 中文翻译 (Chinese translation)
            except Exception: pass # 保持原始的简单错误处理逻辑以解析错误详情 (Keep the original simple error handling logic to parse error details)
        return []
    except (socket.timeout, httplib2.HttpLib2Error) as e_timeout: # 捕获 httplib2 的更具体的超时或连接错误类型 (Catch more specific timeout or connection error types of httplib2)
        logging.error(f"  Google 搜索 API 请求超时或连接错误: {e_timeout} (类型: {type(e_timeout).__name__})", exc_info=True) # 中文翻译 (Chinese translation)
        return []
    except Exception as e: # 捕获所有其他一般性异常 (Catch all other general exceptions)
        logging.error(f"  Google 搜索期间发生未知错误: {e} (类型: {type(e).__name__})", exc_info=True) # 中文翻译 (Chinese translation)
        return []

# V8 新增：用于从知识空白和文档上下文中创建更智能的搜索查询 (V8 New: Used to create smarter search queries from knowledge gaps and document context)
def create_intelligent_search_queries(config: Config, knowledge_gap: str, full_document_context: str) -> list[str]:
    logging.info(f"  正在为知识空白生成智能搜索查询: '{knowledge_gap[:100]}...'") # 中文翻译 (Chinese translation)
    
    context_summary_for_query_gen = truncate_text_for_context(config, full_document_context, 1000, "middle") # 中文翻译 (Chinese translation)

    prompt = f"""
    根据以下提供的“知识空白”和“文档上下文摘要”，生成1-3个高度具体且有效的搜索引擎查询。
    这些查询的目标是找到填补该知识空白所需的信息。
    查询应简洁，并使用关键词。避免提出问题；而是制定搜索词条。

    知识空白:
    "{knowledge_gap}"

    文档上下文摘要:
    ---
    {context_summary_for_query_gen}
    ---

    生成的搜索查询 (每行一个，最多3个):
    """ # 中文提示词 (Chinese prompt)

    messages = [{"role": "user", "content": prompt}]
    
    try:
        response_content = call_ai(
            config,
            config.researcher_model_name, # 使用研究员模型 (Use researcher model)
            messages,
            temperature=0.2, # 略微提高温度以获得多样化的查询 (Slightly increase temperature for diverse queries)
            max_tokens_output=150 # 足够生成几个短查询 (Enough to generate several short queries)
        )

        if "AI模型调用失败" in response_content or not response_content.strip():
            logging.warning(f"    为知识空白生成搜索查询失败或返回空。AI响应: {response_content}。回退到使用知识空白本身作为查询。") # 中文翻译 (Chinese translation)
            return [knowledge_gap]

        queries = [q.strip() for q in response_content.splitlines() if q.strip()]
        
        if not queries: # 如果分割后列表为空 (If list is empty after splitting)
            logging.warning(f"    AI未能为知识空白“{knowledge_gap[:50]}...”生成格式正确的查询。回退。") # 中文翻译 (Chinese translation)
            return [knowledge_gap]
            
        logging.info(f"    为知识空白“{knowledge_gap[:50]}...”生成了 {len(queries)} 个查询: {queries}") # 中文翻译 (Chinese translation)
        return queries[:config.max_queries_per_gap] # 限制查询数量 (Limit number of queries)

    except Exception as e:
        logging.error(f"    为知识空白生成搜索查询时发生异常: {e}。回退到使用知识空白本身作为查询。", exc_info=True) # 中文翻译 (Chinese translation)
        return [knowledge_gap]

# V8 修改：增强了摘要提示词 (V8 Modification: Enhanced summarization prompt)
async def scrape_and_summarize_async(session: aiohttp.ClientSession, config: Config, url: str, knowledge_gap: str, specific_query: str) -> str: # 异步抓取和摘要 (Asynchronous scrape and summarize)
    logging.info(f"  [ASYNC] 抓取和总结中: {url} (针对查询: {specific_query})")
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        proxy_str = os.environ.get('HTTP_PROXY')
        ssl_context = ssl.create_default_context(cafile=certifi.where())

        async with session.get(url, headers=headers, timeout=aiohttp.ClientTimeout(total=30), proxy=proxy_str, ssl=ssl_context) as response: # Increased timeout
            if response.status != 200:
                logging.warning(f"  [ASYNC] 抓取 {url} 失败，状态码：{response.status}。")
                return ""

            content_type = response.headers.get('Content-Type', '').lower()
            text_content = ""

            if 'application/pdf' in content_type:
                logging.info(f"  [ASYNC] 检测到PDF内容: {url}。正在使用 PyMuPDF 提取文本。")
                try:
                    pdf_bytes = await response.read()
                    with fitz.open(stream=pdf_bytes, filetype="pdf") as doc:
                        text_content = "".join(page.get_text() for page in doc)
                    logging.info(f"  [ASYNC] 从PDF {url} 成功提取 {len(text_content)} 字符。")
                except Exception as e_pdf:
                    logging.error(f"  [ASYNC] 解析PDF {url} 时出错: {e_pdf}", exc_info=True)
                    return ""
            
            elif 'text/html' in content_type: # Ensure this is an elif
                logging.info(f"  [ASYNC] 检测到HTML内容: {url}。正在使用 BeautifulSoup 解析。")
                try:
                    html_content = await response.text()
                except UnicodeDecodeError:
                    logging.warning(f"    [ASYNC] {url} 发生 UnicodeDecodeError 错误，尝试使用 latin-1 解码。(UnicodeDecodeError for {url}, trying with latin-1.)")
                    html_content = await response.text(encoding='latin-1')
                
                soup = BeautifulSoup(html_content, 'lxml') # Changed to lxml
                
                for script_or_style in soup(["script", "style"]):
                    script_or_style.decompose()
                
                text_content = soup.get_text(separator='\n', strip=True)
                text_content = re.sub(r'\n\s*\n', '\n\n', text_content)
            else:
                logging.warning(f"  [ASYNC] 跳过不支持的内容类型 '{content_type}' 来自 {url}。")
                return ""

            if not text_content.strip():
                logging.info(f"  [ASYNC] 从 {url} 未提取到文本内容。(No text content extracted from {url}.)")
                return ""

            summary_prompt = f"""请根据以下针对知识空白“{knowledge_gap}”的特定查询“{specific_query}”来总结下面的文本。
提取与此查询最相关的信息、关键论点和数据点。避免无关细节。如果文本与查询不相关，请指明。

文本内容：
---
{truncate_text_for_context(config, text_content, 6000)}
---

总结：
"""
            summary = call_ai(config, config.summary_model_name, [{"role": "user", "content": summary_prompt}], max_tokens_output=768)
            
            if "AI模型调用失败" not in summary and summary.strip():
                logging.info(f"    [ASYNC] {url} 的内容已成功抓取和总结。(Content from {url} scraped and summarized successfully.)")
                return f"URL: {url}\n查询: {specific_query}\n知识空白: {knowledge_gap}\n总结:\n{summary}\n"
            else:
                logging.warning(f"    [ASYNC] {url} 的内容总结失败或为空。AI 响应: {summary} (Failed to summarize or got empty summary for {url}. AI Response: {summary})")
                return ""
# This was the original spot of the indentation error:
#                 summary = call_ai(config, config.summary_model_name, [{"role": "user", "content": summary_prompt}], max_tokens_output=768)
# This should be:
#            summary = call_ai(config, config.summary_model_name, [{"role": "user", "content": summary_prompt}], max_tokens_output=768)
# The `else` block below the `if "AI模型调用失败" not in summary and summary.strip():` was also part of the bad indent.
# It should align with the `if` that checks `response.status != 200`.

    except asyncio.TimeoutError:
        logging.warning(f"    [ASYNC] 抓取 {url} 超时。(Timeout while scraping {url}.)")
        return ""
    except aiohttp.ClientError as e: # 更广泛的 aiohttp 客户端错误 (Broader aiohttp client errors)
        logging.warning(f"    [ASYNC] 抓取 {url} 时发生 aiohttp 客户端错误 (aiohttp ClientError while scraping {url}): {e}")
        return ""
    except Exception as e:
        logging.error(f"    [ASYNC] 抓取和总结 {url} 时发生未知错误 (Unknown error while scraping and summarizing {url}): {e}", exc_info=True)
        return ""

# V8: run_research_cycle_async 的新实现，它现在接收知识空白列表 (V8: New implementation of run_research_cycle_async, it now receives a list of knowledge gaps)
async def run_research_cycle_async(config: Config, knowledge_gaps: list[str], full_document_context: str) -> str:
    if not knowledge_gaps: return "" # 如果没有知识空白，则返回空 (If there are no knowledge gaps, return empty)
    logging.info(f"\n--- V8: 开始为 {len(knowledge_gaps)} 个知识空白进行智能知识发现 --- (--- V8: Starting intelligent knowledge discovery for {len(knowledge_gaps)} knowledge gaps ---)") # 中文翻译 (Chinese translation)
    all_tasks = [] # 所有任务列表 (List of all tasks)

    async with aiohttp.ClientSession() as session: # 创建异步 HTTP 会话 (Create asynchronous HTTP session)
        for gap_idx, gap_text in enumerate(knowledge_gaps): # 遍历每个知识空白 (Iterate through each knowledge gap)
            logging.info(f"正在研究知识空白 {gap_idx+1}/{len(knowledge_gaps)}: '{gap_text}' (Researching knowledge gap {gap_idx+1}/{len(knowledge_gaps)}: '{gap_text}')") # 中文翻译 (Chinese translation)
            cleaned_gap = re.sub(r'[\*\-`#]', '', gap_text).strip() # 清理知识空白文本 (Clean knowledge gap text)
            if not cleaned_gap: # 如果清理后为空 (If empty after cleaning)
                logging.warning(f"  跳过空的知识空白 (Skipping empty knowledge gap): '{gap_text}'") # 中文翻译 (Chinese translation)
                continue

            # V8 修改：使用新的智能查询生成函数 (V8 Modification: Use new intelligent query generation function)
            search_queries = create_intelligent_search_queries(config, cleaned_gap, full_document_context) # 生成搜索查询 (Generate search queries)

            for query in search_queries: # 遍历每个搜索查询 (Iterate through each search query)
                # 使用 asyncio.to_thread 在事件循环中运行同步搜索函数 (Use asyncio.to_thread to run synchronous search function in event loop)
                search_results = await asyncio.to_thread(perform_search, config, query) # 执行搜索 (Perform search)
                if not search_results: # 如果没有搜索结果 (If no search results)
                    logging.info(f"    查询 '{query}' 没有结果。(Query '{query}' returned no results.)") # 中文翻译 (Chinese translation)
                    continue
                
                for res_item in search_results: # 遍历每个搜索结果项 (Iterate through each search result item)
                    url = res_item.get('link') # 获取 URL (Get URL)
                    # V8 修改：将特定查询传递给抓取器/摘要器 (V8 Modification: Pass specific query to scraper/summarizer)
                    if url: all_tasks.append(scrape_and_summarize_async(session, config, url, cleaned_gap, query)) # 添加抓取和摘要任务 (Add scrape and summarize task)

        if not all_tasks: # 如果没有创建任何抓取任务 (If no scrape tasks were created)
            logging.warning("--- 异步研究周期中未创建任何抓取任务 --- (--- No scraping tasks created during asynchronous research cycle ---)") # 中文翻译 (Chinese translation)
            return ""

        logging.info(f"正在并发执行 {len(all_tasks)} 个抓取/摘要任务... (Concurrently executing {len(all_tasks)} scraping/summarizing tasks...)") # 中文翻译 (Chinese translation)
        completed_briefs = await asyncio.gather(*all_tasks, return_exceptions=True) # 并发执行所有任务 (Execute all tasks concurrently)

        final_brief_list = [str(b) for b in completed_briefs if b and not isinstance(b, Exception)] # 整理有效的简报 (Organize valid briefs)

    if not final_brief_list: # 如果没有有效的简报 (If no valid briefs)
        logging.warning("--- V8 智能研究周期未产生任何有效简报 --- (--- V8 intelligent research cycle did not produce any valid briefs ---)") # 中文翻译 (Chinese translation)
        return ""
        
    logging.info(f"--- V8 知识发现完成，生成了 {len(final_brief_list)} 份简报 --- (--- V8 knowledge discovery complete, generated {len(final_brief_list)} briefs ---)") # 中文翻译 (Chinese translation)
    return "\n\n===== 研究简报开始 =====\n\n" + "\n".join(final_brief_list) + "\n===== 研究简报结束 =====\n\n"


def extract_knowledge_gaps(feedback: str) -> list[str]: # 从反馈中提取知识空白 (Extract knowledge gaps from feedback)
    match = re.search(r'###?\s*KNOWLEDGE GAPS\s*###?\s*\n(.*?)(?=\n##|\n<file_marker>|$)', feedback, re.DOTALL | re.IGNORECASE) # 匹配 "KNOWLEDGE GAPS" 部分 (Match "KNOWLEDGE GAPS" section)
    if not match: logging.info("反馈中未找到 'KNOWLEDGE GAPS' 部分。(No 'KNOWLEDGE GAPS' section found in feedback.)"); return [] # 中文翻译 (Chinese translation)
    content = match.group(1).strip() # 获取匹配内容并去除空白 (Get matched content and remove whitespace)
    gaps = [g.strip() for g in re.split(r'\n\s*(?:\d+\.|\-|\*)\s*', content) if g.strip()] # 分割并清理知识空白项 (Split and clean knowledge gap items)
    logging.info(f"从反馈中提取了 {len(gaps)} 个知识空白。") # 中文翻译 (Chinese translation)
    return gaps

def _extract_json_from_ai_response(config: Config, response_text: str, context_for_error_log: str = "AI response") -> str | None:
    """
    使用“三振出局”法从 AI 的文本响应中稳健地提取 JSON 字符串的辅助函数。
    Returns a string that is likely to be parsable by json.loads(), or None.
    """ # 中文翻译 (Helper function to robustly extract JSON string from AI's text response using "three strikes" method. Returns a string that is likely to be parsable by json.loads(), or None.)
    logging.debug(f"Attempting to extract JSON from: {response_text[:500]}... Context: {context_for_error_log}")

    # Helper to try parsing and log success
    def _try_parse(s_to_parse, stage_msg):
        if not s_to_parse or s_to_parse.isspace():
            logging.debug(f"  _try_parse: String is empty or whitespace at stage '{stage_msg}'.")
            return None, False
        try:
            json.loads(s_to_parse)
            logging.info(f"  JSON successfully parsed after {stage_msg}. Length: {len(s_to_parse)}")
            logging.debug(f"    Content parsed: {s_to_parse[:300]}...")
            return s_to_parse, True
        except json.JSONDecodeError as e:
            logging.debug(f"  JSON parsing failed after {stage_msg}. Error: {e}. String (first 300 chars): {s_to_parse[:300]}...")
            return s_to_parse, False

    # 1. Initial extraction
    extracted_str = None
    logging.debug(f"  Starting JSON extraction from raw response (length {len(response_text)}).")
    match_fenced = re.search(r'```(?:json)?\s*([\s\S]+?)\s*```', response_text, re.DOTALL)
    if match_fenced:
        extracted_str = match_fenced.group(1).strip()
        logging.debug(f"Found fenced JSON block: {extracted_str[:200]}...")
    else:
        # Try to find first '{' or '[' and last '}' or ']'
        first_brace = response_text.find('{')
        last_brace = response_text.rfind('}')
        first_bracket = response_text.find('[')
        last_bracket = response_text.rfind(']')

        start_index = -1
        end_index = -1

        if first_brace != -1 and last_brace > first_brace:
            start_index = first_brace
            end_index = last_brace + 1
        
        if first_bracket != -1 and last_bracket > first_bracket:
            if start_index == -1 or first_bracket < start_index : # Prefer the one that starts earlier
                start_index = first_bracket
            if end_index == -1 or last_bracket + 1 > end_index: # Prefer the one that ends later
                 # This logic needs care if they are not nested properly or mixed.
                 # If both present, and one encompasses the other, prefer the outer.
                 # For simplicity now, if bracket starts earlier, it might be the primary one.
                 # If object was already chosen, but array ends later and starts before object ends.
                if not (start_index == first_brace and first_bracket > first_brace and last_bracket < last_brace) : # if array is not inside object
                     end_index = last_bracket + 1


        if start_index != -1 and end_index != -1:
            extracted_str = response_text[start_index:end_index].strip()
            logging.debug(f"Found JSON-like block (heuristic): {extracted_str[:200]}...")
        else:
            extracted_str = response_text.strip() # Fallback to the whole response
            logging.debug(f"No clear JSON block markers (```, {{}}, []) found, using whole stripped response (length {len(extracted_str)}). Preview: {extracted_str[:300]}...")

    if not extracted_str or extracted_str.isspace():
        logging.warning(f"No content to extract JSON from in {context_for_error_log} after initial extraction attempts.")
        return None
    logging.info(f"  Initial extracted string (length {len(extracted_str)}). Preview: {extracted_str[:300]}...")

    # Attempt 1: Direct parse of extracted string
    candidate_json_str, parsed_ok = _try_parse(extracted_str, "initial extraction")
    if parsed_ok:
        return candidate_json_str

    logging.info(f"  Initial JSON parsing failed for {context_for_error_log}. Current candidate length {len(candidate_json_str)}. Attempting pre-repair...")

    # Attempt 2: Regex pre-repair
    pre_repaired_str = preprocess_json_string(candidate_json_str) # candidate_json_str is the string that failed parsing
    logging.info(f"  String after regex pre-repair (length {len(pre_repaired_str)}). Preview: {pre_repaired_str[:300]}...")
    candidate_json_str, parsed_ok = _try_parse(pre_repaired_str, "regex pre-repair")
    if parsed_ok:
        return candidate_json_str
    
    # candidate_json_str is now pre_repaired_str if _try_parse returned it, or None if pre_repaired_str was empty
    if not candidate_json_str or candidate_json_str.isspace():
        logging.warning(f"  String became empty or invalid after pre-repair from {context_for_error_log}. Cannot proceed to AI fix.")
        return None

    logging.info(f"  JSON parsing after regex pre-repair failed for {context_for_error_log}. Current candidate length {len(candidate_json_str)}. Attempting AI fix...")

    # Attempt 3: AI fixer
    # Ensure config is available, if not, this step is skipped.
    # This function might be called from places where config is not directly passed.
    # For now, assuming config is accessible or passed in if this function is refactored.
    # Let's assume 'config' is available in the scope or passed as an argument.
    # This needs to be handled if _extract_json_from_ai_response is used outside a class with self.config
    # For now, this function is a global function, so it needs config passed to it.
    # Plan says modify _extract_json_from_ai_response, will need to adjust its signature or how it gets config.
    # For now, I'll write the logic assuming `config` is passed.
    # The actual call to `call_ai` for json_fixer_model_name will be added in the next step
    # when the function signature is updated. (This is being done now)

    # Attempt 3: AI fixer
    # First, apply pre-processing to the string we are about to send to the fixer AI.
    # This helps the fixer AI by giving it a cleaner starting point.
    string_to_send_to_fixer = candidate_json_str # This is already pre_repaired_str from Attempt 2, or original if pre_repair was skipped
    if not string_to_send_to_fixer or string_to_send_to_fixer.isspace(): # Should have been caught before, but double check
        logging.warning(f"  String is empty before AI fix stage for {context_for_error_log}. Skipping AI fix.")
    else:
        logging.info(f"  Preparing to send to AI JSON fixer. String to fix (length {len(string_to_send_to_fixer)}). Preview: {string_to_send_to_fixer[:300]}...")
        fixer_prompt = f"The following text is supposed to be a valid JSON string, but it's malformed. Please fix it and return ONLY the corrected, valid JSON string. Do not add any explanations, apologies, or markdown formatting like ```json ... ``` around the JSON.\n\nMalformed JSON attempt:\n```\n{string_to_send_to_fixer}\n```\n\nCorrected JSON string:"
        
        logging.info(f"  Attempting AI JSON fix using model: {config.json_fixer_model_name} for {context_for_error_log}...")
        ai_fixed_str_raw = call_ai(config, config.json_fixer_model_name,
                                    [{"role": "user", "content": fixer_prompt}],
                                    max_tokens_output=max(2048, int(len(string_to_send_to_fixer) * 1.5)),
                                    temperature=0.0)

        if "AI模型调用失败" in ai_fixed_str_raw or not ai_fixed_str_raw.strip():
            logging.error(f"  AI JSON fixer call failed or returned empty for {context_for_error_log}. Fixer response preview: {ai_fixed_str_raw[:200]}")
        else:
            logging.info(f"  AI JSON fixer response (raw, length {len(ai_fixed_str_raw)}). Preview: {ai_fixed_str_raw[:300]}...")
            
            cleaned_ai_fixed_str = ai_fixed_str_raw.strip()
            match_fenced_fixer = re.search(r'```(?:json)?\s*([\s\S]+?)\s*```', ai_fixed_str_raw, re.DOTALL)
            if match_fenced_fixer:
                cleaned_ai_fixed_str = match_fenced_fixer.group(1).strip()
                logging.info(f"  Extracted content from AI fixer's fenced block (length {len(cleaned_ai_fixed_str)}). Preview: {cleaned_ai_fixed_str[:200]}...")
            else:
                logging.info(f"  No fenced block in AI fixer response. Using stripped raw response (length {len(cleaned_ai_fixed_str)}).")
            
            if cleaned_ai_fixed_str:
                final_repaired_str_by_ai = preprocess_json_string(cleaned_ai_fixed_str)
                logging.info(f"  AI fixer output after final preprocess (length {len(final_repaired_str_by_ai)}). Preview: {final_repaired_str_by_ai[:300]}...")
                
                candidate_json_str_after_ai_fix, parsed_ok_after_ai_fix = _try_parse(final_repaired_str_by_ai, "AI fix and final pre-repair")
                if parsed_ok_after_ai_fix:
                    return candidate_json_str_after_ai_fix # Success!
            else:
                logging.warning(f"  AI JSON fixer produced an empty string after initial cleaning for {context_for_error_log}.")

    # If all attempts failed, candidate_json_str holds the string from the last failed stage (likely after regex pre-repair)
    logging.warning(f"Could not confidently extract or parse a JSON block from {context_for_error_log} after all three attempts. Final candidate string that failed (length {len(candidate_json_str)}). Preview: {candidate_json_str[:300]}...")
    return None


# Old generate_document_outline function is now removed.
# The new generate_document_outline_with_tools is added before call_ai_core.

def allocate_content_lengths(config: Config, outline_data: dict, total_target_chars: int) -> dict:
    logging.info(f"\n--- Allocating content lengths to outline (Total target: {total_target_chars} chars) ---")
    if not outline_data or "outline" not in outline_data or not outline_data["outline"]:
        logging.error("  Invalid or empty outline data for length allocation."); return outline_data

    outline_items = outline_data["outline"]

    outline_items_dicts = [item for item in outline_items if isinstance(item, dict)]
    if not outline_items_dicts:
        logging.error("  No valid dictionary items found in outline for length allocation.")
        return outline_data

    # Helper to safely convert ratio to float
    def _get_ratio(item_dict, key="target_chars_ratio"):
        ratio_val = item_dict.get(key)
        if ratio_val is None:
            return None # Explicitly None if not found
        try:
            return float(ratio_val)
        except (ValueError, TypeError):
            logging.warning(f"  Could not convert target_chars_ratio '{ratio_val}' to float for item '{item_dict.get('title', 'N/A')}'. Treating as 0.")
            return 0.0

    # Phase 1: Assign ratios if missing, ensuring numeric types
    # Convert all existing ratios to float first and handle errors
    for item in outline_items_dicts:
        numeric_ratio = _get_ratio(item)
        if numeric_ratio is not None:
            item["target_chars_ratio_numeric"] = numeric_ratio
        else:
            item["target_chars_ratio_numeric"] = None # Mark as needing assignment or default

    items_with_ratio = [item for item in outline_items_dicts if item.get("target_chars_ratio_numeric") is not None and item["target_chars_ratio_numeric"] > 0]
    items_without_ratio = [item for item in outline_items_dicts if item.get("target_chars_ratio_numeric") is None or item["target_chars_ratio_numeric"] <= 0]
    
    current_total_assigned_ratio = sum(item.get("target_chars_ratio_numeric", 0.0) for item in items_with_ratio)

    if items_without_ratio:
        logging.info(f"  {len(items_without_ratio)} top-level items without pre-assigned ratio. Total ratio from others: {current_total_assigned_ratio:.3f}")
        remaining_ratio_pool = max(0, 1.0 - current_total_assigned_ratio)
        if len(items_without_ratio) > 0:
            ratio_per_unassigned = remaining_ratio_pool / len(items_without_ratio) if remaining_ratio_pool > 0 else \
                                   (1.0 / len(outline_items_dicts) if current_total_assigned_ratio == 0 and len(outline_items_dicts) > 0 else 0)
            for item in items_without_ratio: item["target_chars_ratio_numeric"] = ratio_per_unassigned # Use numeric key
            logging.info(f"  Assigned ~{ratio_per_unassigned:.3f} ratio to each unassigned top-level item.")

    # Phase 2: Normalize all ratios to sum to 1.0
    final_total_ratio = sum(item.get("target_chars_ratio_numeric", 0.0) for item in outline_items_dicts) # Use numeric key
    if final_total_ratio == 0 and outline_items_dicts:
        logging.warning("  All top-level items have zero or no ratio after assignment. Assigning equal share as fallback.")
        equal_share = 1.0 / len(outline_items_dicts)
        for item in outline_items_dicts: item["target_chars_ratio_numeric"] = equal_share # Use numeric key
        final_total_ratio = 1.0
    elif abs(final_total_ratio - 1.0) > 0.001 and final_total_ratio > 0:
        logging.info(f"  Normalizing total top-level ratio from {final_total_ratio:.3f} to 1.0.")
        for item in outline_items_dicts:
            # Use numeric key for both getting and setting
            item["target_chars_ratio_numeric"] = item.get("target_chars_ratio_numeric", 0.0) / final_total_ratio

    # Phase 3: Allocate characters based on normalized ratios and distribute remainder
    total_allocated_chars = 0
    for item in outline_items_dicts:
        item['allocated_chars'] = int(round(item.get("target_chars_ratio_numeric", 0.0) * total_target_chars)) # Use numeric key
        total_allocated_chars += item['allocated_chars']

    remainder_chars = total_target_chars - total_allocated_chars
    if remainder_chars != 0 and outline_items_dicts:
        logging.info(f"  Distributing {remainder_chars} remainder characters among {len(outline_items_dicts)} top-level items using round-robin.")
        # Use numeric key for sorting
        sorted_items_for_remainder = sorted(outline_items_dicts, key=lambda x: x.get('target_chars_ratio_numeric', 0.0), reverse=(remainder_chars > 0))

        for i in range(abs(remainder_chars)):
            item_to_adjust = sorted_items_for_remainder[i % len(sorted_items_for_remainder)]
            adjustment = 1 if remainder_chars > 0 else -1

            if item_to_adjust['allocated_chars'] + adjustment >= config.min_allocated_chars_for_section // 2 :
                item_to_adjust['allocated_chars'] += adjustment
            else:
                logging.warning(f"    Could not fully distribute remainder {remainder_chars}; item '{item_to_adjust.get('title')}' would go below minimum.")

    # Final logging and recursive call for sub-sections
    final_sum_chars_check = 0
    for item in outline_items_dicts:
        if item['allocated_chars'] > 0 and item['allocated_chars'] < config.min_allocated_chars_for_section:
            logging.warning(f"  Chapter '{item.get('title')}' allocated chars {item['allocated_chars']} is below minimum {config.min_allocated_chars_for_section}. Adjusting if possible or will be skipped.")

        final_sum_chars_check += item['allocated_chars']
        # Use numeric key for logging
        logging.info(f"  - Chapter '{item.get('title', 'N/A')}' (Final Ratio: {item.get('target_chars_ratio_numeric',0.0):.3f}): Allocated {item['allocated_chars']} chars")
        if "sections" in item and isinstance(item["sections"], list) and item["sections"]:
            # Pass the numeric ratio key name or expect _allocate_recursive to also use a helper like _get_ratio
            item["sections"] = _allocate_recursive(config, item["sections"], item['allocated_chars'])

    if final_sum_chars_check != total_target_chars and outline_items_dicts:
        discrepancy = total_target_chars - final_sum_chars_check
        logging.warning(f"  Total allocated chars ({final_sum_chars_check}) for top-level differs from target ({total_target_chars}) by {discrepancy}. Adjusting last non-zero item.")
        for item_to_adjust_final in reversed(outline_items_dicts):
            if item_to_adjust_final['allocated_chars'] > abs(discrepancy) or item_to_adjust_final['allocated_chars'] > config.min_allocated_chars_for_section :
                item_to_adjust_final['allocated_chars'] += discrepancy
                logging.info(f"  Adjusted item '{item_to_adjust_final.get('title')}' by {discrepancy}. New alloc: {item_to_adjust_final['allocated_chars']}.")
                break
        else:
            outline_items_dicts[-1]['allocated_chars'] += discrepancy
            logging.info(f"  Fallback: Adjusted last top-level item by {discrepancy}. New alloc: {outline_items_dicts[-1]['allocated_chars']}")
        if outline_items_dicts[-1]['allocated_chars'] < 0: outline_items_dicts[-1]['allocated_chars'] = 0


    if config.session_dir:
        with open(os.path.join(config.session_dir, "allocated_document_outline.json"), "w", encoding="utf-8") as f:
            json.dump(outline_data, f, ensure_ascii=False, indent=4)
    logging.info("--- Content length allocation complete ---")
    return outline_data

def _allocate_recursive(config: Config, sections_list: list, parent_allocated_chars: int) -> list:
    sections_list_dicts = [s for s in sections_list if isinstance(s, dict)]
    if not sections_list_dicts: return sections_list

    # Helper to safely convert ratio to float (can be shared or redefined if this func moves to a class/module)
    def _get_ratio_recursive(item_dict, key="target_chars_ratio"):
        ratio_val = item_dict.get(key)
        if ratio_val is None:
            return None
        try:
            return float(ratio_val)
        except (ValueError, TypeError):
            logging.warning(f"  _recursive: Could not convert target_chars_ratio '{ratio_val}' to float for item '{item_dict.get('title', 'N/A')}'. Treating as 0.")
            return 0.0

    for s_item in sections_list_dicts:
        numeric_ratio = _get_ratio_recursive(s_item)
        if numeric_ratio is not None:
            s_item["target_chars_ratio_numeric"] = numeric_ratio
        else:
            s_item["target_chars_ratio_numeric"] = None # Mark for assignment

    items_with_ratio = [s for s in sections_list_dicts if s.get("target_chars_ratio_numeric") is not None and s["target_chars_ratio_numeric"] > 0]
    items_without_ratio = [s for s in sections_list_dicts if s.get("target_chars_ratio_numeric") is None or s["target_chars_ratio_numeric"] <= 0]
    current_total_assigned_ratio_level = sum(s.get("target_chars_ratio_numeric", 0.0) for s in items_with_ratio)

    if items_without_ratio:
        remaining_ratio_pool_level = max(0, 1.0 - current_total_assigned_ratio_level)
        if len(items_without_ratio) > 0:
            ratio_per_unassigned_level = remaining_ratio_pool_level / len(items_without_ratio) if remaining_ratio_pool_level > 0 else \
                                           (1.0 / len(sections_list_dicts) if current_total_assigned_ratio_level == 0 and len(sections_list_dicts) > 0 else 0)
            for s_item in items_without_ratio: s_item["target_chars_ratio_numeric"] = ratio_per_unassigned_level # Use numeric key

    final_total_ratio_level = sum(s_item.get("target_chars_ratio_numeric", 0.0) for s_item in sections_list_dicts) # Use numeric key
    if final_total_ratio_level == 0 and sections_list_dicts:
        equal_share_level = 1.0 / len(sections_list_dicts)
        for s_item in sections_list_dicts: s_item["target_chars_ratio_numeric"] = equal_share_level # Use numeric key
        final_total_ratio_level = 1.0
    elif abs(final_total_ratio_level - 1.0) > 0.001 and final_total_ratio_level > 0:
        for s_item in sections_list_dicts:
            # Use numeric key for both getting and setting
            s_item["target_chars_ratio_numeric"] = s_item.get("target_chars_ratio_numeric", 0.0) / final_total_ratio_level

    running_total_chars_subsection = 0
    for s_item in sections_list_dicts:
        s_item['allocated_chars'] = int(round(s_item.get("target_chars_ratio_numeric", 0.0) * parent_allocated_chars)) # Use numeric key
        running_total_chars_subsection += s_item['allocated_chars']

    remainder_s_chars = parent_allocated_chars - running_total_chars_subsection
    if remainder_s_chars != 0 and sections_list_dicts:
        logging.info(f"    Distributing {remainder_s_chars} sub-section remainder chars among {len(sections_list_dicts)} items.")
        # Use numeric key for sorting
        sorted_s_items_for_remainder = sorted(sections_list_dicts, key=lambda x: x.get('target_chars_ratio_numeric', 0.0), reverse=(remainder_s_chars > 0))
        for i_s in range(abs(remainder_s_chars)):
            item_to_adjust_s = sorted_s_items_for_remainder[i_s % len(sorted_s_items_for_remainder)]
            adjustment_s = 1 if remainder_s_chars > 0 else -1
            if item_to_adjust_s['allocated_chars'] + adjustment_s >= 0:
                item_to_adjust_s['allocated_chars'] += adjustment_s
            else:
                logging.warning(f"    Could not fully distribute sub-remainder {remainder_s_chars}; item '{item_to_adjust_s.get('title')}' would go below 0.")

    final_sum_chars_subsection_check = 0
    for s_item in sections_list_dicts:
        if s_item['allocated_chars'] > 0 and s_item['allocated_chars'] < config.min_allocated_chars_for_section // 2 :
            logging.warning(f"    Sub-section '{s_item.get('title')}' alloc {s_item['allocated_chars']} is very small.")

        final_sum_chars_subsection_check += s_item.get('allocated_chars',0)
        # Use numeric key for logging
        logging.info(f"    - Sub-section '{s_item.get('title', 'N/A')}' (Final Ratio: {s_item.get('target_chars_ratio_numeric',0.0):.3f}): Allocated {s_item.get('allocated_chars',0)} chars")
        if "sections" in s_item and isinstance(s_item["sections"], list) and s_item["sections"]:
            s_item["sections"] = _allocate_recursive(config, s_item["sections"], s_item.get('allocated_chars',0)) # Recursive call remains the same

    if final_sum_chars_subsection_check != parent_allocated_chars and sections_list_dicts:
        discrepancy_s = parent_allocated_chars - final_sum_chars_subsection_check
        logging.info(f"    Adjusting last sub-section's allocation by {discrepancy_s} to match parent total {parent_allocated_chars}. Current sub-sum: {final_sum_chars_subsection_check}")
        item_to_adjust_final_s = sections_list_dicts[-1]
        for s_item_rev in reversed(sections_list_dicts):
            if s_item_rev['allocated_chars'] > abs(discrepancy_s) or s_item_rev['allocated_chars'] > config.min_allocated_chars_for_section // 2 :
                item_to_adjust_final_s = s_item_rev
                break

        item_to_adjust_final_s['allocated_chars'] += discrepancy_s
        logging.info(f"    Adjusted sub-section '{item_to_adjust_final_s.get('title')}' by {discrepancy_s}. New alloc: {item_to_adjust_final_s['allocated_chars']}.")
        if item_to_adjust_final_s['allocated_chars'] < 0:
            logging.error(f"    CRITICAL: Sub-section '{item_to_adjust_final_s.get('title')}' went negative ({item_to_adjust_final_s['allocated_chars']}) after sum adjustment. Clamping to 0.")
            item_to_adjust_final_s['allocated_chars'] = 0

    return sections_list_dicts

def save_checkpoint(config: Config, iteration: int, solution: str, feedback_history: list,
                    initial_problem: str, initial_solution_target_chars: int,
                    max_iterations: int, external_data_checksum: str,
                    document_outline_data: dict | None = None,
                    successful_patches: list[dict] | None = None,
                    research_briefs_history: list[str] | None = None,
                    style_guide: str | None = None
                    ):
    checkpoint_data = {
        "iteration": iteration,
        "current_solution": solution, "feedback_history": feedback_history,
        "initial_problem": initial_problem, "initial_solution_target_chars": initial_solution_target_chars,
        "max_iterations": max_iterations, "external_data_checksum": external_data_checksum,
        "document_outline_data": document_outline_data,
        "successful_patches": successful_patches if successful_patches else [],
        "research_briefs_history": research_briefs_history if research_briefs_history else [],
        "style_guide": style_guide
    }
    path = os.path.join(config.session_dir, config.checkpoint_file_name)
    try:
        with open(path, "w", encoding="utf-8") as f: json.dump(checkpoint_data, f, ensure_ascii=False, indent=4)
        logging.info(f"\n--- Checkpoint saved to {path} (Iteration {iteration + 1} completed stage) ---")
    except Exception as e: logging.error(f"\nError saving checkpoint: {e}")

def load_checkpoint(config: Config):
    path = os.path.join(config.session_dir, config.checkpoint_file_name)
    if not os.path.exists(path): return None
    try:
        with open(path, "r", encoding="utf-8") as f: checkpoint_data = json.load(f)
        logging.info(f"\n--- Checkpoint loaded successfully from {path} (Saved after Iteration {checkpoint_data.get('iteration', -1) + 1} stage) ---")
        if "successful_patches" not in checkpoint_data: checkpoint_data["successful_patches"] = []
        if "research_briefs_history" not in checkpoint_data: checkpoint_data["research_briefs_history"] = []
        if "style_guide" not in checkpoint_data: checkpoint_data["style_guide"] = ""
        return checkpoint_data
    except Exception as e:
        logging.error(f"\nError loading checkpoint: {e}. Deleting corrupted file.")
        try: os.remove(path)
        except Exception as re_err: logging.error(f"Error deleting corrupted checkpoint: {re_err}")
    return None

def delete_checkpoint(config: Config):
    path = os.path.join(config.session_dir, config.checkpoint_file_name)
    if os.path.exists(path):
        try: os.remove(path); logging.info(f"\n--- Checkpoint file {path} deleted ---")
        except Exception as e: logging.error(f"\nError deleting checkpoint file: {e}")

# --- Main Optimization Logic with Outline-First Approach ---
async def optimize_solution_with_two_ais(config: Config, initial_problem: str,
                                     style_guide: str,
                                     external_data: str = "",
                                     external_data_checksum: str = "",
                                     vector_db_manager: VectorDBManager | None = None
                                     ) -> tuple[str | None, list[dict], list[str], list[str], str | None]:
    logging.info("### V12: Starting RAG-Enhanced Optimization with Precise Context ###")
    logging.info(f"Initial Problem: {initial_problem[:150]}...")
    if external_data: logging.info(f"External data loaded ({len(external_data)} chars). Checksum: {external_data_checksum}")
    else: logging.info("No external data loaded.")

    # V12 RAG: Generate a unique ID for this document processing session for RAG collection naming
    doc_id_for_rag = hashlib.md5(f"{initial_problem}_{time.time()}".encode()).hexdigest()[:16]
    doc_content_collection_name = f"{config.vector_db_collection_name}_doc_content_{doc_id_for_rag}"
    logging.info(f"Document-specific RAG collection for this run: {doc_content_collection_name}")
    
    # V12 RAG: Function to update the document's RAG collection
    async def update_document_rag_collection(current_doc_text: str, outline_for_chunking: dict):
        if not vector_db_manager or not current_doc_text or not outline_for_chunking:
            logging.warning("  update_document_rag_collection: VectorDBManager, document text, or outline missing. Skipping RAG update.")
            return

        logging.info(f"  Updating RAG collection '{doc_content_collection_name}' for current document content...")
        doc_chunks, doc_metadatas = chunk_document_for_rag(config, current_doc_text, outline_for_chunking, doc_id_for_rag)
        
        if not doc_chunks:
            logging.warning(f"  No chunks generated from current document for RAG collection '{doc_content_collection_name}'.")
            return

        # Prepare IDs for ChromaDB
        doc_chunk_ids = [f"doc_{doc_id_for_rag}_chunk_{i}_{hashlib.md5(chunk.encode()).hexdigest()[:8]}" for i, chunk in enumerate(doc_chunks)]
        
        try:
            # Attempt to delete existing chunks for this doc_id to ensure freshness.
            # This is a simple way; more advanced would be targeted deletions or upserts.
            # ChromaDB's add with same IDs overwrites, so explicit delete might not be needed if IDs are stable.
            # However, if chunking changes, old chunks might remain.
            # A robust way: get all IDs with {"doc_id": doc_id_for_rag} and delete them.
            # For now, let's assume `add_experience` handles updates correctly if IDs clash or make IDs very unique.
            # For simplicity in this step, we'll just add. If using `collection.add` directly, it overwrites.
            # If VectorDBManager's `add_experience` uses `collection.upsert` or handles overwrites by ID, this is fine.
            # Let's ensure our VectorDBManager can handle a collection_name argument.
            # (This implies VectorDBManager.add_experience and .retrieve_experience need collection_name param)
            
            # If the collection was just created for this doc_id, we might not need to delete.
            # But if resuming, or multiple updates, cleaning is good.
            # For now, let's rely on chromadb.add overwriting if IDs are identical.
            # If chunking changes (e.g. section added/deleted), old chunks of deleted sections might persist if not explicitly removed.
            # A simple clear for this specific doc_id:
            existing_ids_in_doc_collection = vector_db_manager.collection.get(
                where={"doc_id": doc_id_for_rag},
                include=[] # Don't need documents/embeddings, just IDs
            ).get("ids", [])

            if existing_ids_in_doc_collection:
                logging.info(f"  Found {len(existing_ids_in_doc_collection)} existing chunks for doc_id '{doc_id_for_rag}'. Deleting before re-adding.")
                vector_db_manager.collection.delete(ids=existing_ids_in_doc_collection, where={"doc_id": doc_id_for_rag}) # Target delete

            logging.info(f"  Adding/Updating {len(doc_chunks)} chunks in RAG collection '{doc_content_collection_name}'.")
            vector_db_manager.add_experience(
                texts=doc_chunks,
                metadatas=doc_metadatas,
                ids=doc_chunk_ids
                # collection_name=doc_content_collection_name # Assuming VectorDBManager is modified to accept this
                # For now, if VectorDBManager is not modified, this won't work as intended.
                # Let's assume for this step that we are operating on a temporarily switched collection in VectorDBManager
                # or the manager is adapted. The ideal is passing collection_name.
                # For now, let's manage the specific collection directly if vector_db_manager.client is accessible
            )
            if vector_db_manager.client: # If we can access the client directly
                doc_specific_collection = vector_db_manager.client.get_or_create_collection(name=doc_content_collection_name)
                embeddings_for_rag = vector_db_manager.embedding_model.get_embeddings(doc_chunks)
                
                valid_chunks, valid_embeddings, valid_metadatas_for_rag, valid_ids_for_rag = [], [], [], []
                for i_emb, emb_item in enumerate(embeddings_for_rag):
                    if emb_item:
                        valid_chunks.append(doc_chunks[i_emb])
                        valid_embeddings.append(emb_item)
                        valid_metadatas_for_rag.append(doc_metadatas[i_emb])
                        valid_ids_for_rag.append(doc_chunk_ids[i_emb])
                
                if valid_chunks:
                    doc_specific_collection.delete(where={"doc_id": doc_id_for_rag}) # Clear previous for this doc
                    doc_specific_collection.add(
                        ids=valid_ids_for_rag,
                        embeddings=valid_embeddings,
                        documents=valid_chunks,
                        metadatas=valid_metadatas_for_rag
                    )
                    logging.info(f"  Successfully added/updated {len(valid_chunks)} chunks to specific RAG collection '{doc_content_collection_name}'.")
                else:
                    logging.warning("  No valid embeddings generated for RAG chunks. Collection not updated.")
            else:
                logging.warning("  Cannot directly access vector_db_manager.client to manage doc-specific RAG collection. RAG updates may be compromised.")

        except Exception as e_rag_update:
            logging.error(f"  Error updating document RAG collection '{doc_content_collection_name}': {e_rag_update}", exc_info=True)

    current_solution: str | None = None
    feedback_history: list[str] = []
    document_outline_data: dict | None = None
    start_iteration_index: int = 0

    successful_patches_history: list[dict] = []
    all_research_briefs: list[str] = []

    loaded_checkpoint = load_checkpoint(config)
    if loaded_checkpoint:
        is_same_task = (
            loaded_checkpoint.get("initial_problem") == initial_problem and
            loaded_checkpoint.get("initial_solution_target_chars") == config.initial_solution_target_chars and
            loaded_checkpoint.get("external_data_checksum") == external_data_checksum and
            loaded_checkpoint.get("document_outline_data") is not None
        )
        if is_same_task:
            completed_iter_idx = loaded_checkpoint["iteration"]
            start_iteration_index = completed_iter_idx + 1
            current_solution = loaded_checkpoint["current_solution"]
            feedback_history = loaded_checkpoint["feedback_history"]
            document_outline_data = loaded_checkpoint["document_outline_data"]
            successful_patches_history = loaded_checkpoint.get("successful_patches", [])
            all_research_briefs = loaded_checkpoint.get("research_briefs_history", [])
            style_guide = loaded_checkpoint.get("style_guide", style_guide)
            logging.info(f"--- Resuming from Iteration {start_iteration_index + 1} (V11 Flow) ---")
            logging.info(f"  Current solution length: {len(current_solution) if current_solution else 0} chars. Outline and Style Guide loaded.")
        else:
            logging.info("--- Checkpoint mismatch or incompatible. Starting fresh. ---")
            delete_checkpoint(config)
            loaded_checkpoint = None

    if not loaded_checkpoint:
        start_iteration_index = 0
        current_solution = None
        feedback_history = []
        successful_patches_history = []
        all_research_briefs = []
        raw_outline = generate_document_outline_with_tools(config, initial_problem) # Use new tool-based function
        if not raw_outline or "outline" not in raw_outline or not raw_outline["outline"]:
            logging.error("CRITICAL: Failed to generate a valid document outline using tools. Cannot proceed.")
            return None, [], [], [], "Error: Document outline generation failed (tool based)."
        document_outline_data = allocate_content_lengths(config, raw_outline, config.initial_solution_target_chars)
        if not document_outline_data.get("outline"):
            logging.error("CRITICAL: Failed to allocate content lengths to the outline. Cannot proceed.")
            return None, [], [], [], "Error: Content length allocation failed."

        if config.interactive_mode:
            logging.info("\n--- Interactive Outline Review ---")
            print("\nGenerated Document Outline (with allocated characters):")
            print(json.dumps(document_outline_data, ensure_ascii=False, indent=2))
            while True:
                action = input("Accept outline (a), regenerate (r), or note for manual edit & continue (m)? [a/r/m]: ").lower()
                if action == 'a':
                    logging.info("Outline accepted by user.")
                    break
                elif action == 'r':
                    logging.info("Regenerating outline as per user request...")
                    raw_outline = generate_document_outline_with_tools(config, initial_problem) # Updated call
                    if not raw_outline or "outline" not in raw_outline or not raw_outline["outline"]:
                        logging.error("CRITICAL: Failed to regenerate a valid document outline (with tools). Cannot proceed with regeneration.")
                        # Stick with the previous outline or ask again
                        if input("Regeneration failed. Continue with previous outline? (y/n): ").lower() != 'y':
                             return None, [], [], [], "Error: Document outline regeneration failed and user chose to stop."
                        # else, we continue with the old document_outline_data before this 'r' attempt
                        break
                    document_outline_data = allocate_content_lengths(config, raw_outline, config.initial_solution_target_chars)
                    if not document_outline_data.get("outline"):
                        logging.error("CRITICAL: Failed to allocate content lengths to the regenerated outline. Cannot proceed with regeneration.")
                        if input("Length allocation for regenerated outline failed. Continue with previous outline? (y/n): ").lower() != 'y':
                            return None, [], [], [], "Error: Length allocation for regenerated outline failed and user chose to stop."
                        break
                    logging.info("Outline regenerated and lengths reallocated.")
                    print("\nNewly Generated Document Outline:")
                    print(json.dumps(document_outline_data, ensure_ascii=False, indent=2))
                    # Loop to accept the new outline or try again
                elif action == 'm':
                    logging.info("User noted to manually edit outline if needed. To do so, edit 'allocated_document_outline.json' in the session's checkpoint and restart. For now, proceeding with the current/regenerated outline.")
                    # This option doesn't halt; it's a note for the user.
                    # If they truly want to stop and edit, they'd need to terminate the script.
                    break
                else:
                    print("Invalid input. Please enter 'a', 'r', or 'm'.")
        # End of interactive outline review block
    
    # V11 MODIFICATION: Initialize (or re-initialize if outline changed) the Context Manager
    context_manager = ContextManager(config, style_guide, document_outline_data, external_data)

    style_guide_prompt_block = f"""
[重要指令：你必须严格遵守以下《风格与声音指南》]
--- STYLE GUIDE START ---
{style_guide}
--- STYLE GUIDE END ---
""" if style_guide else ""

    system_prompt_base = f"{style_guide_prompt_block}\nYou are a top-tier expert with exceptional reasoning and problem-solving skills. Your primary duty is to strictly adhere to the user's original problem and the provided document outline. Ensure all content is highly relevant and avoids any topic deviation."
    
    # V11 MODIFICATION: The large data block is no longer part of the default system prompt
    # It will be managed and summarized by the ContextManager
    # V12 RAG PROMPT REFINEMENT for Critic:
    secondary_ai_base_critique = """你是一位一丝不苟的分析师和批判性思考者。你的任务是找出所提供解决方案中的缺点、错误、遗漏或需要改进的地方。
**在提供反馈时，请尽可能具体。如果你发现问题，请引用有问题的文本或精确描述其位置（例如，“在第 X 章第 Y 节，以‘…’开头的关于 Z 的段落不清楚，因为…”）。这种特殊性对于有针对性的修订至关重要。**

建设性的反馈应涵盖：
1.  准确性：是否存在任何事实错误或逻辑谬误？（引用或描述具体文本。）
2.  完整性：是否根据大纲全面涵盖了所有相关方面？（引用或描述与文档章节相关的具体缺失元素。）
3.  主题/大纲 adherence：是否严格遵守问题和大纲？（引用或描述具体偏差。）
4.  叙事连贯性：论证是否合乎逻辑？章节/小节之间的过渡是否平滑？（引用或描述具体有问题的过渡或逻辑跳跃。）
5.  风格一致性：语气、词汇和句子结构是否与风格指南一致？（引用或描述具体的不一致之处。）
6.  知识空白：**如果解决方案缺乏关键信息或需要外部知识，请在反馈末尾的“### KNOWLEDGE GAPS ###”下列出这些内容。这一点至关重要。**
    - [需要的具体数据，例如，“A.B 节关于现象 Q 的最新统计数据”]
    - [需要外部研究的论点，例如，“对 C 章中提出的理论的反驳论点”]
    - [可能缺失的最新进展，例如，“与结论中讨论相关的技术 T 在 2023 年的进展”]
请提供具体、可操作的反馈。如果解决方案非常出色，请明确说明。"""
    full_system_prompt_secondary_ai_template = f"{style_guide_prompt_block}\n" + secondary_ai_base_critique

    last_completed_iter_idx = -1

    if start_iteration_index >= config.max_iterations:
        logging.info(f"从已完成所有 {config.max_iterations} 次迭代的检查点恢复。进入最终处理阶段。(Resuming from a checkpoint that has already completed all {config.max_iterations} iterations. Proceeding to finalization.)")
        last_completed_iter_idx = config.max_iterations - 1
    else:
        try:
            for i in range(start_iteration_index, config.max_iterations):
                logging.info(f"\n--- 迭代 {i + 1}/{config.max_iterations} --- (--- Iteration {i + 1}/{config.max_iterations} ---)")
                if i == 0 and current_solution is None:
                    logging.info("使用 V11 上下文管理计划生成初始解决方案...(Generating initial solution with V11 Context-Managed Planning...)")
                    if not document_outline_data or not document_outline_data.get("outline"):
                        logging.error("严重错误：初始解决方案生成前缺少文档大纲。(CRITICAL: Document outline is missing before initial solution generation.)")
                        return None, [], [], [], "错误：文档大纲缺失。(Error: Document outline missing.)"

                    assembled_parts = []
                    remaining_chapters_queue = collections.deque(document_outline_data["outline"])
                    
                    while remaining_chapters_queue:
                        current_chapter = remaining_chapters_queue.popleft()
                        title = current_chapter.get("title", "Untitled Section")
                        desc = current_chapter.get("description", "N/A")
                        alloc_chars = current_chapter.get("allocated_chars", 0)

                        if alloc_chars < config.min_allocated_chars_for_section:
                            logging.warning(f"  跳过 '{title}' (分配字符数 {alloc_chars} < 最低要求 {config.min_allocated_chars_for_section})。(Skipping '{title}' (allocated chars {alloc_chars} < min {config.min_allocated_chars_for_section}).)")
                            assembled_parts.append(f"\n\n## {title}\n\n[因分配字符数不足 ({alloc_chars}) 而跳过内容]\n\n")
                            continue
                        
                        # V11.2 MODIFICATION: Differentiate context gathering based on subsections
                        if current_chapter.get("sections"): # Chapter has subsections
                            logging.info(f"  章节 '{title}' 包含子章节。逐个处理子章节。(Chapter '{title}' has subsections. Processing subsection by subsection.)")
                            chapter_subsection_contents = []
                            for sub_idx, subsection_item in enumerate(current_chapter["sections"]):
                                subsection_title = subsection_item.get("title", f"Untitled Subsection {sub_idx+1}")
                                subsection_desc = subsection_item.get("description", "N/A")
                                subsection_alloc_chars = subsection_item.get("allocated_chars", 0)

                                if subsection_alloc_chars < config.min_allocated_chars_for_section // 2: # Allow smaller for subsections
                                    logging.warning(f"    跳过子章节 '{subsection_title}' (在 '{title}' 中) (分配字符数 {subsection_alloc_chars} 过低)。(Skipping subsection '{subsection_title}' in '{title}' (allocated chars {subsection_alloc_chars} too low).)")
                                    chapter_subsection_contents.append(f"\n\n### {subsection_title}\n\n[小节内容跳过，因分配字符数过少 ({subsection_alloc_chars})]\n\n")
                                    context_manager.record_completed_subsection(title, subsection_title, "[因分配字符数过少而跳过]") # Record placeholder
                                    continue
                                
                                logging.info(f"    正在为子章节生成内容: '{title}' -> '{subsection_title}' (Generating content for subsection: '{title}' -> '{subsection_title}')")
                                context_for_subsection = context_manager.get_context_for_subsection(title, sub_idx)
                                
                                # TODO: generate_section_content will need to be adapted to take subsection details
                                # For now, it might still act as if generating a whole section, but with subsection-specific context and prompt (next plan phase)
                                # We'll pass the subsection's specific prompt (desc) and target length.
                                # The system_prompt_base and overall_context (context_for_subsection) are key.
                                # The `title` for generate_section_content should be the subsection_title.
                                subsection_content_str = generate_section_content(
                                    config,
                                    subsection_title, # Use subsection title
                                    subsection_desc,  # Use subsection description as its specific prompt
                                    system_prompt_base,
                                    subsection_alloc_chars,
                                    config.main_ai_model, # Or logic to choose heavy model
                                    context_for_subsection,
                                    is_final_pass_for_section=False # A subsection is not a final pass for itself in this context
                                )
                                chapter_subsection_contents.append(subsection_content_str) # This will include "## subsection_title"
                                context_manager.record_completed_subsection(title, subsection_title, subsection_content_str)
                            
                            # Concatenate subsection contents to form the main chapter's content
                            # generate_section_content already adds "## title", so we need to be careful here.
                            # For now, let's assume generate_section_content for subsections returns "### subsection_title\n content"
                            # and we will wrap it with the main chapter title.
                            # This will be refined when generate_section_content is refactored.
                            full_chapter_text_from_subsections = f"\n\n## {title}\n{current_chapter.get('description', '')}\n\n" + "".join(chapter_subsection_contents)
                            assembled_parts.append(full_chapter_text_from_subsections)
                            # Update context manager for the completed MAIN chapter (with its full content and for summary)
                            context_manager.update_completed_chapter_content(title, full_chapter_text_from_subsections)

                        else: # Chapter is standalone (no subsections)
                            logging.info(f"  章节 '{title}' 是独立的。直接生成其内容。(Chapter '{title}' is standalone. Generating its content directly.)")
                            context_for_generation = context_manager.get_context_for_standalone_chapter(title)
                            
                            content_str = generate_section_content(
                                config, title, desc, system_prompt_base, alloc_chars,
                                config.main_ai_model, context_for_generation, False
                                # is_final_pass_for_section is False here because chapter-level polish might happen later
                            )
                            assembled_parts.append(content_str)
                            # Update context manager for the completed MAIN chapter
                            context_manager.update_completed_chapter_content(title, content_str)

                        if config.enable_dynamic_outline_correction and remaining_chapters_queue:
                            logging.info(f"\n--- V11: 完成 '{title}' 后审查计划 --- (--- V11: Reviewing plan after completing '{title}' ---)")
                            summary_of_work_done = truncate_text_for_context(config, "".join(assembled_parts), 3000, "middle")
                            # Use the new tool-based function
                            revised_remaining_outline_list = review_and_correct_outline_with_tools(
                                config, initial_problem, summary_of_work_done, list(remaining_chapters_queue), style_guide
                            )

                            if revised_remaining_outline_list is not None:
                                total_chars_so_far = sum(len(part) for part in assembled_parts)
                                remaining_chars_target = max(config.min_allocated_chars_for_section * len(revised_remaining_outline_list), config.initial_solution_target_chars - total_chars_so_far)
                                logging.info(f"  V11: 目前总字数: {total_chars_so_far}。剩余目标字数: {remaining_chars_target}。(V11: Total chars so far: {total_chars_so_far}. Remaining target: {remaining_chars_target}.)")

                                temp_outline_for_realloc = {"outline": revised_remaining_outline_list}
                                reallocated_outline_data = allocate_content_lengths(config, temp_outline_for_realloc, remaining_chars_target)
                                remaining_chapters_queue = collections.deque(reallocated_outline_data.get("outline", []))
                                
                                # V11 MODIFICATION: Update the main outline object in the context manager
                                current_titles = {part.split('\n')[2].replace('## ', '').strip() for part in assembled_parts if part.startswith("\n\n## ")}
                                current_chapters = [ch for ch in document_outline_data['outline'] if ch.get('title') in current_titles]
                                context_manager.outline['outline'] = current_chapters + list(remaining_chapters_queue)
                                document_outline_data = context_manager.outline # Keep main variable in sync
                                logging.info("--- V11: 计划已自适应更新。--- (--- V11: Plan has been adaptively updated. ---)")
                            else:
                                logging.info("--- V11: 规划师认为无需更改。--- (--- V11: Planner decided no changes were needed. ---)")

                    current_solution = "".join(assembled_parts)
                    logging.info(f"初始解决方案已生成 ({len(current_solution)} 字符)。(Initial solution generated ({len(current_solution)} chars).)")
                    # V12 RAG: Populate the document-specific RAG collection
                    if vector_db_manager and document_outline_data:
                        await update_document_rag_collection(current_solution, document_outline_data)
                    
                    if config.session_dir:
                        with open(os.path.join(config.session_dir, f"iteration_{i}_initial_solution_v12.txt"), "w", encoding="utf-8") as f:
                            f.write(current_solution)

                elif current_solution is not None and feedback_history:
                    logging.info("根据先前的反馈应用补丁 (V12 RAG 增强版)...(Applying patches based on previous feedback (V12 RAG-Enhanced)...)")
                    
                    # V12 RAG: Retrieve relevant chunks from the document's own RAG collection based on critique
                    rag_excerpts_str = ""
                    if vector_db_manager and vector_db_manager.client:
                        critique_query = feedback_history[-1] # Use full critique as query for now
                        # More advanced: parse critique for specific points to query individually
                        logging.info(f"  正在使用评论查询文档 RAG '{doc_content_collection_name}': {critique_query[:200]}... (Querying doc-RAG '{doc_content_collection_name}' with critique: {critique_query[:200]}...)")
                        
                        doc_specific_collection_for_query = vector_db_manager.client.get_collection(name=doc_content_collection_name)
                        if doc_specific_collection_for_query.count() > 0: # query only if collection has items
                            query_embedding = vector_db_manager.embedding_model.get_embedding(critique_query)
                            if query_embedding:
                                rag_results = doc_specific_collection_for_query.query(
                                    query_embeddings=[query_embedding],
                                    n_results=min(5, doc_specific_collection_for_query.count()), # Get up to 5 results, or fewer if less items
                                    include=["documents", "metadatas", "distances"]
                                )
                                if rag_results and rag_results.get("documents") and rag_results["documents"][0]:
                                    excerpts_parts = ["\n\n[用于辅助修补的相关文档 RAG 摘录 (上下文：评论)]"]
                                    for rag_idx, doc_text in enumerate(rag_results["documents"][0]):
                                        meta = rag_results["metadatas"][0][rag_idx] if rag_results["metadatas"] and rag_results["metadatas"][0] else {}
                                        dist = rag_results["distances"][0][rag_idx] if rag_results["distances"] and rag_results["distances"][0] else -1.0
                                        excerpts_parts.append(f"\n--- RAG 摘录 {rag_idx+1} (来源: Ch '{meta.get('chapter_title','N/A')}' / Sec '{meta.get('section_title','N/A')}', 标题: '{meta.get('original_header', 'N/A')}', 距离: {dist:.4f}) ---\n{doc_text}")
                                    rag_excerpts_str = "\n".join(excerpts_parts) + "\n--- RAG 摘录结束 ---\n"
                                    logging.info(f"  已检索到 {len(rag_results['documents'][0])} 条 RAG 摘录用于修补。总长度: {len(rag_excerpts_str)} 字符。(Retrieved {len(rag_results['documents'][0])} RAG excerpts for patcher. Total length: {len(rag_excerpts_str)} chars.)")
                                else:
                                    logging.info("  在特定文档集合中未找到与评论相关的 RAG 摘录。(No relevant RAG excerpts found for the critique in doc-specific collection.)")
                            else:
                                logging.warning("  未能为评论查询获取嵌入向量。无法使用 RAG 进行修补。(Failed to get embedding for critique query. Cannot use RAG for patching.)")
                        else:
                            logging.info(f"  特定文档 RAG 集合 '{doc_content_collection_name}' 为空。跳过 RAG 查询以进行修补。(Doc-specific RAG collection '{doc_content_collection_name}' is empty. Skipping RAG query for patcher.)")
                    else:
                        logging.warning("  VectorDBManager 或其客户端不可用。跳过 RAG 以进行修补。(VectorDBManager or its client not available. Skipping RAG for patching.)")

                    # Determine which chapter is most relevant for patching based on feedback.
                    # This is a simplification; a more advanced method would parse feedback for explicit chapter mentions.
                    # For now, we'll assume the Patcher AI can figure it out or we target the whole doc.
                    # However, get_context_for_chapter_critique needs a chapter title.
                    # Let's try to find the first mentioned chapter in feedback, or default.
                    first_chapter_title_in_feedback = None
                    if document_outline_data and document_outline_data.get("outline"):
                        for chap in document_outline_data["outline"]:
                            if chap.get("title","").lower() in feedback_history[-1].lower():
                                first_chapter_title_in_feedback = chap.get("title")
                                break
                        if not first_chapter_title_in_feedback:
                             first_chapter_title_in_feedback = document_outline_data["outline"][0].get("title", "Introduction") # Fallback
                    
                    precise_context_for_patcher = ""
                    if first_chapter_title_in_feedback:
                        precise_context_for_patcher = context_manager.get_context_for_chapter_critique(first_chapter_title_in_feedback, current_solution)
                        precise_context_for_patcher = f"\n\n[Precise Context for Chapter '{first_chapter_title_in_feedback}' (Full N-1, Full N, Summary N+1)]\n{precise_context_for_patcher}\n--- End Precise Context ---\n"

                    # Patcher System Prompt for Tool Calling - V12 RAG Enhanced
                    patch_tool_sys_prompt = f"""{style_guide_prompt_block}
你是一位专家编辑。你的任务是根据给定的反馈和上下文信息，生成一个 JSON 补丁操作列表来修改提供的 Markdown 文本。
你必须使用 `generate_json_patch_list` 工具来提供你的响应。
如果不需要任何更改，请使用空的“补丁”列表调用该工具。
严格遵守原始问题陈述和整体文档大纲。
密切关注提供的任何“[文档中的相关 RAG 摘录]”和“[章节的精确上下文...]”以指导你的补丁。
确保补丁中的 `target_section` 使用在原始文档内容中找到的 EXACT Markdown 标题（例如，“## 引言”，“### 方法详情”），这可能在 RAG 摘录或精确上下文中有所提示。**不要为 `target_section` 发明新的标题；如果需要创建某个部分，那是一个大纲修订任务，而不是一个补丁。**
"""
                    patcher_tools = get_patcher_tool_definition()
                    outline_ctx_for_patcher = f"\n\n[Document Outline Reference]\n---\n{json.dumps(document_outline_data, ensure_ascii=False, indent=2)}\n---\n" if document_outline_data else ""
                    
                    # Token management for Patcher AI, including RAG excerpts and Precise Context
                    fixed_prompt_elements = [
                        patch_tool_sys_prompt,
                        initial_problem,
                        outline_ctx_for_patcher,
                        feedback_history[-1],
                        rag_excerpts_str, # V12 RAG
                        precise_context_for_patcher # V12 Precise Context
                    ]
                    fixed_prompt_parts_tokens_estimate = sum(config.count_tokens(el) for el in fixed_prompt_elements if el)
                    
                    MIN_SOLUTION_CONTEXT_TOKENS_FOR_PATCHER = 800 # Reduced slightly as other contexts are richer
                    safety_margin_patcher = 600

                    available_tokens_for_solution = config.max_context_for_long_text_review_tokens - fixed_prompt_parts_tokens_estimate - safety_margin_patcher
                    solution_tokens_for_patcher = max(MIN_SOLUTION_CONTEXT_TOKENS_FOR_PATCHER, available_tokens_for_solution)
                    
                    if available_tokens_for_solution < MIN_SOLUTION_CONTEXT_TOKENS_FOR_PATCHER:
                        logging.warning(f"  Patcher (Tool Call): available_tokens_for_solution ({available_tokens_for_solution}) is low. Solution context might be minimal.")
                    
                    # The solution_to_send is now less critical if RAG excerpts and Precise Context are good.
                    # It serves as a fallback or for very broad context.
                    solution_to_send = truncate_text_for_context(config, current_solution, solution_tokens_for_patcher, "middle")
                    
                    patch_user_prompt = f"""Original Problem:\n---\n{initial_problem}\n---
{outline_ctx_for_patcher}
{precise_context_for_patcher}
{rag_excerpts_str}
Document to Patch (overall context, possibly truncated if RAG/Precise Context are primary focus):\n---\n{solution_to_send}\n---
Improvement Feedback:\n---\n{feedback_history[-1]}\n---
Now, call the `generate_json_patch_list` tool with the necessary patch operations based on ALL provided information.
"""
                    
                    json_patch_str = None
                    try:
                        logging.info("  修补程序 (带 RAG 上下文的工具调用) 尝试... (Patcher (Tool Call with RAG context) attempt...)")
                        response = config.client.chat.completions.create(
                            model=config.main_ai_model,
                            messages=[
                                {"role": "system", "content": patch_tool_sys_prompt},
                                {"role": "user", "content": patch_user_prompt}
                            ],
                            tools=patcher_tools,
                            tool_choice={"type": "function", "function": {"name": "generate_json_patch_list"}},
                            temperature=0.0 # Low temperature for precise patching
                        )
                        
                        response_message = response.choices[0].message
                        tool_calls = response_message.tool_calls

                        if tool_calls:
                            function_args_str = tool_calls[0].function.arguments
                            # Attempt to parse the arguments, which should be a JSON string for the 'patches' list
                            # Use the robust _extract_json_from_ai_response to handle potential malformations
                            # The 'function_args_str' itself is the JSON string like '{"patches": [...] }'
                            # We need to parse this outer JSON, then get the "patches" list, then stringify that list for apply_patch
                            
                            # Re-evaluating: _extract_json_from_ai_response is for AI text output.
                            # Tool call arguments are usually well-formed JSON from the API.
                            # So, a direct json.loads should be fine here for the arguments string.
                            parsed_tool_args = json.loads(function_args_str)
                            patches_list_from_tool = parsed_tool_args.get("patches")

                            if patches_list_from_tool is not None: # Can be an empty list []
                                json_patch_str = json.dumps(patches_list_from_tool) # Stringify the list for apply_patch
                                logging.info(f"  通过工具调用成功生成补丁 JSON。补丁 (Successfully generated patch JSON via Tool Calling. Patches): {json_patch_str[:300]}...")
                            else:
                                logging.warning("  修补程序 (工具调用): 工具参数中未找到 'patches' 键。默认为无补丁。(Patcher (Tool Call): 'patches' key not found in tool arguments. Defaulting to no patches.)")
                                json_patch_str = "[]"
                        else:
                            logging.error("  修补程序 (工具调用) 未按预期返回工具调用。默认为无补丁。(Patcher (Tool Call) did not return a tool call as expected. Defaulting to no patches.)")
                            json_patch_str = "[]"

                    except json.JSONDecodeError as json_e:
                        logging.error(f"  修补程序 (工具调用) 参数不是有效的 JSON (Patcher (Tool Call) arguments were not valid JSON): {json_e}. 参数 (Args): {function_args_str[:500]}...", exc_info=True)
                        json_patch_str = "[]"
                    except Exception as e:
                        logging.error(f"  修补程序 (工具调用) 失败，出现异常 (Patcher (Tool Call) failed with an exception): {e}", exc_info=True)
                        json_patch_str = "[]"

                    if json_patch_str:
                        logging.info(f"主 AI (带 RAG 的修补程序) JSON 补丁字符串 (Main AI (Patcher with RAG) JSON Patch String):\n{json_patch_str}")
                        if config.session_dir:
                            with open(os.path.join(config.session_dir, f"iteration_{i}_patch_extracted_rag.json"), "w", encoding="utf-8") as f: f.write(json_patch_str)
                        
                        if json_patch_str.strip() and json_patch_str not in ["[]", "{}"]:
                            original_len = len(current_solution)
                            current_solution = apply_patch(current_solution, json_patch_str)
                            if len(current_solution) != original_len:
                                logging.info(f"补丁已应用。长度变化 (Patches applied. Length change): {original_len} -> {len(current_solution)}")
                                # V12 RAG: Update the document RAG store after successful patching
                                if vector_db_manager and document_outline_data:
                                    logging.info("  补丁后：更新文档 RAG 集合。(Post-patch: Updating document RAG collection.)")
                                    await update_document_rag_collection(current_solution, document_outline_data)

                                try:
                                    parsed_patches = json.loads(json_patch_str)
                                    if isinstance(parsed_patches, list) and parsed_patches:
                                        successful_patches_history.append({
                                            "iteration": i,
                                            "feedback_applied": feedback_history[-1][:500] + "...",
                                            "patch_json": parsed_patches,
                                            "rag_excerpts_provided_length": len(rag_excerpts_str),
                                            "precise_context_provided_length": len(precise_context_for_patcher)
                                        })
                                except json.JSONDecodeError:
                                    logging.warning(f"无法解析成功的补丁 JSON 以用于历史记录 (Unable to parse successful patch JSON for history): {json_patch_str[:200]}")
                            else:
                                logging.info("补丁已应用，但解决方案长度未变 (或补丁无效/为空)。(Patches applied, but solution length unchanged (or patches were ineffective/empty).)")
                        else:
                            logging.info("修补程序 AI 未在补丁 JSON 中提供有效更改。(No valid changes provided in patch JSON by Patcher AI.)")
                    else:
                        # This case should ideally not be reached if json_patch_str defaults to "[]"
                        logging.error("修补程序未能生成任何补丁字符串 (甚至默认的 '[]')。跳过此迭代的补丁。(Patcher failed to produce any patch string (even default '[]'). Skipping patch for this iteration.)")

                elif i > 0 and not feedback_history:
                    logging.warning(f"迭代 {i + 1}: 没有可应用补丁的反馈历史记录。跳过补丁阶段。(Iteration {i + 1}: No feedback history to apply patches from. Skipping patch phase.)")

                if current_solution is None :
                    logging.error(f"迭代 {i+1}: current_solution 在评论阶段前为 None。中止迭代。(Iteration {i+1}: current_solution is None before critic phase. Aborting iteration.)")
                    break

                logging.info("评论 AI 正在分析当前解决方案...(Critic AI analyzing current solution...)")
                # The critic also needs the full external data for accurate assessment
                critic_system_prompt_data_block = f"\n\nPlease strictly refer to the following background materials:\n--- External Reference Materials Start ---\n{external_data}\n--- External Reference Materials End ---\n" if external_data else ""
                critic_sys_prompt_full = full_system_prompt_secondary_ai_template + critic_system_prompt_data_block
                if document_outline_data:
                    critic_sys_prompt_full += f"\n\nDocument Outline Reference (for context):\n---\n{json.dumps(document_outline_data, ensure_ascii=False, indent=2)}\n---"
                tokens_for_critic_sys = config.count_tokens(critic_sys_prompt_full)
                tokens_for_critic_user_wrapper = config.count_tokens(f"Original Problem:\n---\n{initial_problem}\n---\nSolution (partial):\n---\n \n---\nCritique:")
                available_tokens_for_critic_solution = config.max_context_for_long_text_review_tokens - tokens_for_critic_sys - tokens_for_critic_user_wrapper - 200
                solution_for_critic = truncate_text_for_context(config, current_solution, available_tokens_for_critic_solution)
                critic_user_prompt = f"Original Problem:\n---\n{initial_problem}\n---\nSolution (partial, use full memory & outline for review):\n---\n{solution_for_critic}\n---\nProvide your critique, including KNOWLEDGE GAPS if any:"
                feedback = call_ai(config, config.secondary_ai_model, [{"role": "system", "content": critic_sys_prompt_full}, {"role": "user", "content": critic_user_prompt}])
                feedback_history.append(feedback)
                logging.info(f"评论 AI 反馈 (Critic AI Feedback):\n{feedback[:1000]}...")
                if config.session_dir:
                    with open(os.path.join(config.session_dir, f"iteration_{i}_critic_feedback.txt"), "w", encoding="utf-8") as f: f.write(feedback)

                knowledge_gaps = extract_knowledge_gaps(feedback)
                if knowledge_gaps: # Original condition: if knowledge_gaps exist
                    if config.interactive_mode:
                        logging.info("\n--- 交互式知识空白审查 --- (--- Interactive Knowledge Gap Review ---)")
                        print("\n提取的潜在研究知识空白：(Extracted Knowledge Gaps for Potential Research:)")
                        for idx_gap, gap_text_display in enumerate(knowledge_gaps):
                            print(f"{idx_gap + 1}. {gap_text_display}")
                        
                        while True:
                            user_choice_gap = input("接受空白以进行 AI 驱动的查询生成 (a)，跳过这些空白的研究 (s)，或直接提供您自己的查询 (p)? [a/s/p]: ").lower()
                            if user_choice_gap == 'a':
                                logging.info("用户接受知识空白以进行 AI 驱动的研究查询生成。(Knowledge gaps accepted by user for AI-driven research query generation.)")
                                break
                            elif user_choice_gap == 's':
                                logging.info("用户选择跳过这些特定空白的研究。(User chose to skip research for these specific gaps.)")
                                knowledge_gaps = [] # Clear gaps to prevent research
                                break
                            elif user_choice_gap == 'p':
                                logging.info("请逐行提供您的搜索查询。在空行按 Enter 结束。(Please provide your search queries, one per line. Press Enter on an empty line to finish.)")
                                custom_queries = []
                                while True:
                                    query_input = input("自定义查询 (Custom Query): ")
                                    if not query_input.strip():
                                        break
                                    custom_queries.append(query_input.strip())
                                if custom_queries:
                                    # We'll use these custom_queries as if they were the knowledge_gaps,
                                    # and the create_intelligent_search_queries might further refine them or use them directly.
                                    # Or, a better way: pass them as pre-defined queries to run_research_cycle_async.
                                    # For now, let's assume create_intelligent_search_queries will handle these as inputs.
                                    knowledge_gaps = custom_queries
                                    logging.info(f"使用用户提供的查询作为研究基础 (Using user-provided queries as a basis for research): {knowledge_gaps}")
                                else:
                                    logging.info("未提供自定义查询。为安全起见，由于选择了 'p' 但未提供查询，默认为“跳过研究”。(No custom queries provided. Defaulting to 'skip research' for safety as 'p' was chosen but no queries given.)")
                                    knowledge_gaps = [] # Skip research
                                break
                            else:
                                print("无效输入。请输入 'a', 's', 或 'p'。(Invalid input. Please enter 'a', 's', or 'p'.)")
                    
                    # Proceed with research only if knowledge_gaps is still populated
                    if knowledge_gaps:
                        document_context_for_research = truncate_text_for_context(config, current_solution, 4000)
                        # The run_research_cycle_async will use create_intelligent_search_queries, which takes these 'gaps' (or user queries)
                        research_brief = await run_research_cycle_async(config, knowledge_gaps, document_context_for_research)
                        if research_brief:
                            logging.info("将新的研究简报整合到 external_data 中。(Integrating new research brief into external_data.)")
                        all_research_briefs.append(research_brief)
                        external_data += research_brief # Add to the main external data variable
                        external_data_checksum = calculate_checksum(external_data)
                        
                        # V11 MODIFICATION: Update the context manager with new external data
                        context_manager._summarize_external_data(external_data)
                        logging.info("系统提示和上下文管理器已使用新的研究简报更新。(System prompts and context manager updated with new research brief.)")

                last_completed_iter_idx = i
                save_checkpoint(config, last_completed_iter_idx, current_solution, feedback_history, initial_problem, config.initial_solution_target_chars, config.max_iterations, external_data_checksum, document_outline_data, successful_patches_history, all_research_briefs, style_guide)

                user_forced_continue = False
                if config.interactive_mode and i < config.max_iterations - 1: # Don't ask on the very last iteration
                    logging.info("\n--- 交互式迭代审查 --- (--- Interactive Iteration Review ---)")
                    print(f"\n迭代 {i + 1} / {config.max_iterations} 已完成。(Iteration {i + 1} of {config.max_iterations} completed.)")
                    print("最新 AI 反馈摘要 (前 1000 字符)：(Summary of latest AI Feedback (first 1000 chars):)")
                    print(feedback_history[-1][:1000] + "..." if feedback_history else "尚无反馈。(No feedback yet.)")
                    
                    is_ai_satisfied = any(phrase in feedback_history[-1] for phrase in ["没有显著改进空间", "非常完善", "无需进一步修改"]) and len(feedback_history[-1].strip()) > 30
                    
                    if is_ai_satisfied:
                        print("\nAI 反馈表明解决方案已完成或得到显著改进。(AI feedback suggests the solution is complete or significantly improved.)")
                        action_iter = input("进入最终处理阶段 (f)，或强制再进行一次迭代 (c)? [f/c]: ").lower()
                        if action_iter == 'f':
                            logging.info("用户选择根据 AI 反馈进入最终处理阶段。(User chose to proceed to finalization based on AI feedback.)")
                            break # Break from the main iteration loop
                        elif action_iter == 'c':
                            logging.info("用户选择强制再进行一次迭代，尽管 AI 已满意。(User chose to force another iteration despite AI satisfaction.)")
                            user_forced_continue = True # Flag to override AI satisfaction break
                        else:
                            logging.info("无效输入，默认为进入最终处理阶段。(Invalid input, defaulting to proceed to finalization.)")
                            break
                    else:
                        action_iter = input("继续下一次迭代 (c)，或停止迭代并最终处理 (s)? [c/s]: ").lower()
                        if action_iter == 's':
                            logging.info("用户选择停止迭代并最终处理。(User chose to stop iterations and finalize.)")
                            break # Break from the main iteration loop
                        elif action_iter == 'c':
                            logging.info("用户选择继续下一次迭代。(User chose to continue to the next iteration.)")
                        else:
                            logging.info("无效输入，假设继续下一次迭代。(Invalid input, assuming continue to next iteration.)")
                
                # Check for AI suggesting completion (this was already here, ensure it's compatible with interactive override)
                if not user_forced_continue and any(phrase in feedback for phrase in ["没有显著改进空间", "非常完善", "无需进一步修改"]) and len(feedback.strip()) > 30:
                    logging.info("评论 AI 认为解决方案已完成。进入最终处理阶段。(Critic AI deems solution complete. Proceeding to finalization.)")
                    break
                if i == config.max_iterations - 1: # This is the true end of loop if not broken earlier
                    logging.info("已达到最大迭代次数。进入最终处理阶段。(Max iterations reached. Proceeding to finalization.)")
        except (KeyboardInterrupt, Exception) as e:
            logging.error(f"优化循环在迭代 {last_completed_iter_idx + 2} 期间中断/失败 (Optimization loop interrupted/failed during iteration {last_completed_iter_idx + 2}): {e}", exc_info=True)
            if current_solution and last_completed_iter_idx >= -1: # Ensure last_completed_iter_idx is valid
                save_checkpoint(config, last_completed_iter_idx, current_solution, feedback_history, initial_problem, config.initial_solution_target_chars, config.max_iterations, external_data_checksum, document_outline_data, successful_patches_history, all_research_briefs, style_guide)
            error_message = f"优化失败 (Optimization failed): {e}"
            logging.error(error_message)
            return None, successful_patches_history, all_research_briefs, feedback_history, error_message

    if current_solution:
        # V11 MODIFICATION: Use the clean system prompt for conclusion as well
        final_conclusion_text = generate_final_conclusion(
            config, current_solution, initial_problem, system_prompt_base, document_outline_data
        )
        if not current_solution.endswith(("\n", "\n\n")): current_solution += "\n\n"
        current_solution += final_conclusion_text
        logging.info(f"包含结论的文档已生成 ({len(current_solution)} 字符)。(Document with conclusion generated ({len(current_solution)} chars).)")

        logging.info("\n--- 对整个文档进行最终润色 --- (--- Performing Final Polish on Entire Document ---)")
        current_solution = perform_final_polish(config, current_solution, style_guide)
        logging.info(f"最终润色后的文档已准备就绪 ({len(current_solution)} 字符)。(Final polished document ready ({len(current_solution)} chars).)")

        save_checkpoint(config, last_completed_iter_idx, current_solution, feedback_history, initial_problem, config.initial_solution_target_chars, config.max_iterations, external_data_checksum, document_outline_data, successful_patches_history, all_research_briefs, style_guide)
        output_filename = f"final_solution_v11_{datetime.now().strftime('%H%M%S')}.txt"
        output_filepath = os.path.join(config.session_dir, output_filename) if config.session_dir else output_filename
        try:
            if config.session_dir: os.makedirs(config.session_dir, exist_ok=True)
            with open(output_filepath, "w", encoding="utf-8") as f: f.write(current_solution)
            logging.info(f"最终解决方案已保存至 (Final solution saved to): {output_filepath}")
        except Exception as e: logging.error(f"保存最终解决方案时出错 (Error saving final solution): {e}")

        is_task_fully_completed = (last_completed_iter_idx >= config.max_iterations - 1) or \
                                    (feedback_history and any(phrase in feedback_history[-1] for phrase in ["没有显著改进空间", "非常完善", "无需进一步修改"]))
        if is_task_fully_completed:
            if vector_db_manager:
                logging.info("--- 任务完成。进行经验积累。--- (--- Task complete. Proceeding to experience accumulation. ---)")
                accumulate_experience(config, vector_db_manager, initial_problem, current_solution, feedback_history, successful_patches_history, all_research_briefs)
            delete_checkpoint(config)
        else:
            logging.info(f"任务未完全完成 (上次完成的迭代索引: {last_completed_iter_idx}, 最大迭代次数: {config.max_iterations})。检查点已保留。(Task not fully complete (last completed iter index: {last_completed_iter_idx}, max_iters: {config.max_iterations}). Checkpoint retained.)")
    else:
        logging.warning("未生成解决方案。跳过最终保存和经验积累。(No solution generated. Skipping final save and experience accumulation.)")

    return current_solution, successful_patches_history, all_research_briefs, feedback_history, None

def generate_style_guide(config: Config) -> str:
    """Generates a style and voice guide for the entire document."""
    logging.info("\n--- V11: 生成风格与声音指南 --- (--- V11: Generating Style & Voice Guide ---)")
    prompt = f"""
    你是一位经验丰富的总编辑。请根据用户的核心问题，为即将撰写的深度报告制定一份简明扼要的《风格与声音指南》。

    # 核心问题:
    "{config.user_problem}"

    # 你的任务是定义以下几点:
    1.  **核心论点 (Core Thesis)**: 用一句话总结本文最关键、最想证明的中心思想。
    2.  **目标读者 (Audience)**: 这篇文章是写给谁看的？（例如：物理专业的大学生、对空气动力学感兴趣的高中生、领域内的专家学者）
    3.  **写作语气 (Tone)**: 文章应该是什么感觉？（例如：学术严谨、科普风趣、客观中立、带有批判性思维）
    4.  **叙事节奏 (Narrative Pace)**: 内容应该如何展开？（例如：从基本概念入手，逐步深入，最后进行高阶推导；还是开门见山，直接切入核心，再补充背景）
    5.  **关键术语 (Key Terminology)**: 列出本文必须统一使用的3-5个核心术语及其简要定义，确保全文一致性。

    请直接输出这份指南，不要添加任何额外的解释。
    """
    messages = [{"role": "system", "content": "你是一位创作风格指南的大师级编辑。"},
                {"role": "user", "content": prompt}]

    style_guide = call_ai(config, config.editorial_model_name, messages, max_tokens_output=1024, temperature=0.1)

    if "AI模型调用失败" in style_guide:
        logging.error(f"生成风格指南失败 (Failed to generate style guide): {style_guide}")
        return ""

    logging.info("--- V11: 风格与声音指南已生成 --- (--- V11: Style & Voice Guide Generated ---)")
    logging.info(style_guide)
    return style_guide

# 【最终方案】替换掉旧的 review_and_correct_outline 函数 (【Final Solution】Replace the old review_and_correct_outline function)
def review_and_correct_outline_with_tools(config: Config, original_problem: str, completed_summary: str, remaining_outline: list, style_guide: str, latest_feedback: str = "") -> list | None:
    """
    使用“函数调用/工具调用”模式，让AI审查并修正大纲，确保返回可靠的JSON。
    """ # 中文翻译 (Use "function call/tool call" mode to let AI review and correct the outline, ensuring reliable JSON is returned.)
    logging.info("--- V11: 调用战略规划师审查剩余大纲 (使用工具调用) --- (--- V11: Calling Strategic Planner to review remaining outline (using Tool Calling) ---)")

    # 1. 获取我们预先定义好的工具结构 (Get our predefined tool structure)
    tools = get_outline_review_tool_definition()

    # 2. 准备给AI的指令 (Prompt) (Prepare instructions (Prompt) for AI)
    style_guide_prompt_block = f"[风格指南]\n{style_guide}\n" if style_guide else ""
    remaining_outline_json = json.dumps(remaining_outline, ensure_ascii=False, indent=2)
    feedback_section = f"\n[最近的专家反馈]\n{latest_feedback}" if latest_feedback else ""
    prompt = f"""
你是一位顶级的项目战略规划师，正在对一个复杂的报告项目进行中期复盘。
{style_guide_prompt_block}
[原始目标]
用户的核心需求是："{original_problem}"

[已完成的工作摘要]
---
{completed_summary}
---

[剩余部分的原始计划]
---
{remaining_outline_json}
---
{feedback_section}

[你的任务]
基于已完成的工作和反馈，审慎评估“剩余计划”是否是最佳路径。然后，必须调用 `update_document_outline` 函数来提交最终的、经过优化的剩余计划。
- 如果计划需要调整（例如，章节需要拆分、合并或调整重点），请在调用函数时传入修改后的计划。
- 如果原始计划依然完美，无需改动，请在调用函数时传入与原始计划完全相同的JSON内容。
"""
    messages = [{"role": "system", "content": "你是一位战略规划师，必须使用 `update_document_outline` 工具进行响应。"},
              {"role": "user", "content": prompt}]

    try:
        # 3. 调用API，并强制使用我们定义的工具 (Call API and force use of our defined tool)
        response = config.client.chat.completions.create(
            model=config.planning_review_model_name,
            messages=messages,
            tools=tools,
            tool_choice={"type": "function", "function": {"name": "update_document_outline"}}, # <-- 强制执行的关键 (<- Key to force execution)
            temperature=0.1
        )

        # 4. 解析API返回的结构化响应 (Parse structured response returned by API)
        response_message = response.choices[0].message
        tool_calls = response_message.tool_calls

        if not tool_calls:
            logging.error("V11 规划师 (工具) 未按预期返回工具调用。(V11 Planner (Tools) did not return a tool call as expected.)")
            return None
        
        # 提取由API保证格式正确的JSON字符串参数 (Extract JSON string parameters whose format is guaranteed by API)
        function_args_str = tool_calls[0].function.arguments
        
        # 直接解析，无需担心格式错误 (Direct parsing, no need to worry about format errors)
        parsed_args = json.loads(function_args_str)
        new_plan = parsed_args.get("revised_plan")

        if not isinstance(new_plan, list):
            logging.error(f"V11 规划师 (工具) 为 'revised_plan' 返回了无效的数据类型 (预期列表，得到 {type(new_plan)})。(V11 Planner (Tools) returned invalid data type for 'revised_plan' (expected list, got {type(new_plan)}).)")
            return None

        # 检查计划是否有实质性变动 (Check if the plan has substantial changes)
        original_titles = {item.get('title') for item in remaining_outline}
        new_titles = {item.get('title') for item in new_plan}
        if original_titles == new_titles and len(original_titles) == len(new_plan):
            logging.info("V11 规划师 (工具) 审查了计划并确认无需更改。(V11 Planner (Tools) reviewed the plan and confirmed no changes are needed.)")
            return None # 返回None表示无需修改 (Return None means no modification is needed)

        logging.info("V11 规划师 (工具) 提出了更新后的大纲。(V11 Planner (Tools) has suggested an updated outline.)")
        return new_plan

    except Exception as e:
        logging.error(f"V11 规划师 (工具) 失败，出现异常 (V11 Planner (Tools) failed with an exception): {e}", exc_info=True)
        return None

def perform_final_polish(config: Config, full_document_text: str, style_guide: str) -> str:
    """Performs a final pass on the entire document to improve cohesion and flow."""
    logging.info("\n--- V11: 对整个文档进行最终润色 --- (--- V11: Performing Final Polish on the Entire Document ---)")

    style_guide_context = f"""
    # 附：风格与声音指南 (请确保最终稿符合此指南)
    ---
    {style_guide}
    ---
    """ if style_guide else ""

    prompt = f"""
    你是一位顶级的润色编辑。下面的文本是一份报告的初稿，由多个部分拼接而成。你的任务是通读全文，进行微调，使其浑然一体。

    # 你需要完成以下任务：
    1.  **平滑过渡**: 检查每个以 "##" 标题开始的章节之间的过渡。如果衔接生硬，请重写前一章的结尾段落或后一章的开头段落，使其衔接流畅。你不需要重写整个章节。
    2.  **统一术语**: 通读全文，确保核心概念的术语在全文中保持严格一致。
    3.  **调整语气**: 确保全文的语气和风格符合指南。修正任何看起来突兀的句子或段落。
    4.  **消除冗余**: 删除重复的句子或想法。

    # 注意事项：
    - 你的目标是“润色”而非“重写”。请尽可能保留原文的结构和内容。
    - 直接输出经过你润色后的完整文稿。

    {style_guide_context}

    # 待润色的初稿：
    ---
    {truncate_text_for_context(config, full_document_text, 28000)}
    ---
    """

    messages = [{"role": "system", "content": "你是一位大师级的润色编辑。"},
                {"role": "user", "content": prompt}]

    polished_text = call_ai(config, config.editorial_model_name, messages, max_tokens_output=8192, temperature=0.1)

    if "AI模型调用失败" in polished_text or len(polished_text) < len(full_document_text) * 0.8:
        logging.error(f"最终润色失败或生成的文本过短。返回原始文档。错误 (Final polish failed or produced a much shorter text. Returning original document. Error): {polished_text}")
        return full_document_text

    logging.info("--- V11: 最终润色完成 --- (--- V11: Final Polish Complete ---)")
    return polished_text


# Old review_and_correct_outline function is now removed.
# The new function review_and_correct_outline_with_tools is already added above.


def generate_final_conclusion( config: Config, document_content: str, problem_statement: str,
                               system_prompt: str, outline_data: dict | None = None) -> str:
    logging.info("\n--- 为文档生成最终结论 --- (--- Generating final conclusion for the document ---)")

    doc_tail_tokens = config.max_context_for_long_text_review_tokens // 3
    truncated_doc_for_ctx = truncate_text_for_context(config, document_content, doc_tail_tokens, "tail")

    conclusion_desc_from_outline = "Summarize key findings, address the main problem, and offer outlook/limitations."
    target_chars_conclusion = 500
    if outline_data and outline_data.get("outline"):
        # Iterate backwards to find the conclusion, which is usually last
        for item in reversed(outline_data["outline"]):
            if isinstance(item, dict) and ("conclusion" in item.get("title","").lower() or "总结" in item.get("title","")):
                conclusion_desc_from_outline = item.get("description", conclusion_desc_from_outline)
                target_chars_conclusion = item.get("allocated_chars", target_chars_conclusion)
                break

    user_prompt_conclusion = f"""
    Original Problem:
    ---
    {problem_statement}
    ---
    Guidance for Conclusion (from outline): "{conclusion_desc_from_outline}"
    Target length for conclusion: ~{target_chars_conclusion} characters.

    Main Document Content (Latter Part for Context):
    ---
    {truncated_doc_for_ctx}
    ---
    Based on the full document (you have memory of its generation) and the above context, please write a comprehensive and insightful "Conclusion" section.
    It should summarize the primary arguments/findings and directly address the original problem.
    Output ONLY the content for the conclusion section, starting a new paragraph. Do not add the "## Conclusion" title yourself.
    """
    messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt_conclusion}]

    estimated_tokens = max(256, target_chars_conclusion // 3)
    max_tokens_for_conclusion = min(config.max_chunk_tokens, estimated_tokens + 512)

    conclusion_text = call_ai(config, config.main_ai_model, messages, max_tokens_output=max_tokens_for_conclusion, temperature=0.15)

    if "AI模型调用失败" in conclusion_text:
        logging.error(f"  生成最终结论失败 (Failed to generate final conclusion): {conclusion_text}")
        return "[最终结论生成失败]"

    logging.info(f"--- 最终结论已生成 ({len(conclusion_text)} 字符) --- (--- Final conclusion generated ({len(conclusion_text)} chars) ---)")
    return f"## 结论\n\n{conclusion_text.strip()}\n"


def quality_check(config: Config, content: str) -> str:
    content_for_review = truncate_text_for_context(config, content, config.max_context_for_long_text_review_tokens - 1000)
    prompt = f"Deeply evaluate the quality of the following content. Provide a score (0-10) for: Depth, Detail, Structure, Coherence, Problem Fit, Reference Use. List main pros & cons.\n\nContent:\n{content_for_review}"
    return call_ai(config, config.secondary_ai_model, [{"role": "user", "content": prompt}])

def accumulate_experience(config: Config, db_manager: VectorDBManager,
                          problem: str, final_solution: str | None,
                          feedback_history: list[str],
                          successful_patches: list[dict],
                          research_briefs: list[str]):
    logging.info("\n--- 将经验积累到向量数据库 --- (--- Accumulating Experience into Vector Database ---)")
    if not db_manager or not db_manager.collection:
        logging.error("VectorDBManager 未正确初始化。跳过经验积累。(VectorDBManager not properly initialized. Skipping experience accumulation.)")
        return

    experience_items = []
    metadatas = []
    ids = []
    current_time_iso = datetime.now().isoformat()
    problem_hash = hashlib.md5(problem.encode()).hexdigest()

    if final_solution:
        experience_items.append(final_solution)
        metadatas.append({"type": "final_solution", "problem_summary": problem[:200], "date": current_time_iso})
        ids.append(f"solution_{problem_hash}_{int(time.time())}")

    for i, patch_info in enumerate(successful_patches):
        feedback_text = patch_info.get('feedback_applied', 'N/A')
        if not isinstance(feedback_text, str): feedback_text = str(feedback_text)

        text_content = f"Feedback at Iteration {patch_info.get('iteration', 'N/A')}:\n{feedback_text}\n\nApplied Patch:\n{json.dumps(patch_info.get('patch_json', {}), ensure_ascii=False, indent=2)}"
        experience_items.append(text_content)
        metadatas.append({"type": "feedback_patch_pair", "problem_summary": problem[:200], "iteration": patch_info.get('iteration', 'N/A'), "date": current_time_iso})
        ids.append(f"patchpair_{problem_hash}_{patch_info.get('iteration', 'N/A')}_{i}")

    for i, brief in enumerate(research_briefs):
        experience_items.append(brief)
        metadatas.append({"type": "research_brief", "problem_summary": problem[:200], "date": current_time_iso})
        ids.append(f"research_{problem_hash}_{int(time.time())}_{i}")

    if final_solution :
        task_summary_prompt = f"""
        Based on the original problem and the final solution provided below, generate a concise summary of what was learned or achieved during this task.
        Focus on key insights, successful methods, or important knowledge points. This summary will be stored as a learning experience.

        Original Problem:
        ---
        {problem}
        ---

        Final Solution (excerpt for context, focus on overall learning):
        ---
        {truncate_text_for_context(config, final_solution, 2000, "middle")}
        ---

        Task Learning Summary (be concise and insightful):
        """
        task_learning_summary = call_ai(config, config.summary_model_name, [{"role": "user", "content": task_summary_prompt}], max_tokens_output=512)
        if task_learning_summary and "AI模型调用失败" not in task_learning_summary:
            experience_items.append(task_learning_summary)
            metadatas.append({"type": "task_learning_summary", "problem_summary": problem[:200], "date": current_time_iso})
            ids.append(f"learningsummary_{problem_hash}_{int(time.time())}")
        else:
            logging.warning("生成任务学习摘要失败。(Failed to generate task learning summary.)")

    if experience_items:
        db_manager.add_experience(texts=experience_items, metadatas=metadatas, ids=ids)
    else:
        logging.info("此任务无新经验可积累。(No new experiences to accumulate for this task.)")


async def generate_extended_content_workflow(config: Config, vector_db_manager: VectorDBManager | None) -> str:
    logging.info("\n--- 开始扩展内容工作流 (V11 上下文管理器修复) --- (--- Starting Extended Content Workflow (V11 Context Manager Fix) ---)")
    initial_problem = config.user_problem
    external_data_files = config.external_data_files

    loaded_ext_data = load_external_data(config, external_data_files or [])

    style_guide = generate_style_guide(config)

    retrieved_experience_text = ""
    if vector_db_manager:
        logging.info("--- 尝试从向量数据库检索相关经验 --- (--- Attempting to retrieve relevant experiences from Vector DB ---)")
        retrieved_exps = vector_db_manager.retrieve_experience(initial_problem)
        if retrieved_exps:
            formatted_experiences = ["\n\n===== 开始：上下文检索到的经验 =====\n"]
            for exp_idx, exp in enumerate(retrieved_exps):
                exp_info = f"类型: {exp.get('metadata', {}).get('type', 'N/A')}, 距离: {exp.get('distance', -1):.4f}"
                logging.info(f"  检索到的经验 {exp_idx+1}: {exp_info} - {exp.get('document', '')[:100]}... (Retrieved Experience {exp_idx+1}: {exp_info} - {exp.get('document', '')[:100]}...)")
                formatted_experiences.append(f"\n---经验 {exp_idx+1} ({exp_info}) 开始---\n{exp.get('document')}\n---历史经验 {exp_idx+1} 结束---\n")
            formatted_experiences.append("\n===== 结束：检索到的经验 =====\n\n")
            retrieved_experience_text = "\n".join(formatted_experiences)
            logging.info(f"成功检索并格式化了 {len(retrieved_exps)} 条经验。(Successfully retrieved and formatted {len(retrieved_exps)} experiences.)")
    else:
        logging.warning("向量数据库管理器不可用。在没有历史经验的情况下继续。(Vector DB manager not available. Proceeding without historical experience.)")

    final_external_data = retrieved_experience_text + loaded_ext_data
    ext_data_checksum = calculate_checksum(final_external_data)
    if retrieved_experience_text:
        logging.info(f"添加经验后的总外部数据长度: {len(final_external_data)} 字符。(Total external data length after adding experiences: {len(final_external_data)} chars.)")

    final_answer, successful_patches_hist, research_briefs_hist, feedback_history, error_message = None, [], [], [], None

    try:
        final_answer, successful_patches_hist, research_briefs_hist, feedback_history, error_message = await optimize_solution_with_two_ais(
            config, initial_problem, style_guide, final_external_data, ext_data_checksum, vector_db_manager
        )
        if error_message:
            logging.error(f"优化过程因错误而完成 (Optimization process completed with an error): {error_message}")
            if not final_answer: final_answer = f"错误 (Error): {error_message}"

    except OptimizationError as oe:
        logging.error(f"优化过程已中断 (Optimization process was interrupted): {oe}")
        if oe.partial_data and isinstance(oe.partial_data, tuple):
            partial_solution, partial_feedback, patches_partial, briefs_partial = oe.partial_data
            if vector_db_manager and config.interactive_mode:
                approval = input("过程已中断。是否积累部分经验？(是/否) (Process interrupted. Accumulate partial experiences? (yes/no)): ").lower()
                if approval == 'yes':
                    accumulate_experience(config, vector_db_manager, initial_problem,
                                          None, # No final solution
                                          partial_feedback, patches_partial, briefs_partial)
        final_answer = f"错误：优化过程失败。(Error: Optimization process failed.) {oe}"
    except Exception as e:
        logging.critical(f"主工作流程中发生意外严重错误 (An unexpected critical error occurred in the main workflow): {e}", exc_info=True)
        final_answer = f"错误：工作流程严重失败。(Error: Critical failure in workflow.) {e}"

    logging.info("\n--- 工作流程完成 --- (--- Workflow Complete ---)")
    if final_answer and not final_answer.startswith("Error:"):
        logging.info(f"最终生成长度: {len(final_answer)} 字符。(Final generated length: {len(final_answer)} chars)")
        logging.info("\n--- 质量评估报告 --- (--- Quality Assessment Report ---)")
        quality_report = quality_check(config, final_answer)
        logging.info(quality_report)

        is_approved_for_memory = False
        if config.interactive_mode:
            approval = input("\n>>> 此输出是否符合质量标准以添加到长期知识库？(是/否) (Does this output meet quality standards to be added to the long-term knowledge base? (yes/no)): ").lower()
            if approval == 'yes':
                is_approved_for_memory = True
        else: # Automatic approval based on quality check
            if "10/10" in quality_report and "9/10" in quality_report:
                logging.info("输出通过自动质量阈值。已批准用于内存。(Output passed automatic quality threshold. Approved for memory.)")
                is_approved_for_memory = True
            else:
                logging.info("输出未达到自动质量阈值。(Output did not meet automatic quality threshold.)")

        if is_approved_for_memory:
            if vector_db_manager:
                accumulate_experience(config, vector_db_manager, initial_problem, final_answer, feedback_history, successful_patches_hist, research_briefs_hist)
            else:
                logging.warning("已批准内存，但 VectorDBManager 不可用。(Memory approval given, but VectorDBManager is not available.)")

    else:
        logging.error(f"工作流程未能生成有效的最终答案或因错误而结束 (Workflow failed to produce a valid final answer or ended with an error): {final_answer}")

    return final_answer if final_answer else "错误：工作流程未产生任何答案。(Error: Workflow resulted in no answer.)"


if __name__ == "__main__":
    config = Config()
    config.setup_logging()

    logging.info("--- 脚本执行开始 --- (--- Script Execution Started ---)")
    logging.info("确保设置了环境变量：DEEPSEEK_API_KEY, EMBEDDING_API_BASE_URL, EMBEDDING_API_KEY (Ensure environment variables are set: DEEPSEEK_API_KEY, EMBEDDING_API_BASE_URL, EMBEDDING_API_KEY)")
    logging.info("可选变量：GOOGLE_API_KEY, GOOGLE_CSE_ID (Optional variables: GOOGLE_API_KEY, GOOGLE_CSE_ID)")
    logging.info(f"向量数据库路径 (Vector DB Path): {config.vector_db_path}")
    logging.info(f"嵌入模型 (Embedding Model): {config.embedding_model_name}")

    try:
        config._initialize_deepseek_client()
    except Exception as e:
        logging.critical(f"致命错误：DeepSeek 客户端初始化失败 (FATAL: DeepSeek client initialization failed): {e}. 正在退出。(Exiting.)")
        sys.exit(1)

    embedding_model_instance = None
    vector_db_manager_instance = None
    try:
        embedding_model_instance = EmbeddingModel(config)
        if embedding_model_instance.client:
            vector_db_manager_instance = VectorDBManager(config, embedding_model_instance)
        else:
            logging.warning("嵌入客户端初始化失败。长期内存功能已禁用。(Embedding client failed to initialize. Long-term memory features are disabled.)")
    except Exception as e:
        logging.error(f"初始化嵌入或 VectorDB 管理器失败 (Failed to initialize Embedding or VectorDB manager): {e}. 长期内存功能可能受限。(Long-term memory features may be limited.)", exc_info=True)
        vector_db_manager_instance = None


    config.user_problem = os.getenv("USER_PROBLEM", """ 研究回旋镖飞行的具体原理，研究其运动如何取决于相关参数，最后要有可以联立求解运动轨迹的微分方程组，只要详细的理论推导，不许偏离主题，不允许讲工科部分，最后给我规范论文的格式，有引用，用中文回答我，要步步推导有逻辑，不确定的标注（此处不确定）""")

    external_files_str = os.getenv("EXTERNAL_FILES", "")
    config.external_data_files = [p.strip() for p in external_files_str.split(',') if p.strip()]

    if not config.external_data_files:
        logging.warning("未设置 EXTERNAL_FILES 环境变量。回退到硬编码的示例路径。(EXTERNAL_FILES environment variable not set. Falling back to hardcoded example paths.)")
        config.external_data_files = [
            # Example: "C:\\Users\\user\\Documents\\project_data\\source1.pdf"
        ]

    logging.info(f"用户问题 (User Problem): {config.user_problem[:100]}...")
    logging.info(f"外部文件 (External Files): {config.external_data_files}")
    logging.info(f"最大迭代次数 (Max Iterations): {config.max_iterations}, 目标字符数 (Target Chars): {config.initial_solution_target_chars}")
    logging.info(f"异步研究 (Async Research): {config.use_async_research}")
    logging.info(f"自适应规划 (Adaptive Planning): {config.enable_dynamic_outline_correction}")

    valid_files = [f for f in config.external_data_files if os.path.exists(f)]
    if len(valid_files) != len(config.external_data_files):
        logging.warning("某些指定的外部文件不存在，将被忽略。(Some specified external files do not exist and will be ignored.)")
    config.external_data_files = valid_files
    if not config.external_data_files:
        logging.info("未提供或找到有效的外部数据文件。(No valid external data files were provided or found.)")

    final_result = "错误：由于先前的初始化失败，工作流程未运行。(Error: Workflow did not run due to prior initialization failure.)"
    
    async def main_async_workflow(current_config, current_vector_db_manager):
        try:
            return await generate_extended_content_workflow(current_config, current_vector_db_manager)
        except Exception as e:
            logging.critical(f"主异步工作流程中发生未捕获的异常 (An uncaught exception occurred in the main async workflow): {e}", exc_info=True)
            return f"错误：异步工作流程严重失败。(Error: Critical failure in async workflow.) {e}"

    try:
        # With nest_asyncio.apply() at the top, asyncio.run() should now work
        final_result = asyncio.run(main_async_workflow(config, vector_db_manager_instance))
    except Exception as e:
        logging.critical(f"使用 asyncio.run 运行主异步工作流程时出错 (即使使用 nest_asyncio) (Error running main async workflow with asyncio.run (even with nest_asyncio)): {e}", exc_info=True)
        final_result = f"错误：运行主异步任务失败。(Error: Failed to run main async tasks.) {e}"

    logging.info("\n--- 主脚本执行完毕 --- (--- Main script execution finished. ---)")
    if final_result.startswith("Error:"):
        logging.error(f"脚本因错误而结束 (Script ended with an error): {final_result}")
    else:
        logging.info(f"成功生成内容。最终长度: {len(final_result)} 字符。(Successfully generated content. Final length: {len(final_result)} chars.)")
        if vector_db_manager_instance:
            vector_db_manager_instance.get_db_stats()
